<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[毁掉一个年轻人最好的方式，就是让他成为多面手]]></title>
    <url>%2F2018%2F10%2F05%2F%E6%AF%81%E6%8E%89%E4%B8%80%E4%B8%AA%E5%B9%B4%E8%BD%BB%E4%BA%BA%E6%9C%80%E5%A5%BD%E7%9A%84%E6%96%B9%E5%BC%8F%EF%BC%8C%E5%B0%B1%E6%98%AF%E8%AE%A9%E4%BB%96%E6%88%90%E4%B8%BA%E5%A4%9A%E9%9D%A2%E6%89%8B%2F</url>
    <content type="text"><![CDATA[毁掉一个年轻人最好的方式，就是让他成为多面手 转自：腾讯企鹅号 时间过得真快。今天是国庆假期的第 5 天。 这个假期，不管你是人在旅途，还是蜷缩在家，都开启了放飞自我模式。 抖音、微博、今日头条等，它们的设计初衷就是为了蚕食你的碎片时间与精力。 抖音的全屏设计甚至覆盖了手机顶部的时间显示，像一个既没有窗户也没有时钟的赌场。 这些 APP 的存在，让人们的生活变得丰富多彩，也使娱乐变得极为廉价。 现在，长假过半，该收心啦！ 今天的文章，来自书单的特约荐书人石若萧。 他为大家推荐的这本书，说出了一个扎心的道理：人与人之间的差距，是由碎片时间拉开的。 你的注意力出了问题 你是否面临过这种苦恼： 某一天，你接到了领导分配的任务，要求在三天之内写出一个报告。 你觉得这个任务不算难，于是做好了计划，打算按部就班地推进。 但当你坐在电脑前，面对空白的文档发了几分钟呆之后，便打开了浏览器，在百度信息流的引导下打开了一堆页面，开始浏览各种各样的内容。 看着看着，然后你觉得头有点晕，肚子又有点饿，于是你点了个外卖。 吃完后，把饭盒扔进垃圾桶，收拾了下桌子。 接着感到血糖上升，又开始犯困，不由自主地躺到了床上。一觉醒来已经是晚上了。 躺在床上，你心想，如果现在起来工作，晚上势必睡不着，第二天也说不定报销了。于是干脆闭眼继续睡了下去。 然而第二天，又重复了第一天的日程，直到第三天，deadline 临近，你才开始急吼吼地赶报告，饭也顾不上吃了。 最后你交给了领导一个逻辑乱七八糟，完全不符合预期的结果。 领导低头看着这份报告，又抬头看看你，慢慢皱起了眉头…… 那么问题就来了。既然所有的计划都做好了，只需要推进即可，但为什么就是完全无法控制自己？ 一切就在于，你在管理自己的注意力方面出现了问题。 今天，我和你分享的这本书，就叫《注意力曲线：打败分心与焦虑》。 为什么我们的注意力无法集中？ 远古时期，人类既是猎人，有时候也充当被捕食者的角色。 实时对变化的环境作出反应，是人类生存所需的必备技能。 想象一下：现在是五万年前，你正坐在河边钓鱼。这时候，身边的草丛发出沙沙声，一条毒蛇正向着你的方向潜行过来。 你察觉到了危险，肾上腺素分泌量大大增加。 这时，摆在你面前的有两种选择——要么跳起来跑，要么捡块石头把蛇砸死。 但如果你钓鱼太过认真，没发现蛇的到来，结局就只能是被蛇咬了。 这是写在人的基因中的本能，学名叫“定向反应”。而那些过于专心，没有感觉到野兽正在接近的先辈们，都被吃掉了。 当代的情况恰恰反了过来。 远古时期，一人往往要分饰屠夫、猎人、采果工、打柴工等几角。但现在却讲究专业化。 因此，想要做好手头的工作，比起“定向反应”，我们更需要的是“持续性注意”。 但很多人都有种错觉，认为自己可以同时做好几件事情而不影响效率。 事实并非如此。电脑中同时运行好几个程序时，其处理速度会显著下降，或者无法对新的指令做出反应，甚至干脆死机。 人脑和计算机其实差不多。当一个人同时干几件事情的时候，就等于过多地使用了自己的脑部资源，很容易变得效率低下。 况且，人类其实是无法真正意义上地完成多重任务的。 比如，你不可能同时完成看电视 + 写作业两件事。 实际情况是，你盯着电视看了两分钟，然后想起来自己该写作业了，又匆匆补了几笔。 这时你听到了女主角在哭，于是又好奇地抬起头来继续看电视。 这不叫多重任务，本质上这是一种在不同状态间的快速切换。 问题是，人在从一种状态切换到另一个状态时，很容易受到残留状态的影响。 比如，你如果在刚看完电视的时候写作业，心里免不了会一直挂念着女主角的结局。这种挂念，很容易让你作业的错误率显著上升。 研究表明，电视画面每 4 秒变化一次，这种持续且重复的定向反应让人的肾上腺素分泌激增，更加无法集中注意力。 到了移动互联网时代，信息的生产更加爆炸。 因此，为了掌控自己的生活，我们必须要学会首先掌控自己的注意力。 注意力是由什么决定的？ 《注意力曲线》一书的作者、心理学家露西·乔·帕拉迪诺认为，注意力主要由肾上腺素的分泌量来决定。 但注意力和肾上腺素的分泌量，两者之间的关系，并非是简单的正比或反比，而是呈一个“倒 U 型”曲线。 简单来说，如果一个人肾上腺素分泌量很低，那么他就会整天犯困，对什么都提不起兴致。 但如果肾上腺素分泌量过高，整个人就会变得狂躁不安，东一榔头西一棒槌，没办法专心做好任何事情。 诀窍就在于，如何将肾上腺素的分泌量，控制在“倒 U 型”曲线中的“注意力专区”中。 如果换算成 0-10 分的打分制，那么在现代工作中的大部分时间里，理想的注意力专区大概在 3-7 分之间。 0-2 分，可能是比较理想的休息区间，但只适合泡温泉。如果这时候办公，就会很容易打瞌睡。 而 8-10 分只适用于高强度的对抗性运动。对工作有害无益。 《注意力曲线》中提到，只要经过恰当的训练，这种分泌量其实是可以自我控制的。 如果你有注意力不足的问题，那么可以试着增大刺激。 通常来说，这种情况在那些居家工作的自由职业者身上比较常见。 因此，可以试着换个环境，到当地的书店、图书馆或咖啡馆办公。或者听一下积极向上的音乐，令自己的精神振奋起来。 如果你的问题是接受的刺激太大，任务太多，以至于无法沉下心力完成工作。 那么你可以采用瑜伽式的呼吸疗法，或者暂时中断一下任务，去玩一会游戏，睡个午觉，来让紧张的心情放松下来。 一言以蔽之，人的大脑就像肌肉一样，是可以通过不断地训练实现进步的。 理论上来说，只要训练得当，每个人最终都可以快速进入状态，针对不同情况合理分配自己的注意力。 如何恰当地管理好自己的注意力？ 但问题真的这么简单吗？ 《注意力曲线》给我的最重要启发，就是人往往不是因为任务太多导致无法集中注意力。 真正让人焦躁不安的，其实是逃避心理。 人性是有弱点的。研究表明，人对未完成任务的记忆，远远大于对已完成任务的记忆。 同时，人对失败的恐惧，也大过对成功的喜悦。 经济学研究表明，股票市场中，损失 3000 美元带来的痛苦感，要远大过赚到 3000 美元带来的激动。 因为害怕可能导致的糟糕结果，所以干脆什么也不做。 这是当代绝大多数拖延症背后的心理成因，并且人们还往往会用“完美主义”之类的词来粉饰这类行为。 如果不想方法克服这种恐惧心理，那么前述的做法甚至有可能会引起反作用。 比如说，你觉得压力太大，想放松一会儿。于是中断任务去玩游戏，结果因为迟迟不愿重新开始，最后把一整晚都浪费掉了。 其实，不光是你，当代人或多或少都要同这种恐惧做斗争。 在工业不发达的时期，人们的工作流程和结果都是相对固定的。 古时候，砍柴多久，得柴几斤，都有定数。工厂工人也一样，计件工资，按劳取酬，多劳多得，只消按部就班去做即可，完全没有拖延的理由。 但现在，机械重复的岗位越来越少了，和以前比起来，现在工作内容更需要创造力，存在着更多的不确定性，学习成本也更高。 面对这种不确定性。人们很容易担心，自己付出了时间，付出了精力，最终什么也得不到，还要被老板骂——就跟害怕损失 3000 美元的心态一模一样。 因此，倘若不克服恐惧，所有表面上的解决办法，都只是治标不治本，最终甚至成为继续拖延的借口。 书中给出了如下几种方式，让你学会直面自己。 1. 对己诚实 人们都很难承认，自己享受的事情，会成为自己成功的阻碍。 比如说，很多人习惯在办公的时候听嘈杂的摇滚乐，并自我暗示说这种行为会提升效率。 实际情况很可能并非如此。正确的态度，是换几种音乐类型作为对照。 2. 建立信心 人首先要相信自己，以及确定自己在做的事情有价值时，才能够全力以赴。 同时，为了不让自己太过焦虑，不妨把工作内容分解成几个小部分，每天完成一点。 3. 点燃希望 时刻提醒自己，按时完成工作会收获什么？而没有按时完成工作又会损失什么？ 同时，不妨多看看名人传记，在名人名言中点燃自己对未来的希望。 4. 直面过去 很多时候，拖延其实和童年的境遇和习惯有关。 举个例子，很多人童年时可能都有这种经验：被父母严格要求，不做完作业就不能出去玩。而作业又确实太难，可父母并不能理解到这一点。 这时候，孩子感到很无助，很生气，但又不能公然反抗说“我不想做了”。 因此他们只是尽可能拖延时间，一直拖到睡觉时间还没写完。父母也只能无可奈何地接受现实。 本质上，这是一种反抗行为。而且这种反抗很容易形成习惯，让人把“拖延”和“反抗权威”下意识地等同起来。 这一点尤为重要。因为多数人花费了太多时间处理眼下的危机，而没有充足的时间回溯过往，找到自己行为模式背后的心理成因。 注意力管理，是当代人必须要掌握的一项技巧。 随着 AI 技术的发展，简单重复的工作岗位正在逐渐消失，而缺少标准化流程指导的创造性任务正在大量出现。 面对这类工作，人们很容易感到无所适从，继而开始拖延不前。再加之廉价的娱乐产品推波助澜，让情况更是雪上加霜。 这就是我和大家分享《注意力曲线》这本书的原因。 没有注意力管理，我们要么被飞速前进的社会甩下，要么在被社会甩下的担忧中不自主地吸收大量无效信息，影响正常的工作产出，从而被甩得越来越远，陷入恶性循环。 为了迎接并应对社会的变化与挑战，我们需要系统化地学习大量知识。 但学习从来都不是一件轻松的事。学会如何恰当地管理好自己的注意力，并将它分配到阶段性的学习任务上，就变得分外重要了。 愿你能在这个飞速变化的世界中站稳脚跟。 不颓废，不浮躁。 静下心来，苦心钻研，力求突破，直至有所成就。]]></content>
      <categories>
        <category>随笔</category>
      </categories>
      <tags>
        <tag>随笔</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[详解日志的 5 个级别]]></title>
    <url>%2F2018%2F10%2F02%2F%E8%AF%A6%E8%A7%A3%E6%97%A5%E5%BF%97%E7%9A%845%E4%B8%AA%E7%BA%A7%E5%88%AB%2F</url>
    <content type="text"><![CDATA[日志记录是软件开发的一个概念，几乎所有（可能并不是所有）软件都能从日志记录中获得很多好处。在开始一个大项目时，日志记录通常是我第一个要搭建的子系统。关于它的好处，我可以说出一大堆，但我想把这个机会留给其他人（或者哪一天我想说了再说）。现在，我想说一说日志级别。 日志级别是对基本的“滚动文本”式日志记录的一个重要补充。每条日志消息都会基于其重要性或严重程度分配到一个日志级别。例如，“你的电脑着火了”是一个非常重要的消息，而“无法找到配置文件”的重要等级可能就低一些。 很多应用程序和库会根据自己或用户的需求定义自己的日志级别（参考文末的“外部例子”了解这方面的内容）。当然，并没有一种约定俗成的方法来做这件事，想怎么做都是可以的，但我想要说说我认为的最重要的五个（或者六个，或者四个）日志级别，它们应该是你自定义日志级别的基础。 我还将讨论给这些级别分配的颜色（或者说风格），因为带有不同颜色（或风格）的日志更容易追踪。如果采用了这样的系统，就可以很容易检查你的程序状态，就算没有受过训练的人也可以轻易分辨。谁知道呢，你可能留下电脑跑去吃午饭了，如果出现问题只能找别人来查看日志。 Error错误已经发生了，这是毫无疑问的。错误的来源可能是在外部，但不管怎样都需要看一下是怎么回事。 可以用这个级别来表示需要引起人们注意（大多数时候需要采取行动）的错误。大多数难以优雅处理的异常都属于 Error 范畴。 风格：能引起人们注意的东西。我使用红色文本来表示（我的终端背景是黑色的）。 例子： 无法找到”crucial.dat”文件 错误的处理数据：[堆栈追踪或后续的调试消息] 在连接数据库时在连接数据库时 Warn错误有可能已经发生了。我只是一条日志消息，无法分析到底发生了什么，或许需要其他人来看看是不是有问题。 这可能是一个平行空间里的错误。它可能是当前或未来潜在问题（比如响应速度慢、连接断开、内存吃紧等等）的预兆，也可能是程序在处理某些任务时出现错误（但可能不一会再发生类似的情况）。 风格：能引起人们注意但又不会让人感到厌烦的风格，以免你在解决其他问题时没空来处理这些错误。与 Error 的风格不同，我使用黄色的文本来表示 Warn。 例子： 连接关闭，在 2 秒后重新连接 无法找到”logging.conf”[在配置文件中指定的]，回到默认配置 30 秒后尝试连接超时 出现 FileVersionTooOldException 异常，回到 Version12Parser Info通知用户一个操作或状态发生了变化。别紧张，继续手头的活。 Info 可以说是（一般的非技术）用户可以接触到的最“啰嗦”的日志级别。如果有人把它们大声念给你听，你也不会介意，这是你最乐于见到的日志记录。它不会包含很多技术细节，可能只包含普通用户会关注的信息（比如文件名等）。 风格：可以和背景颜色区分开来，我使用白色文本。 例子： 代理初始化完毕 加载存档”yeti02” 进入高速模式 当前目录是”/tmp” 上行线路已建立 渲染完成，耗时 42.999 秒 Debug如果你能读懂这个级别的日志，那么你离程序已经很近了。 这就是为什么你需要保存日志文件，在修复 bug 时需要这些日志。这是开发人员最关心的内容。 在转储程序运行流程和其他技术问题时，应该使用 Debug 级别的日志。除非日志太多了（在这种情况下使用 Trace 级别更合适）或者更适合使用更高级别的日志，否则 Debug 日志是非常值得保留的，毕竟是你自己在代码中记录这些日志的。如果和其他的 Debug 或更高级别的消息重叠，而且没有包含更多的信息，那么可以考虑将其删除。 风格：可以很容易就忽略的风格。我使用浅灰色或米黄色文本，也就是我的终端的默认文本颜色。 例子： 从”/etc/octarine/octarine.conf”读取配置 使用”/home/aib/.octarinerc”覆盖配置 分析完成，创建图… 作为”user”连接到服务器:4242 发送两条消息 渲染时故障： Foo 0.990 秒 Bar 42.009 秒 Trace这些技术细节可能与程序不是很相干，除非你正好需要它们。 Trace 的信息是更加具体的调试信息，你可能并不想看到它（除非你向保存日志的人卖硬盘的时候需要）。它会包含比如说调用了什么函数（函数名），或是和客户端交换了什么网络包等内容。它善于找到一些低级错误，但通常你可以在调试消息中缩小范围，找到问题。 大多数 Trace 消息包含了你已经知道的信息（Debug 消息中说了是“登录”，所以这肯定是登录相关的数据包），所以可能对你不是很有用，除非你的假设是错误的。（”它会不会是登出消息？！“、”这里应该调用 foo。为什么 foo 的 Trace 信息没有打印出来呢？”） 风格：使用比 Debug 消息更加不显眼的风格。我使用深灰色，通常用来表示禁用的颜色。 例子： 调用参数 (“baz”, “bar”, 42) 函数”foo” -&gt;GET / HTTP/1.1\nHost: localhost\n\n 收到: &lt;?xml version=”1.0” encoding=”UTF-8” ?&gt;\n\n […] Fatal发生了一个致命错误，我们要退出了。祝你好运！ 它应该比 Error 更严重，但使用它的频率比 Trace 还少，所以我把它放在文章的最后。顾名思义，致命错误表示这种情况的发生将导致程序无法继续运行。因此，给它们专门设置一个级别没什么意义。但是致命的错误也可能是常见和可恢复的（比如重启就能解决），因此仍然值得一提。 风格：如果你想不出其他样式的话，可以选择比 Error 更显眼的风格。我使用紫色文本，从远处看的话和 Error 的红色文本相近，但近看就不一样。 例子： 内存不足 无法分配 65536 字节的磁盘空间 许可过期，切换到免费软件模式 外部例子 任何成熟的日志记录 API 或库都应该有自己的日志级别（可能支持用户自定义）。以下是广泛使用的库，仅供参考： Linux 的 printk Python 的 logging Java 的 java.util.logging.Level或 log4j 的 org.apache.log4j.Level JavaScript 的 console.level 调用(WHATWG 或 Node.js 的 Console API 规范) NLog 的日志级别 英文原文]]></content>
      <categories>
        <category>日志</category>
      </categories>
      <tags>
        <tag>日志</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DevOps 漫谈：选择基础设施部署和配置管理工具]]></title>
    <url>%2F2018%2F10%2F01%2FDevOps%20%E6%BC%AB%E8%B0%88%EF%BC%9A%E9%80%89%E6%8B%A9%E5%9F%BA%E7%A1%80%E8%AE%BE%E6%96%BD%E9%83%A8%E7%BD%B2%E5%92%8C%E9%85%8D%E7%BD%AE%E7%AE%A1%E7%90%86%E5%B7%A5%E5%85%B7%2F</url>
    <content type="text"><![CDATA[DevOps 漫谈：选择基础设施部署和配置管理工具 Ansible vs. Chef vs. Fabric vs. Puppet vs. SaltStack 在生产环境中工作，常常意味着连续部署和遍布全球的基础设施。如果您的基础架构是分散式和基于云的，同时您需要在大量服务器上频繁部署大量类似的服务，如果此时有一种方法可以自动配置和维护以上所有内容将是您的一大福音。 部署管理工具（Deployment management tools）和配置管理工具 (configuration management tools) 是为此目的而设计的。它们使您能够使用“食谱”（recipes），“剧本” (playbooks)，模板 (templates) 或任何术语来简化整个环境中的自动化和编排，以提供标准、一致的部署。 在选择工具时请记住几点注意事项。首先是了解工具的模型。有些工具采用主控模式（master-client model），它有一个集中控制点（master）与分布式部署的服务器进行通信，其他部分则可以在更本地的层面上运行。另一个考虑因素是你的环境构成。有些工具是采用不同的语言编写的，对于特定的操作系统或设置可能会有所不同。确保您选择的工具与您的环境完美配合，充分利用团队的特定技能可以为您节省很多麻烦。 1. Ansible Ansible 是一种开源工具，用于以可重复的方式将应用程序部署到远程节点和配置服务器。它为您提供了基于推送模型（push model ）推送多层应用程序和应用程序组件的通用框架，您也可以根据需要将其设置为 master-client 模式。 Ansible 建立在可用于各种系统上部署应用程序的剧本(playbook)。 何时使用它 ：对您来说最重要的是快速，轻松地启动和运行，并且您不想在远程节点或受管服务器上安装代理（Agent）。如果您的需求重点更多地放在系统管理员身上，专注于精简和快速，请考虑 Ansible 。 价格：免费的开源版本，Ansible Tower 的付费套餐每年 5000 美元（最多可容纳 100 个节点）。 赞成的理由： 基于 SSH , 不需要在远程节点安装任何代理 学习曲线平缓、使用 YAML Playbook 结构简单，结构清晰 具有变量注册功能，可以使前一个任务作为后一个任务的变量 代码库精简 反对的理由： 相较其他编程语言的工具功能有限。 通过 DSL 实现其逻辑，这意味着需要经常查看文档直到您学会为止 即使是最基本功能也需要变量注册，这可能使简单任务变得复杂 内省（Introspection）很差。例如很难在剧本中看到变量的值 输入，输出和配置文件格式之间缺乏一致性 性能存在一定瓶颈 2. Chef Chef 是一个配置管理开源工具，用户群专注面向开发者。Chef 作为 master-client 模式运行，需要一个单独的工作站来控制 master 。Chef 基于 Ruby 开发，纯 Ruby 可以支持大多数元素。Chef 的设计是透明的，并遵循给定的指示，这意味着你必须确保你的指示是清楚的。 何时使用它：在考虑 Chef 之前，需要确保你熟悉 Git ，因为它需要配置 Git ，你必须编写 Ruby 代码。Chef 适合以开发为中心（development-focused ）的团队和环境。对于寻求更成熟异构环境解决方案的企业来说，这是一件好事。 价格：提供免费的开源版本，标准版和高级版计划以每年节点为单位定价。 Chef Automate 的价格为每个节点 137 美元，或者采用 Hosted Chef 每个节点每年节省 72 美元。 赞成的理由： 丰富的模块和配置配方(recipes) 代码驱动的方法为您提供更多的配置控制和灵活性 以 Git 为中心赋予 Chef 强大的版本控制功能 ‘Knife’工具（使用 SSH 从工作站部署代理）减轻了安装负担 反对的理由： 如果您还不熟悉 Ruby 和面向过程编程，学习曲线会非常陡峭 这不是一个简单的工具，可能需要维护大量的代码库和复杂的环境 不支持推送功能 3. Fabric Fabric 是一个基于 Python 的应用程序部署工具。Fabric 的主要用途是在多个远程系统上运行任务，但它也可以通过插件的方式进行扩展，以提供更高级的功能。 Fabric 将配置您的操作系统，进行操作系统 / 服务器管理，自动化部署您的应用。 何时使用它：如果您刚刚开始进入部署自动化领域，Fabric 是一个良好的开端。如果您的环境至少包含一点 Python，它都会有所帮助。 价格：免费 赞成的理由： 擅长部署以任何语言编写的应用程序。它不依赖于系统架构，而是依赖 - 于操作系统和软件包管理器 相比其他工具更简单，更易于部署 与 SSH 进行了广泛的整合，以实现基于脚本的流水线 反对的理由： Fabric 是单点设置（通常是运行部署的机器） 使用 PUSH 模型，因此不如其他工具那样适合流水线部署模型 虽然它是用于在大多数语言中部署应用程序的绝佳工具，但它确实需要运行 Python，所以您的环境中必须至少有一个适用于 Fabric 的 Python 环境 4. Puppet Puppet 长期依赖是全面配置管理领域的标准工具之一。Puppet 是一个开源工具，但是考虑到它已经存在了多长时间，它已经在一些最大和最苛刻的环境中进行了部署和验证。 Puppet 基于 Ruby 开发，但使用更接近 JSON 的领域专用语言（Domain Specific Language，DSL）。Puppet 采用 master-client 模式运行，并采用模型驱动 (model-driven) 的方法。 Puppet 将工作设计为一系列依赖关系列表，根据您的设置，这可以使事情变得更容易或更容易混淆。 何时使用它： 如果稳定性和成熟度对您来说是最关键的因素，Puppet 是一个不错的选择。对于具有异构环境的大型企业和涉及多种技能范围的 DevOps 团队而言而言，这是一件好事。 价格：Puppet 分为免费的开源版本和付费的企业版本，商业版每年每个节点 120 美元（提供批量折扣）。 赞成的理由： 通过 Puppet Labs 建立了完善的支持社区 具有最成熟的接口，几乎可以在所有操作系统上运行 安装和初始设置简单 最完整的 Web UI 强大的报表功能 反对的理由： 对于更高级的任务，您需要使用基于 Ruby 的 CLI（这意味着您必须了解 Ruby） 纯 Ruby 版本的支持正在缩减（而不是那些使用 Puppet 定制 DSL 的版本） Puppet 代码库可能会变得庞大，新人需要更多的帮助 与代码驱动方法相比，模型驱动方法意味着用户的控制更少 5. Saltstack SaltStack（或 Salt）是一种基于 CLI 的工具，可以将其设置为 master-client 模型或非集中模型。 Salt 基于 Python 开发，提供了 PUSH 和 SSH 两种方法与客户端通讯。 Salt 允许对客户端和配置模板进行分组，以简化对环境的控制。 何时使用它： 如果可扩展性和弹性是一个大问题，则 Salt 是一个不错的选择。对系统管理员来说，Salt 提供的可用性非常重要。 价格：提供免费的开源版本，以及基于年度 / 节点订阅的 SaltStack Enterprise 版本。具体的价格没有在他们的网站上列出，据说每个节点每年的起步价为 150 美元。 赞成的理由： 一旦你渡过了入门阶段，就可以简单地组织和使用 DSL 功能丰富，不需要逻辑和状态 输入，输出和配置非常一致，全部所有 YAML （一个可读性高，用来表达数据序列的格式） 内省 (Introspection) 非常好。很容易看到 Salt 内部发生了什么 强大的社区 很高的可扩展性和灵活性 反对的理由： 对于新用户来说，非常难以配置，学习曲线陡峭 在入门级别而言，文档很难理解 Web UI 比同领域的其他工具更新、更轻量 对非 Linux 操作系统没有很好的支持 比较 您使用的配置管理或部署自动化工具取决于您的环境需求和偏好。 Chef 和 Puppet 是一些较老的、更成熟的选项，它们适用于那些重视成熟性和稳定性而非简单性的大型企业和环境。 Ansible 和 SaltStack 是寻求快速和简约解决方案人士的理想选择，同时在不需要支持某些特殊功能或具有大量操作系统的环境中工作。Fabric 对于小型环境和那些正在寻求更低门槛和入门级解决方案的人来说是一个很好的工具。 WordBookYAML YAML 语言（发音 /ˈjæməl/ ）实质上是一种通用的数据串行化格式。基本语法规则如下: 大小写敏感 使用缩进表示层级关系 缩进时不允许使用 Tab 键，只允许使用空格。缩进的空格数目不重要，只要相同层级的元素左侧对齐即可 表示行注释YAML 支持的数据结构： 对象：键值对的集合，又称为映射（mapping）/ 哈希（hashes） / 字典（dictionary）数组：一组按次序排列的值，又称为序列（sequence） / 列表（list）纯量（scalars）：单个的、不可再分的值 YAML 语言教程 DSL|Domain Specific Language, 领域专用语言 DSL 的目标受众是非程序员，业务员或者最终用户。DSL 最大的设计原则就是简单，通过简化语言中的元素，降低使用者的负担；无论是 Regex、SQL 还是 HTML 以及 CSS，其说明文档往往只有几页，非常易于学习和掌握。但是，由此带来的问题就是，DSL 中缺乏抽象的概念，比如：模块化、变量以及方法等。 领域专用语言 (DSL) 迷思 谈谈 DSL 以及 DSL 的应用（以 CocoaPods 为例） 扩展阅读： DevOps 漫谈系列 Kanban 看板管理实践 DevOps 漫谈：基础设施部署和配置管理 Linux 容器安全的十重境界 工程师的自我修养：全英文技术学习实践 DevOps 实践的本质是文化 学习力－团队生命之根 带领团队翻译书籍 Don’t make me think 凡是被很多人不断重复的好习惯，要将其自动化整合到工具]]></content>
      <categories>
        <category>Deployment</category>
      </categories>
      <tags>
        <tag>Deployment</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Git 使用教程]]></title>
    <url>%2F2018%2F09%2F29%2FGit%E4%BD%BF%E7%94%A8%E6%95%99%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[一、Git 是什么？Git 是目前世界上最先进的分布式版本控制系统。工作原理 / 流程： Workspace：工作区 Index / Stage：暂存区 Repository：仓库区（或本地仓库）Remote：远程仓库 二、SVN 与 Git 的最主要的区别？SVN 是集中式版本控制系统，版本库是集中放在中央服务器的，而干活的时候，用的都是自己的电脑，所以首先要从中央服务器哪里得到最新的版本，然后干活，干完后，需要把自己做完的活推送到中央服务器。集中式版本控制系统是必须联网才能工作，如果在局域网还可以，带宽够大，速度够快，如果在互联网下，如果网速慢的话，就纳闷了。 Git 是分布式版本控制系统，那么它就没有中央服务器的，每个人的电脑就是一个完整的版本库，这样，工作的时候就不需要联网了，因为版本都是在自己的电脑上。既然每个人的电脑都有一个完整的版本库，那多个人如何协作呢？比如说自己在电脑上改了文件 A，其他人也在电脑上改了文件 A，这时，你们两之间只需把各自的修改推送给对方，就可以互相看到对方的修改了。 三、在 windows 上如何安装 Git？msysgit 是 windows 版的 Git, 如下： 需要从网上下载一个，然后进行默认安装即可。安装完成后，在开始菜单里面找到 “Git --&gt; Git Bash“, 如下： 会弹出一个类似的命令窗口的东西，就说明 Git 安装成功。如下： 安装完成后，还需要最后一步设置，在命令行输入如下： 因为 Git 是分布式版本控制系统，所以需要填写用户名和邮箱作为一个标识。 注意：git config --global 参数，有了这个参数，表示你这台机器上所有的 Git 仓库都会使用这个配置，当然你也可以对某个仓库指定的不同的用户名和邮箱。 四、如何操作？一：创建版本库。什么是版本库？版本库又名仓库，英文名repository, 你可以简单的理解一个目录，这个目录里面的所有文件都可以被 Git 管理起来，每个文件的修改，删除，Git 都能跟踪，以便任何时刻都可以追踪历史，或者在将来某个时刻还可以将文件”还原”。 所以创建一个版本库也非常简单，如下我是 D 盘 –&gt; www 下 目录下新建一个 testgit 版本库。 pwd 命令是用于显示当前的目录。 通过命令 git init 把这个目录变成 git 可以管理的仓库，如下： 这时候你当前 testgit 目录下会多了一个.git 的目录，这个目录是 Git 来跟踪管理版本的，没事千万不要手动乱改这个目录里面的文件，否则，会把 git 仓库给破坏了。如下： 下面先看下 demo 如下演示： 我在版本库 testgit 目录下新建一个记事本文件 readme.txt 内容如下：11111111 第一步：使用命令 git add readme.txt 添加到暂存区里面去。如下： 如果和上面一样，没有任何提示，说明已经添加成功了。 第二步：用命令 git commit 告诉 Git，把文件提交到仓库。 现在我们已经提交了一个 readme.txt 文件了，我们下面可以通过命令 git status 来查看是否还有文件未提交，如下： 说明没有任何文件未提交，但是我现在继续来改下 readme.txt 内容，比如我在下面添加一行 2222222222 内容，继续使用 git status 来查看下结果，如下： 上面的命令告诉我们 readme.txt 文件已被修改，但是未被提交的修改。 把文件添加到版本库中。 首先要明确下，所有的版本控制系统，只能跟踪文本文件的改动，比如 txt 文件，网页，所有程序的代码等，Git 也不列外，版本控制系统可以告诉你每次的改动，但是图片，视频这些二进制文件，虽能也能由版本控制系统管理，但没法跟踪文件的变化，只能把二进制文件每次改动串起来，也就是知道图片从 1kb 变成 2kb，但是到底改了啥，版本控制也不知道。 接下来我想看下 readme.txt 文件到底改了什么内容，如何查看呢？可以使用如下命令： git diff readme.txt 如下： 如上可以看到，readme.txt 文件内容从一行 11111111 改成 二行 添加了一行 22222222 内容。 知道了对 readme.txt 文件做了什么修改后，我们可以放心的提交到仓库了，提交修改和提交文件是一样的 2 步(第一步是 git add 第二步是：git commit)。 如下： 二、版本回退：如上，我们已经学会了修改文件，现在我继续对 readme.txt 文件进行修改，再增加一行 内容为 33333333333333. 继续执行命令如下： 现在我已经对 readme.txt 文件做了三次修改了，那么我现在想查看下历史记录，如何查呢？我们现在可以使用命令 git log 演示如下所示： git log 命令显示从最近到最远的显示日志，我们可以看到最近三次提交，最近的一次是, 增加内容为 333333. 上一次是添加内容 222222，第一次默认是 111111. 如果嫌上面显示的信息太多的话，我们可以使用命令 git log –pretty=oneline 演示如下： 现在我想使用版本回退操作，我想把当前的版本回退到上一个版本，要使用什么命令呢？可以使用如下 2 种命令，第一种是：git reset –hard HEAD^ 那么如果要回退到上上个版本只需把 HEAD^ 改成 HEAD^^ 以此类推。那如果要回退到前 100 个版本的话，使用上面的方法肯定不方便，我们可以使用下面的简便命令操作：git reset –hard HEAD~100 即可。未回退之前的 readme.txt 内容如下： 如果想回退到上一个版本的命令如下操作： 再来查看下 readme.txt 内容如下：通过命令 cat readme.txt 查看 可以看到，内容已经回退到上一个版本了。我们可以继续使用 git log 来查看下历史记录信息，如下： 我们看到 增加 333333 内容我们没有看到了，但是现在我想回退到最新的版本，如：有 333333 的内容要如何恢复呢？我们可以通过版本号回退，使用命令方法如下： git reset –hard 版本号 ，但是现在的问题假如我已经关掉过一次命令行或者 333 内容的版本号我并不知道呢？要如何知道增加 3333 内容的版本号呢？可以通过如下命令即可获取到版本号：git reflog 演示如下： 通过上面的显示我们可以知道，增加内容 3333 的版本号是 6fcfc89. 我们现在可以命令 git reset –hard 6fcfc89 来恢复了。演示如下： 可以看到 目前已经是最新的版本了。 三：理解工作区与暂存区的区别？工作区：就是你在电脑上看到的目录，比如目录下 testgit 里的文件(.git 隐藏目录版本库除外)。或者以后需要再新建的目录文件等等都属于工作区范畴。版本库(Repository)：工作区有一个隐藏目录.git, 这个不属于工作区，这是版本库。其中版本库里面存了很多东西，其中最重要的就是 stage(暂存区)，还有 Git 为我们自动创建了第一个分支 master, 以及指向 master 的一个指针 HEAD。 我们前面说过使用 Git 提交文件到版本库有两步： 第一步：是使用 git add 把文件添加进去，实际上就是把文件添加到暂存区。 第二步：使用 git commit 提交更改，实际上就是把暂存区的所有内容提交到当前分支上。 我们继续使用 demo 来演示下： 我们在 readme.txt 再添加一行内容为 4444444，接着在目录下新建一个文件为 test.txt 内容为 test，我们先用命令 git status 来查看下状态，如下： 现在我们先使用 git add 命令把 2 个文件都添加到暂存区中，再使用 git status 来查看下状态，如下： 接着我们可以使用 git commit 一次性提交到分支上，如下： 四、Git 撤销修改和删除文件操作。一：撤销修改：比如我现在在 readme.txt 文件里面增加一行 内容为 555555555555，我们先通过命令查看如下： 在我未提交之前，我发现添加 5555555555555 内容有误，所以我得马上恢复以前的版本，现在我可以有如下几种方法可以做修改： 第一：如果我知道要删掉那些内容的话，直接手动更改去掉那些需要的文件，然后 add 添加到暂存区，最后 commit 掉。 第二：我可以按以前的方法直接恢复到上一个版本。使用 git reset –hard HEAD^ 但是现在我不想使用上面的 2 种方法，我想直接想使用撤销命令该如何操作呢？首先在做撤销之前，我们可以先用 git status 查看下当前的状态。如下所示： 可以发现，Git 会告诉你，git checkout – file 可以丢弃工作区的修改，如下命令：git checkout – readme.txt, 如下所示： 命令 git checkout –readme.txt 意思就是，把 readme.txt 文件在工作区做的修改全部撤销，这里有 2 种情况，如下： 1.readme.txt 自动修改后，还没有放到暂存区，使用 撤销修改就回到和版本库一模一样的状态。2. 另外一种是 readme.txt 已经放入暂存区了，接着又作了修改，撤销修改就回到添加暂存区后的状态。对于第二种情况，我想我们继续做 demo 来看下，假如现在我对 readme.txt 添加一行 内容为 6666666666666，我 git add 增加到暂存区后，接着添加内容 7777777，我想通过撤销命令让其回到暂存区后的状态。如下所示： 注意：命令 git checkout – readme.txt 中的 – 很重要，如果没有 – 的话，那么命令变成创建分支了。 二：删除文件。假如我现在版本库 testgit 目录添加一个文件 b.txt, 然后提交。如下： 如上：一般情况下，可以直接在文件目录中把文件删了，或者使用如上 rm 命令：rm b.txt ，如果我想彻底从版本库中删掉了此文件的话，可以再执行 commit 命令 提交掉，现在目录是这样的， 只要没有 commit 之前，如果我想在版本库中恢复此文件如何操作呢？ 可以使用如下命令 git checkout – b.txt，如下所示： 再来看看我们 testgit 目录，添加了 3 个文件了。如下所示： 五、远程仓库。在了解之前，先注册 github 账号，由于你的本地 Git 仓库和 github 仓库之间的传输是通过 SSH 加密的，所以需要一点设置：第一步：创建 SSH Key。在用户主目录下，看看有没有.ssh 目录，如果有，再看看这个目录下有没有 id_rsa 和 id_rsa.pub 这两个文件，如果有的话，直接跳过此如下命令，如果没有的话，打开命令行，输入如下命令： ssh-keygen -t rsa –C “youremail@example.com”, 由于我本地此前运行过一次，所以本地有，如下所示： id_rsa 是私钥，不能泄露出去，id_rsa.pub 是公钥，可以放心地告诉任何人。 第二步：登录 github, 打开” settings”中的 SSH Keys 页面，然后点击“Add SSH Key”, 填上任意 title，在 Key 文本框里黏贴 id_rsa.pub 文件的内容。 点击 Add Key，你就应该可以看到已经添加的 key。 如何添加远程库？现在的情景是：我们已经在本地创建了一个 Git 仓库后，又想在 github 创建一个 Git 仓库，并且希望这两个仓库进行远程同步，这样 github 的仓库可以作为备份，又可以其他人通过该仓库来协作。 首先，登录 github 上，然后在右上角找到“create a new repo”创建一个新的仓库。如下： 在 Repository name 填入 testgit，其他保持默认设置，点击“Create repository”按钮，就成功地创建了一个新的 Git 仓库： 目前，在 GitHub 上的这个 testgit 仓库还是空的，GitHub 告诉我们，可以从这个仓库克隆出新的仓库，也可以把一个已有的本地仓库与之关联，然后，把本地仓库的内容推送到 GitHub 仓库。 现在，我们根据 GitHub 的提示，在本地的 testgit 仓库下运行命令： git remote add origin https://github.com/tugenhua0707/testgit.git所有的如下： 把本地库的内容推送到远程，使用 git push 命令，实际上是把当前分支 master 推送到远程。 由于远程库是空的，我们第一次推送 master 分支时，加上了 –u 参数，Git 不但会把本地的 master 分支内容推送的远程新的 master 分支，还会把本地的 master 分支和远程的 master 分支关联起来，在以后的推送或者拉取时就可以简化命令。推送成功后，可以立刻在 github 页面中看到远程库的内容已经和本地一模一样了，上面的要输入 github 的用户名和密码如下所示： 从现在起，只要本地作了提交，就可以通过如下命令： git push origin master 把本地 master 分支的最新修改推送到 github 上了，现在你就拥有了真正的分布式版本库了。 如何从远程库克隆？ 上面我们了解了先有本地库，后有远程库时候，如何关联远程库。 现在我们想，假如远程库有新的内容了，我想克隆到本地来 如何克隆呢？ 首先，登录 github，创建一个新的仓库，名字叫 testgit2. 如下： 如下，我们看到： 现在，远程库已经准备好了，下一步是使用命令 git clone 克隆一个本地库了。如下所示： 接着在我本地目录下 生成 testgit2 目录了，如下所示： 六、创建与合并分支。在版本回填退里，你已经知道，每次提交，Git 都把它们串成一条时间线，这条时间线就是一个分支。截止到目前，只有一条时间线，在 Git 里，这个分支叫主分支，即 master 分支。HEAD 严格来说不是指向提交，而是指向 master，master 才是指向提交的，所以，HEAD 指向的就是当前分支。 首先，我们来创建 dev 分支，然后切换到 dev 分支上。如下操作： git checkout 命令加上 –b 参数表示创建并切换，相当于如下 2 条命令 git branch dev git checkout dev git branch 查看分支，会列出所有的分支，当前分支前面会添加一个星号。然后我们在 dev 分支上继续做 demo，比如我们现在在 readme.txt 再增加一行 7777777777777 首先我们先来查看下 readme.txt 内容，接着添加内容 77777777，如下： 现在 dev 分支工作已完成，现在我们切换到主分支 master 上，继续查看 readme.txt 内容如下： 现在我们可以把 dev 分支上的内容合并到分支 master 上了，可以在 master 分支上，使用如下命令 git merge dev 如下所示： git merge 命令用于合并指定分支到当前分支上，合并后，再查看 readme.txt 内容，可以看到，和 dev 分支最新提交的是完全一样的。 注意到上面的 Fast-forward 信息，Git 告诉我们，这次合并是“快进模式”，也就是直接把 master 指向 dev 的当前提交，所以合并速度非常快。 合并完成后，我们可以接着删除 dev 分支了，操作如下： 总结创建与合并分支命令如下： 查看分支：git branch 创建分支：git branch name 切换分支：git checkout name 创建 + 切换分支：git checkout –b name 合并某分支到当前分支：git merge name 删除分支：git branch –d name 如何解决冲突？下面我们还是一步一步来，先新建一个新分支，比如名字叫 fenzhi1，在 readme.txt 添加一行内容 8888888，然后提交，如下所示： 同样，我们现在切换到 master 分支上来，也在最后一行添加内容，内容为 99999999，如下所示： 现在我们需要在 master 分支上来合并 fenzhi1，如下操作： Git 用 &lt;&lt;&lt;&lt;&lt;&lt;&lt;，=======，&gt;&gt;&gt;&gt;&gt;&gt;&gt; 标记出不同分支的内容，其中 &lt;&lt;&lt;HEAD 是指主分支修改的内容，&gt;&gt;&gt;&gt;&gt;fenzhi1 是指 fenzhi1 上修改的内容，我们可以修改下如下后保存： 如果我想查看分支合并的情况的话，需要使用命令 git log. 命令行演示如下： 3. 分支管理策略。 通常合并分支时，git 一般使用”Fast forward”模式，在这种模式下，删除分支后，会丢掉分支信息，现在我们来使用带参数 –no-ff 来禁用”Fast forward”模式。首先我们来做 demo 演示下： 创建一个 dev 分支。修改 readme.txt 内容。添加到暂存区。切换回主分支(master)。合并 dev 分支，使用命令 git merge –no-ff -m “注释” dev 查看历史记录截图如下： 分支策略：首先 master 主分支应该是非常稳定的，也就是用来发布新版本，一般情况下不允许在上面干活，干活一般情况下在新建的 dev 分支上干活，干完后，比如上要发布，或者说 dev 分支代码稳定后可以合并到主分支 master 上来。 七、bug 分支 在开发中，会经常碰到 bug 问题，那么有了 bug 就需要修复，在 Git 中，分支是很强大的，每个 bug 都可以通过一个临时分支来修复，修复完成后，合并分支，然后将临时的分支删除掉。 比如我在开发中接到一个 404 bug 时候，我们可以创建一个 404 分支来修复它，但是，当前的 dev 分支上的工作还没有提交。比如如下： 并不是我不想提交，而是工作进行到一半时候，我们还无法提交，比如我这个分支 bug 要 2 天完成，但是我 issue-404 bug 需要 5 个小时内完成。怎么办呢？还好，Git 还提供了一个 stash 功能，可以把当前工作现场 ”隐藏起来”，等以后恢复现场后继续工作。如下： 所以现在我可以通过创建 issue-404 分支来修复 bug 了。 首先我们要确定在那个分支上修复 bug，比如我现在是在主分支 master 上来修复的，现在我要在 master 分支上创建一个临时分支，演示如下： 修复完成后，切换到 master 分支上，并完成合并，最后删除 issue-404 分支。演示如下： 现在，我们回到 dev 分支上干活了。 工作区是干净的，那么我们工作现场去哪里呢？我们可以使用命令 git stash list 来查看下。如下： 工作现场还在，Git 把 stash 内容存在某个地方了，但是需要恢复一下，可以使用如下 2 个方法： 1.git stash apply 恢复，恢复后，stash 内容并不删除，你需要使用命令 git stash drop 来删除。2. 另一种方式是使用 git stash pop, 恢复的同时把 stash 内容也删除了。演示如下 八、多人协作。 当你从远程库克隆时候，实际上 Git 自动把本地的 master 分支和远程的 master 分支对应起来了，并且远程库的默认名称是 origin。 要查看远程库的信息 使用 git remote 要查看远程库的详细信息 使用 git remote –v 如下演示： 一：推送分支： 推送分支就是把该分支上所有本地提交到远程库中，推送时，要指定本地分支，这样，Git 就会把该分支推送到远程库对应的远程分支上： 使用命令 git push origin master 比如我现在的 github 上的 readme.txt 代码如下： 本地的 readme.txt 代码如下： 现在我想把本地更新的 readme.txt 代码推送到远程库中，使用命令如下： 我们可以看到如上，推送成功，我们可以继续来截图 github 上的 readme.txt 内容 如下： 可以看到 推送成功了，如果我们现在要推送到其他分支，比如 dev 分支上，我们还是那个命令 git push origin dev 那么一般情况下，那些分支要推送呢？ master 分支是主分支，因此要时刻与远程同步。一些修复 bug 分支不需要推送到远程去，可以先合并到主分支上，然后把主分支 master 推送到远程去。二：抓取分支： 多人协作时，大家都会往 master 分支上推送各自的修改。现在我们可以模拟另外一个同事，可以在另一台电脑上（注意要把 SSH key 添加到 github 上）或者同一台电脑上另外一个目录克隆，新建一个目录名字叫 testgit2 但是我首先要把 dev 分支也要推送到远程去，如下 接着进入 testgit2 目录，进行克隆远程的库到本地来，如下： 现在目录下生成有如下所示： 现在我们的小伙伴要在 dev 分支上做开发，就必须把远程的 origin 的 dev 分支到本地来，于是可以使用命令创建本地 dev 分支： git checkout –b dev origin/dev 现在小伙伴们就可以在 dev 分支上做开发了，开发完成后把 dev 分支推送到远程库时。 如下： 小伙伴们已经向 origin/dev 分支上推送了提交，而我在我的目录文件下也对同样的文件同个地方作了修改，也试图推送到远程库时，如下： 由上面可知：推送失败，因为我的小伙伴最新提交的和我试图推送的有冲突，解决的办法也很简单，上面已经提示我们，先用 git pull 把最新的提交从 origin/dev 抓下来，然后在本地合并，解决冲突，再推送。 git pull 也失败了，原因是没有指定本地 dev 分支与远程 origin/dev 分支的链接，根据提示，设置 dev 和 origin/dev 的链接：如下： 这回 git pull 成功，但是合并有冲突，需要手动解决，解决的方法和分支管理中的 解决冲突完全一样。解决后，提交，再 push：我们可以先来看看 readme.txt 内容了。 现在手动已经解决完了，我接在需要再提交，再 push 到远程库里面去。如下所示： 因此：多人协作工作模式一般是这样的： 1. 首先，可以试图用 git push origin branch-name 推送自己的修改. 2. 如果推送失败，则因为远程分支比你的本地更新早，需要先用 git pull 试图合并。 3. 如果合并有冲突，则需要解决冲突，并在本地提交。再用 git push origin branch-name 推送。 Git 基本常用命令 mkdir： XX (创建一个空目录 XX 指目录名) pwd： 显示当前目录的路径。 git init 把当前的目录变成可以管理的 git 仓库，生成隐藏.git 文件。 git add XX 把 xx 文件添加到暂存区去。 git commit –m “XX” 提交文件 –m 后面的是注释。 git status 查看仓库状态 git diff XX 查看 XX 文件修改了那些内容 git log 查看历史记录 git reset –hard HEAD^ 或者 git reset –hard HEAD~ 回退到上一个版本 (如果想回退到第 100 个版本，使用 git reset –hard HEAD~100) cat XX 查看 XX 文件内容 git reflog 查看历史记录的版本号 id git checkout – XX 把 XX 文件在工作区的修改全部撤销。 git rm XX 删除 XX 文件 git remote add origin https://github.com/tugenhua0707/testgit 关联一个远程库 git push –u(第一次要用 -u 以后不需要) origin master 把当前 master 分支推送到远程库 git clone https://github.com/tugenhua0707/testgit 从远程库中克隆 git checkout –b dev 创建 dev 分支 并切换到 dev 分支上 git branch 查看当前所有的分支 git checkout master 切换回 master 分支 git merge dev 在当前的分支上合并 dev 分支 git branch –d dev 删除 dev 分支 git branch name 创建分支 git stash 把当前的工作隐藏起来 等以后恢复现场后继续工作 git stash list 查看所有被隐藏的文件列表 git stash apply 恢复被隐藏的文件，但是内容不删除 git stash drop 删除文件 git stash pop 恢复文件的同时 也删除文件 git remote 查看远程库的信息 git remote –v 查看远程库的详细信息 git push origin master Git 会把 master 分支推送到远程库对应的远程分支上 参考： www.cnblogs.com/tugenhua0707/p/4050072.html 推 荐 阅 读 推荐几个原创公众号 优雅的使用 ThreadLocal Linux 养成计划（六） 微信扫码登录实战（附代码）]]></content>
      <categories>
        <category>Git</category>
      </categories>
      <tags>
        <tag>Git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kubernetes 集群之路（二）etcd 集群部署]]></title>
    <url>%2F2018%2F09%2F21%2FKubernetes%E9%9B%86%E7%BE%A4%E4%B9%8B%E8%B7%AF%EF%BC%88%E4%BA%8C%EF%BC%89etcd%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2%2F</url>
    <content type="text"><![CDATA[在前面我们生成了所有 kubernetes 相关的 TLS 证书，kubernetes 集群自身所有配置相关信息都存储在 etcd 之中，而 flannel 也将网络子网网段注册到 etcd 之中并为集群中节点的 pod 提供了加入同一局域网的能力。因此接下来我们安装部署 etcd 集群。 因为 flannel 插件也依赖于 etcd 存储信息，所以我们首先需要安装 etcd 集群，使之实现高可用。 在开始之前请确保在上一篇文章中生成的 TLS 证书都分发到需要部署的 所有机器节点 的以下位置： /etc/kubernetes/ssl/etcd.pem /etc/kubernetes/ssl/etcd-key.pem /etc/kubernetes/ssl/ca.pem 部署 etcd我们采用纯二进制安装 etcd, 因此不使用默认的包管理器中的安装文件。在每台需要部署的 etcd 的节点上，通过官方仓库下载你需要的版本的 etcd 二进制安装包： https://github.com/coreos/etcd/releases 下载安装二进制文件 目前最新版本是v3.3.4（截止到我写这篇文章的时候），而 kubernetes v1.10 验证过的版本为3.1.12, 如果没有特殊需求请尽量使用验证版本（如果你是不升级不舒服斯基当我没说）找到对应的系统架构并直接下载: https://github.com/coreos/etcd/releases/download/v3.1.12/etcd-v3.1.12-linux-amd64.tar.gz 在所有需要安装 etcd 节点执行以下命令来安装 etcd 和etcdctl(注意:etcd 目前不支持降级，如果你初始安装版本过高，后续像降级到验证版是比较麻烦的): wget https://github.com/coreos/etcd/releases/download/v3.1.12/etcd-v3.1.12-linux-amd64.tar.gz tar zxvf etcd-v3.1.12-linux-amd64.tar.gz cd etcd-v3.1.12-linux-amd64 sudo mv etcd etcdctl /usr/local/bin/ 配置 systemd unit接着，我们需要编辑对应的 systemd unit service 文件，我们需要新建一个 etcd.service 文件并放置于以下路径：/usr/lib/systemd/system/etcd.service并键入以下内容： [Unit] Description=Etcd Server After=network.target After=network-online.target Wants=network-online.target Documentation=https://github.com/coreos [Service] Type=notify WorkingDirectory=/var/lib/etcd/ EnvironmentFile=-/etc/etcd/etcd.conf ExecStart=/usr/local/bin/etcd \ --name ${ETCD_NAME} \ --cert-file=${ETCD_TRUST_CERT_FILE} \ --key-file=${ETCD_TRUST_CERT_KEY} \ --peer-cert-file=${ETCD_TRUST_CERT_FILE} \ --peer-key-file=${ETCD_TRUST_CERT_KEY} \ --trusted-ca-file=${ETCD_TRUST_CA_FILE} \ --peer-trusted-ca-file=${ETCD_TRUST_CA_FILE} \ --initial-advertise-peer-urls ${ETCD_INITIAL_ADVERTISE_PEER_URLS} \ --listen-peer-urls ${ETCD_LISTEN_PEER_URLS} \ --listen-client-urls ${ETCD_LISTEN_CLIENT_URLS} \ --advertise-client-urls ${ETCD_ADVERTISE_CLIENT_URLS} \ --initial-cluster-token ${ETCD_INITIAL_CLUSTER_TOKEN} \ --initial-cluster ${ETCD_CLUSTER_NODE_LIST} \ --initial-cluster-state new \ --data-dir=${ETCD_DATA_DIR} Restart=on-failure RestartSec=5 LimitNOFILE=65536 [Install] WantedBy=multi-user.target WorkingDirectory: 指定 etcd 的工作目录和数据目录为 /var/lib/etcd，需在启动服务前创建这个目录。 为了保证通信安全，需要指定 etcd 的公私钥 (cert-file 和key-file)、Peers 通信的公私钥和 CA 证书(peer-cert-file、peer-key-file、peer-trusted-ca-file)、客户端的 CA 证书（trusted-ca-file）。 --initial-cluster-state 值为 new 时，--name 的参数值必须位于 --initial-cluster 列表中。 我们将其中一些参数的设置抽取为环境变量，以便于我们修改参数的时候不需要再次 systemctl daemon-reload。 带有 --peer-xxx 前缀的配置为 etcd 与其它 etcd 节点通信的相关配置，不带有的该前缀的则为客户端（例如：etcdctl）与 etcd 节点（作为服务器）通信的相关配置。 对应的，我们在 /etc/etcd/etcd.conf 路径中新建一个 etcd.conf 文件并键入以下内容: # [member] ETCD_NAME=node1 ETCD_DATA_DIR=&quot;/var/lib/etcd&quot; ETCD_LISTEN_PEER_URLS=&quot;https://0.0.0.0:2380&quot; ETCD_LISTEN_CLIENT_URLS=&quot;https://0.0.0.0:2379&quot; ETCD_TRUST_CA_FILE=&quot;/etc/kubernetes/ssl/ca.pem&quot; ETCD_TRUST_CERT_FILE=&quot;/etc/kubernetes/ssl/etcd.pem&quot; ETCD_TRUST_CERT_KEY=&quot;/etc/kubernetes/ssl/etcd-key.pem&quot; #[cluster] ETCD_INITIAL_ADVERTISE_PEER_URLS=&quot;https://10.138.148.161:2380&quot; ETCD_INITIAL_CLUSTER_TOKEN=&quot;etcd-cluster&quot; ETCD_ADVERTISE_CLIENT_URLS=&quot;https://10.138.148.161:2379&quot; ETCD_CLUSTER_NODE_LIST=&quot;node1=https://10.138.148.161:2380,node2=https://10.138.196.180:2380,node3=https://10.138.212.68:2380&quot; 这是节点 IP 为 10.138.148.161 的环境变量配置文件内容，对于其他节点，修改对应的 ETCD_NAME 为对应的 node1、node2、node3，并将 ETCD_INITIAL_ADVERTISE_PEER_URLS 和ETCD_INITIAL_ADVERTISE_PEER_URLS修改为对应的节点的 ip。 此处需要特别说明的是：ETCD_CLUSTER_NODE_LIST中的 ip 必须在生成 etcd TLS 证书时在 etcd-csr.json 中的 hosts 字段中指定（Subject Alternative Name（SAN）），否则可能会得到 (error &quot;remote error: tls: bad certificate&quot;, ServerName &quot;&quot;) 这样的错误。 所有需要加入的节点都需要在 ETCD_CLUSTER_NODE_LIST 中指定，并正确配置其ETCD_NAME。 验证 etcd 安装 在所有的节点上完成了上述两步之后，我们分别执行以下命令来启动 etcd（初始可能会阻塞一段时间）: sudo systemctl daemon-reload sudo systemctl start etcd 如果配置正确，那么上述命令执行结果应该是任何输出的。如果结果有错，请参照上述配置和环境变量文件检查配置。一旦我们顺利启动 etcd 服务，我们还需要正确检查我们的 etcd 集群是否可用，在 etcd 集群 中任一节点中执行以下命令： etcdctl --endpoint https://127.0.0.1:2379 \ --ca-file=/etc/kubernetes/ssl/ca.pem\ --cert-file=/etc/kubernetes/ssl/etcd.pem\ --key-file=/etc/kubernetes/ssl/etcd-key.pem \ cluster-health 在一切正常情况下，你会得到类似如下的输出结果: member 245a74588a3e85d0 is healthy: got healthy result from https://xxx.xxx.xxx.xxx:2379 member 953bacee4a009939 is healthy: got healthy result from https://xxx.xxx.xxx.xxx:2379 member f43a05b0ce1a8ed6 is healthy: got healthy result from https://xxx.xxx.xxx.xxx:2379 cluster is healthy 后记 需要特别说明的是：etcd集群是否和 kubernetes 部署在同样的服务器节点上是 可选的 。也就是说etcd 集群可以脱离 kubernetes 部署的集群而单独部署在其他单独的服务器上，且并不需要和 kubernetes 节点数对应。经过我的实践如果有条件的话请务必： 将 etcd 部署在 kubernetes 的 Node 节点之外负载比较低的服务器节点上。 etcd 的集群数量尽量为奇数，以确保某些情况下部分 etcd 节点挂掉的选举问题。 至此，我们的 etcd 集群已经顺利安装完成。接下来安装 flannel 插件。 参考资料 在 CentOS 上部署 kubernetes 集群]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>K8S</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kubernetes 集群之路（一）TLS 证书配置]]></title>
    <url>%2F2018%2F09%2F20%2FKubernetes%E9%9B%86%E7%BE%A4%E4%B9%8B%E8%B7%AF%EF%BC%88%E4%B8%80%EF%BC%89TLS%E8%AF%81%E4%B9%A6%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[Kubernetes 是 Google 开源的容器化集群管理系统，其提供的应用部署、扩展、服务发现等机制对于微服务化架构应用有着十分重要的作用。 本系列文章基于以下版本来讲述如何使用二进制方式安装 Kubernetes 集群顺便讲述下踩坑的心路历程： Kubernetes version: v1.10 System: CentOS Linux 7 Kernel: Linux 3.10.0 Kubernetes 系统的各个组件需要使用 TLS 证书对其通信加密以及授权认证，所以在部署之前我们需要先生成相关的 TLS 证书以便后续操作能够顺利进行。 在后续安装部署中，将不使用 kube-apiserver 的 HTTP 非安全端口，所有组件都启用 TLS 双向认证通信。因此 TLS 证书配置是在安装配置 Kubernetes 系统中最容易出错和难于排查问题的一步，所以请务必耐心仔细。 在开始前，为了模拟集群节点，我们假定需要在以下三台 Linux 主机上部署 Kubernetes: 10.138.148.161：作为 master 节点 10.138.196.180：作 为 Node节点 10.138.212.68：作为 Node 节点 同一台主机上可以同时部署 master 和 Node 节点相关组件，即同时作为控制节点和工作节点，不过这么做可能导致 master 节点负载过高而失去响应进而导致整个集群出现无法预知的问题。 安装 CFSSL 证书生成工具 我们将使用 Cloudflare 的PKI工具集 cloudflare/cfssl 来生成集群所需要的各种 TLS 证书。 执行以下命令直接下载二进制文件进行安装: wget https://pkg.cfssl.org/R1.2/cfssl_linux-amd64 -O cfssl chmod +x cfssl sudo mv cfssl /usr/local/bin wget https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64 -O cfssljson chmod +x cfssljson sudo mv cfssljson /usr/local/bin wget https://pkg.cfssl.org/R1.2/cfssl-certinfo_linux-amd64 -O cfssl-certinfo chmod +x cfssl-certinfo sudo mv cfssl-certinfo /usr/local/bin export PATH=/usr/local/bin:$PATH 创建 CA 根证书（Certificate Authority）CA（Certificate Authority）是自签名的根证书，用来签名后续创建的其它 TLS 证书；确认 CFSSL 工具安装成功之后，我们先通过 CFSSL 工具来创建模版配置 json 文件: cfssl print-defaults config &gt; config.json cfssl print-defaults csr &gt; csr.json 创建 CA 配置文件 这将生成两个模版 json 文件，后续 CFSSL 将读取 json 文件内容并生成对应的 pem 文件。我们先复制 config.json 为ca-config.json文件并做如下修改: { &quot;signing&quot;: { &quot;default&quot;: {&quot;expiry&quot;: &quot;99999h&quot;}, &quot;profiles&quot;: { &quot;kubernetes&quot;: { &quot;expiry&quot;: &quot;99999h&quot;, &quot;usages&quot;: [ &quot;signing&quot;, &quot;key encipherment&quot;, &quot;server auth&quot;, &quot;client auth&quot; ] } } } } profiles：可以定义多个 profiles，分别指定不同的过期时间、使用场景等参数；后续在签名证书时使用某个特定的 profile。 signing：表示该证书可用于签名 (签发) 其它证书，生成的 ca.pem 证书中 CA=TRUE。 server auth：表示 `client 可以用该 CA（生成的 ca.pem） 对 server 提供的证书进行验证。 client auth：表示 server 可以用该 CA(生成的 ca.pem）对 client 提供的证书进行验证。 创建 CA 证书签名请求 我们复制 csr.json 为ca-csr.json并做以下修改: { &quot;CN&quot;: &quot;kubernetes&quot;, &quot;key&quot;: { &quot;algo&quot;: &quot;rsa&quot;, &quot;size&quot;: 2048 }, &quot;names&quot;: [ { &quot;C&quot;: &quot;CN&quot;, &quot;ST&quot;: &quot;Shanghai&quot;, &quot;L&quot;: &quot;Shanghai&quot;, &quot;O&quot;: &quot;k8s&quot;, &quot;OU&quot;: &quot;System&quot; } ] } CN(Common Name): 后续 kube-apiserver 组件将从证书中提取该字段作为请求的用户名； O(Organtzation): 后续 kube-apiserver 组件将从证书中提取该字段作为请求的用户所属的用户组； 生成 CA 证书和私钥 执行以下命令来生成 CA 证书和私钥： cfssl gencert -initca ca-csr.json | cfssljson -bare ca ls ca* ca.csr ca-csr.json ca-key.pem ca.pem 这样，我们就生成了 CA 证书和私钥了，因为我们需要双向 TLS 认证，所以需要拷贝ca-key.pem 和ca.pem到所有要部署的机器的 /etc/kubernetes/ssl 目录下备用。 创建 kubernetes 组件认证授权证书 因为我们准备部署的 kubernetes 组件是使用 TLS 双向认证 的，包括 kube-apiserver 不打算使用 HTTP 端口，因此，我们需要生成以下的证书以供后续组件部署的时候备用： etcd证书：etcd 集群之间通信加密使用的 TLS 证书。 kube-apiserver证书：配置 kube-apiserver 组件的证书。 kube-controller-manager 证书：用于和 kube-apiserver 通信认证的证书。 kube-scheduler证书：用于和 kube-apiserver 通信认证的证书。 kubelet证书【可选, 非必需】：用于和 kube-apiserver 通信认证的证书，如果使用 TLS Bootstarp 认证方式，将没有必要配置。 kube-proxy证书【可选, 非必需】：用于和 kube-apiserver 通信认证的证书，如果使用 TLS Bootstarp 认证方式，将没有必要配置。 下面我们将逐个创建对应的 TLS 证书，并做相应的简短说明： 创建 etcd 证书：首选我们创建 etcd 证书签名请求 (CSR)，拷贝csr.json 为etcd-csr.json并做以下修改: { &quot;CN&quot;: &quot;etcd&quot;, &quot;hosts&quot;: [ &quot;127.0.0.1&quot;, &quot;10.138.212.68&quot;, &quot;10.138.196.180&quot;, &quot;10.138.148.161&quot;, &quot;master&quot;, &quot;node1&quot;, &quot;node2&quot; ], &quot;key&quot;: { &quot;algo&quot;: &quot;rsa&quot;, &quot;size&quot;: 2048 }, &quot;names&quot;: [ { &quot;C&quot;: &quot;CN&quot;, &quot;ST&quot;: &quot;Shanghai&quot;, &quot;L&quot;: &quot;Shanghai&quot;, &quot;O&quot;: &quot;k8s&quot;, &quot;OU&quot;: &quot;System&quot; } ] } 此处需要指定 host 字段的值，该值为所有需要部署 etcd 节点的 ip 域名 或者 hostname，etcd 需要使用Subject Alternative Name（SAN） 来校验集群以及防止滥用。如果你不清楚应该使用哪个 ip，默认情况下使用 ip a 查看 eth0 即可。此处指定的 ip 与后续指定的 etcd 的systemd配置 initial-cluster 相关。 相关阅读: Option to accept TLS client certificates even if they lack correct Subject Alternative Names 生成 etcd 证书和私钥： cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes etcd-csr.json | cfssljson -bare etcd ls etcd* etcd.csr etcd-csr.json etcd-key.pem etcd.pem 将生成的 etch-key.pem 和etcd.pem拷贝到所有需要部署 etcd 集群 的服务器 /etc/etcd/ssl 目录下备用。 创建 kube-apiserver 证书 创建 kube-apiserver 证书签名请求配置文件，拷贝 csr.json 为kubernetes-csr.json并做以下修改： { &quot;CN&quot;: &quot;kubernetes&quot;, &quot;hosts&quot;: [ &quot;127.0.0.1&quot;, &quot;10.138.212.68&quot;, &quot;10.138.196.180&quot;, &quot;10.138.148.161&quot;, &quot;10.254.0.1&quot;, &quot;kubernetes&quot;, &quot;kubernetes.default&quot;, &quot;kubernetes.default.svc&quot;, &quot;kubernetes.default.svc.cluster&quot;, &quot;kubernetes.default.svc.cluster.local&quot; ], &quot;key&quot;: { &quot;algo&quot;: &quot;rsa&quot;, &quot;size&quot;: 2048 }, &quot;names&quot;: [ { &quot;C&quot;: &quot;CN&quot;, &quot;ST&quot;: &quot;Shanghai&quot;, &quot;L&quot;: &quot;Shanghai&quot;, &quot;O&quot;: &quot;k8s&quot;, &quot;OU&quot;: &quot;System&quot; } ] } 此处指定了 host 字段 来表示授权使用该证书的 ip 或域名 列表，因此上述配置文件指定了要部署的 kubernetes 三台服务器 ip（实际上只需要指定打算部署 master 节点的 ip 即可）以及 kube-apiserver 注册的名为 kubernetes 服务的服务 ip（一般默认为后续配置 kube-apiserve 组件的时候指定的 —service-cluster-ip-range 网段的第一个 ip。）如果你不清楚怎么操作，可以留空 host 字段。 如果你指定了 host 字段，这里如果有 VIP 的，也是需要填写的。 生成 kube-apiserver 证书和私钥： cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes kubernetes-csr.json | cfssljson -bare apiserver ls apiserver* apiserver.csr apiserver-key.pem apiserver.pem 我们将该证书拷贝到需要部署到 master 节点 上的 /etc/kubernetes/ssl 上备用。 因为我们 master 节点的组件之间的通信使用 非 HTTP的安全端口，所以同样也需要 TLS 认证 授权，因此我们也需要配置 kube-controller-manager 和kube-scheduler的证书来供这两个组件访问kube-apiserver. 如果你的集群 master 节点组件使用 HTTP 非安全端口通信，那么可以不需要配置这两个证书。 创建 kube-controller-manager 证书 复制 car.json 为kube-controller-manager-csr.json并做以下修改： { &quot;CN&quot;: &quot;system:kube-controller-manager&quot;, &quot;hosts&quot;: [], &quot;key&quot;: { &quot;algo&quot;: &quot;rsa&quot;, &quot;size&quot;: 2048 }, &quot;names&quot;: [ { &quot;C&quot;: &quot;CN&quot;, &quot;ST&quot;: &quot;Shanghai&quot;, &quot;L&quot;: &quot;Shanghai&quot;, &quot;O&quot;: &quot;k8s&quot;, &quot;OU&quot;: &quot;System&quot; } ] } 上述的配置中，kube-apiserver将提取 CN 作为客户端组件 (kube-controller-manager) 的用户名 (system:kube-controller-manager)，kube-apiserver 预定义的 RBAC 使用 ClusterRoleBinding system:kube-controller-manager 将用户 system:kube-controller-manager 与ClusterRole system:kube-controller-manager绑定。 生成 kube-controller-manager 证书和私钥： cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes kube-controller-manager-csr.json | cfssljson -bare controller-manager ls controller-manager* controller-manager.csr controller-manager-key.pem controller-manager.pem 将证书拷贝到需要部署 kube-controller-manager 的 master 节点 /etc/kubernetes/ssl 上备用。 创建 kube-scheduler` 证书 与kube-controller-manager一样，kube-scheduler同样也需要 TLS 证书来访问 kube-apiserver。此处不再赘述。直接上kube-scheduler-csr.json 文件内容： { &quot;CN&quot;: &quot;system:kube-scheduler&quot;, &quot;hosts&quot;: [], &quot;key&quot;: { &quot;algo&quot;: &quot;rsa&quot;, &quot;size&quot;: 2048 }, &quot;names&quot;: [ { &quot;C&quot;: &quot;CN&quot;, &quot;ST&quot;: &quot;Shanghai&quot;, &quot;L&quot;: &quot;Shanghai&quot;, &quot;O&quot;: &quot;k8s&quot;, &quot;OU&quot;: &quot;System&quot; } ] } kube-scheduler 将提取 CN 作为客户端的用户名, 这里是 system:kube-scheduler。 kube-apiserver 预定义的 RBAC 使用的 ClusterRoleBindingssystem:kube-scheduler 将用户system:kube-scheduler 与ClusterRole system:kube-scheduler 绑定。 生成 kube-scheduler 证书以及私钥： cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes kube-scheduler-csr.json | cfssljson -bare scheduler ls scheduler* scheduler.csr scheduler-key.pem scheduler.pem 将证书拷贝到需要部署 kube-scheduler 的master 节点 /etc/kubernetes/ssl 上备用。 至此，master节点上的证书生成就全部完成了，接下来是生成 worker 节点 的证书，需要注意的是：生成 worker 证书是可选的，如果你使用 TLS Bootstarpping 那么你可以跳过以下步骤 worker 证书生成工作。直接转到部署的实际操作环节。关于 TLS 证书 和TLS Bootstarpping 认证 方式的区别，后续考虑单独写一遍文章展开来讲。 创建 kubelet 证书 拷贝 car.json 为kubelet-csr.json并做以下修改: { &quot;CN&quot;: &quot;system:node:node&quot;, &quot;hosts&quot;: [], &quot;key&quot;: { &quot;algo&quot;: &quot;rsa&quot;, &quot;size&quot;: 2048 }, &quot;names&quot;: [ { &quot;C&quot;: &quot;CN&quot;, &quot;ST&quot;: &quot;Shanghai&quot;, &quot;L&quot;: &quot;Shanghai&quot;, &quot;O&quot;: &quot;system:nodes&quot;, &quot;OU&quot;: &quot;System&quot; } ] } O为用户组，kubernetes RBAC定义了 ClusterRoleBinding 将Group system:nodes和 CLusterRole system:node 关联起来。 注意: 在 kubernetes v1.8+ 以上版本，将不会自动创建binding, 因此我们后续需要手动创建绑定关系。 生成 kubelet 证书和私钥： cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes kubelet-csr.json | cfssljson -bare kubelet ls kubelet* kubelet.csr kubelet-csr.json kubelet-key.pem kubelet.pem 将生成的证书和秘钥拷贝到所有需要部署的 worker 节点上的 /etc/kubernetes/ssl 下备用。 创建 kube-proxy 证书 拷贝 car.json 为kube-proxy-csr.json并做以下修改: { &quot;CN&quot;: &quot;system:kube-proxy&quot;, &quot;hosts&quot;: [], &quot;key&quot;: { &quot;algo&quot;: &quot;rsa&quot;, &quot;size&quot;: 2048 }, &quot;names&quot;: [ { &quot;C&quot;: &quot;CN&quot;, &quot;ST&quot;: &quot;Shanghai&quot;, &quot;L&quot;: &quot;Shanghai&quot;, &quot;O&quot;: &quot;k8s&quot;, &quot;OU&quot;: &quot;System&quot; } ] } CN 指定该证书的 User 为 system:kube-proxy。Kubernetes RBAC 定义了 ClusterRoleBinding 将 system:kube-proxy 用户与 system:node-proxier 角色绑定。system:node-proxier 具有 kube-proxy 组件访问 ApiServer 的相关权限。 生成 kube-proxy 证书和私钥： cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes kube-proxy-csr.json | cfssljson -bare kube-proxy ls kube-proxy* kube-proxy.csr kube-proxy-csr.json kube-proxy-key.pem kube-proxy.pem 将生成的证书和私钥拷贝到所有需要部署 worker 节点的 /etc/kubernetes/ssl 下备用。 在完成证书分发之后，这样我们的证书相关的生成工作就完成了。接下来开始配置各个组件。 参考资料： Using RBAC Authorization Kubernetes HA Cluster Build 在 CentOS 上部署 kubernetes 集群]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>K8S</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[cpu 内存磁盘 io 过高问题处理]]></title>
    <url>%2F2018%2F09%2F15%2Fcpu%E5%86%85%E5%AD%98%E7%A3%81%E7%9B%98io%E8%BF%87%E9%AB%98%E9%97%AE%E9%A2%98%E5%A4%84%E7%90%86%2F</url>
    <content type="text"><![CDATA[CPU 占用过高分析 可以用 top 命令查看哪一个进程占用 cpu 高 或者哪一个占用内存大 top - 13:55:32 up 59 days, 19:18, 2 users, load average: 0.00, 0.04, 0.09 Tasks: 161 total, 1 running, 160 sleeping, 0 stopped, 0 zombie %Cpu(s): 3.7 us, 0.3 sy, 0.0 ni, 96.0 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 st KiB Mem : 16267564 total, 271608 free, 9033932 used, 6962024 buff/cache KiB Swap: 8388604 total, 8364736 free,23868 used. 5913400 avail Mem PID USER PR NIVIRTRESSHR S %CPU %MEM TIME+ COMMAND 29433 wuuser20 0 7805852 1.132g 19216 S 0.7 7.3 6:07.13 java 29355 wuuser20 0 6909304 1.093g 17248 S 0.3 7.0 2:29.96 java 29558 wuuser20 0 7763476 974788 15860 S 0.3 6.0 1:54.47 java 29945 wuuser20 0 7793536 1.202g 21344 S 0.3 7.7 4:31.02 java 可以看到占用最高的是 29433 进程 用 top -H -p pid 命令查看进程内各个线程占用的 CPU 百分比 （ top -Hp 29433）cpu 消耗情况 top -H -p 29433 top - 13:58:57 up 59 days, 19:22, 2 users, load average: 0.04, 0.07, 0.10 Threads: 132 total, 0 running, 132 sleeping, 0 stopped, 0 zombie %Cpu(s): 0.1 us, 0.0 sy, 0.0 ni, 99.8 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 st KiB Mem : 16267564 total, 271024 free, 9034476 used, 6962064 buff/cache KiB Swap: 8388604 total, 8364736 free,23868 used. 5913044 avail Mem PID USER PR NIVIRTRESSHR S %CPU %MEM TIME+ COMMAND 29433 wuuser20 0 7805852 1.132g 19216 S 0.0 7.3 0:00.00 java 29436 wuuser20 0 7805852 1.132g 19216 S 0.0 7.3 0:08.91 java 29437 wuuser20 0 7805852 1.132g 19216 S 0.0 7.3 0:00.34 java 29438 wuuser20 0 7805852 1.132g 19216 S 0.0 7.3 0:00.35 java 29439 wuuser20 0 7805852 1.132g 19216 S 0.0 7.3 0:00.34 java 29440 wuuser20 0 7805852 1.132g 19216 S 0.0 7.3 0:00.34 java 29441 wuuser20 0 7805852 1.132g 19216 S 0.0 7.3 0:00.34 java 上面看到线程 29436 占用高时长多 接着 使用 printf &quot;%x\n&quot; 线程号 将异常线程号转化为 16 进制 printf &quot;%x\n&quot; 29436 72fc 接着使用jstack 29433|grep 72fc -A90 来定位出现异常的代码 jstack 29433|grep 72fc -A90 &quot;main&quot; prio=10 tid=0x00007f16fc00e000 nid=0x72fc in Object.wait() [0x00007f17056d9000] java.lang.Thread.State: WAITING (on object monitor) at java.lang.Object.wait(Native Method) - waiting on &lt;0x0000000778044670&gt; (a java.lang.Class for com.wu.mortgage.service.OrderServiceStartup) at java.lang.Object.wait(Object.java:503) at com.wu.mortgage.service.OrderServiceStartup.main(OrderServiceStartup.java:47) - locked &lt;0x0000000778044670&gt; (a java.lang.Class for com.wu.mortgage.service.OrderServiceStartup) &quot;VM Thread&quot; prio=10 tid=0x00007f16fc14a000 nid=0x7309 runnable &quot;Gang worker#0 (Parallel GC Threads)&quot; prio=10 tid=0x00007f16fc01f800 nid=0x72fd runnable &quot;Gang worker#1 (Parallel GC Threads)&quot; prio=10 tid=0x00007f16fc021800 nid=0x72fe runnable &quot;Gang worker#2 (Parallel GC Threads)&quot; prio=10 tid=0x00007f16fc023000 nid=0x72ff runnable &quot;Gang worker#3 (Parallel GC Threads)&quot; prio=10 tid=0x00007f16fc025000 nid=0x7300 runnable &quot;Gang worker#4 (Parallel GC Threads)&quot; prio=10 tid=0x00007f16fc027000 nid=0x7301 runnable &quot;Gang worker#5 (Parallel GC Threads)&quot; prio=10 tid=0x00007f16fc029000 nid=0x7302 runnable &quot;Gang worker#6 (Parallel GC Threads)&quot; prio=10 tid=0x00007f16fc02b000 nid=0x7303 runnable &quot;Gang worker#7 (Parallel GC Threads)&quot; prio=10 tid=0x00007f16fc02d000 nid=0x7304 runnable &quot;Concurrent Mark-Sweep GC Thread&quot; prio=10 tid=0x00007f16fc106800 nid=0x7307 runnable &quot;Gang worker#0 (Parallel CMS Threads)&quot; prio=10 tid=0x00007f16fc102000 nid=0x7305 runnable &quot;Gang worker#1 (Parallel CMS Threads)&quot; prio=10 tid=0x00007f16fc104000 nid=0x7306 runnable &quot;VM Periodic Task Thread&quot; prio=10 tid=0x00007f16fcbed800 nid=0x7314 waiting on condition JNI global references: 522 CPU 使用率较低但负载较高 问题描述： Linux 系统没有业务程序运行，通过 top 观察，类似如下图所示，CPU 很空闲，但是 load average 却非常高： 处理办法： load average 是对 CPU 负载的评估，其值越高，说明其任务队列越长，处于等待执行的任务越多。出现此种情况时，可能是由于僵死进程导致的。可以通过指令 ps -axjf 查看是否存在 D 状态进程。D 状态是指不可中断的睡眠状态。该状态的进程无法被 kill，也无法自行退出。只能通过恢复其依赖的资源或者重启系统来解决。 内存占用过高 使用free -h 查看内存使用情况 top 按m 查看内存可以看到使用率 / 总内存 KiB Mem : 63.6/1626756 free -h totalusedfree shared buff/cache available Mem:15G8.6G274M854M6.6G5.7G Swap: 8.0G 23M8.0G top 按 m 查看内存可以看到使用率 / 总内存 KiB Mem : 63.6/16267564 top - 14:32:12 up 59 days, 19:55, 2 users, load average: 0.01, 0.06, 0.05 Tasks: 160 total, 1 running, 159 sleeping, 0 stopped, 0 zombie %Cpu(s): 1.8 us, 0.2 sy, 0.0 ni, 98.0 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 st KiB Mem : 63.6/16267564 [|||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||] KiB Swap: 0.3/8388604 [] PID USER PR NIVIRTRESSHR S %CPU %MEM TIME+ COMMAND 1515 wuuser20 0 157716 2288 1536 R 0.7 0.0 0:00.13 top 10262 root 20 0 0 0 0 S 0.3 0.0 0:00.31 kworker/2:1 29433 wuuser20 0 7806112 1.133g 19216 S 0.3 7.3 6:23.73 java 29754 wuuser20 0 6991120 764416 11932 S 0.3 4.7 5:50.32 java 29945 wuuser20 0 7793536 1.202g 21344 S 0.3 7.7 4:41.44 java 1 root 20 0 191152 3040 2108 S 0.0 0.0 10:59.88 systemd 2 root 20 0 0 0 0 S 0.0 0.0 0:01.54 kthreadd 3 root 20 0 0 0 0 S 0.0 0.0 0:11.69 ksoftirqd/0 5 root 0 -20 0 0 0 S 0.0 0.0 0:00.00 kworker/0:0H 7 root rt 0 0 0 0 S 0.0 0.0 0:04.75 migration/0 cat /proc/meminfo查看内存信息 cat /proc/meminfo MemTotal: 16267564 kB MemFree: 279388 kB MemAvailable:5926640 kB Buffers: 0 kB Cached: 6471844 kB SwapCached: 708 kB Active: 11110540 kB Inactive:4181420 kB Active(anon):8299564 kB Inactive(anon): 1395416 kB Active(file):2810976 kB Inactive(file): 2786004 kB Unevictable: 0 kB Mlocked: 0 kB SwapTotal: 8388604 kB SwapFree:8364736 kB Dirty: 192 kB Writeback: 0 kB AnonPages: 8819440 kB Mapped:98168 kB Shmem:874860 kB Slab: 495476 kB SReclaimable: 388612 kB SUnreclaim: 106864 kB KernelStack: 16560 kB PageTables:27572 kB NFS_Unstable: 0 kB Bounce:0 kB WritebackTmp: 0 kB CommitLimit:16522384 kB Committed_AS: 27461628 kB VmallocTotal: 34359738367 kB VmallocUsed: 179248 kB VmallocChunk: 34359341052 kB HardwareCorrupted: 0 kB AnonHugePages: 7559168 kB HugePages_Total: 0 HugePages_Free:0 HugePages_Rsvd:0 HugePages_Surp:0 Hugepagesize: 2048 kB DirectMap4k: 81856 kB DirectMap2M:16695296 kB 查看占用内存最大的 10 个进程: ps -aux | sort -k4nr | head -n 10 查看内存占用最大的进程的命令: ps aux| grep -v &quot;USER&quot; |sort -n -r -k 4 |awk &apos;NR==1{print $0}&apos; io 过高现象iostat 查看 io 情况 linux 机器 wa% 过高 top top - 15:35:47 up 265 days, 23:08, 6 users, load average: 0.03, 0.12, 0.13 Tasks: 203 total, 1 running, 202 sleeping, 0 stopped, 0 zombie %Cpu(s): 1.1 us, 0.2 sy, 0.0 ni, 98.7 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 st KiB Mem : 18497460 total, 581076 free, 3876592 used, 14039792 buff/cache KiB Swap: 4063228 total, 4003928 free,59300 used. 13227728 avail Mem PID USER PR NIVIRTRESSHR S %CPU %MEM TIME+ COMMAND 12850 root 20 0 9316608 335148 11836 S 11.1 1.8 104:05.35 java 1 root 20 0 44188 6664 3916 S 0.0 0.0 216:00.79 systemd 2 root 20 0 0 0 0 S 0.0 0.0 0:24.61 kthreadd 3 root 20 0 0 0 0 S 0.0 0.0 0:11.27 ksoftirqd/0 5 root 0 -20 0 0 0 S 0.0 0.0 0:00.00 kworker/0:0H us：用户态使用的 cpu 时间比 sy：系统态使用的 cpu 时间比 ni：用做 nice 加权的进程分配的用户态 cpu 时间比 id：空闲的 cpu 时间比 wa：cpu 等待磁盘写入完成时间 hi：硬中断消耗时间 si：软中断消耗时间 st：虚拟机偷取时间 如果一台机器看到 wa 特别高，那么一般说明是磁盘 IO 出现问题，可以使用 iostat 等命令继续进行详细分析。 安装 iostat yum install sysstat 安装后就可以使用 iostat 命令了 iostat -d -k 2 参数 -d 表示，显示设备（磁盘）使用状态；-k某些使用 block 为单位的列强制使用 Kilobytes 为单位；2表示，数据显示每隔 2 秒刷新一次。 iostat -d -k 2 Linux 3.10.0-327.el7.x86_64 (szfyruat01)07/11/2018 _x86_64_(8 CPU) Device:tpskB_read/skB_wrtn/skB_readkB_wrtn sda 0.9810.34 9.89 237682005 227295642 sdb 0.64 0.79 9.37 18104315 215368108 dm-0 1.46 1.2419.26 28388124 442501536 dm-1 0.00 0.00 0.01 39692 127628 Device:tpskB_read/skB_wrtn/skB_readkB_wrtn sda 0.50 4.00 0.00 8 0 sdb 0.00 0.00 0.00 0 0 dm-0 0.50 4.00 0.00 8 0 dm-1 0.00 0.00 0.00 0 0 tps：该设备每秒的传输次数（Indicate the number of transfers per second that were issued to the device.）。”一次传输”意思是”一次 I/O 请求”。多个逻辑请求可能会被合并为”一次 I/O 请求”。”一次传输”请求的大小是未知的。 kB_read/s：每秒从设备（drive expressed）读取的数据量； kB_wrtn/s：每秒向设备（drive expressed）写入的数据量； kB_read：读取的总数据量； kB_wrtn：写入的总数量数据量；这些单位都为 Kilobytes。 指定监控的设备名称为sda，该命令的输出结果和上面命令完全相同 iostat -d sda 2 Linux 3.10.0-327.el7.x86_64 (szfyruat01)07/11/2018 _x86_64_(8 CPU) Device:tpskB_read/skB_wrtn/skB_readkB_wrtn sda 0.9810.34 9.89 237685197 227298479 Device:tpskB_read/skB_wrtn/skB_readkB_wrtn sda 0.00 0.00 0.00 0 0 Device:tpskB_read/skB_wrtn/skB_readkB_wrtn sda 0.00 0.00 0.00 0 0 iostat还有一个比较常用的选项 -x，该选项将用于显示和 io 相关的扩展数据。 iostat -d -x -k 1 10 iostat -d -x -k 1 10 Linux 3.10.0-327.el7.x86_64 (szfyruat01)07/11/2018 _x86_64_(8 CPU) Device: rrqm/s wrqm/s r/s w/srkB/swkB/s avgrq-sz avgqu-sz await r_await w_await svctm %util sda 0.00 0.070.310.6810.34 9.8941.10 0.004.251.365.55 0.90 0.09 sdb 0.00 0.130.100.54 0.79 9.3731.86 0.005.820.556.77 1.55 0.10 dm-0 0.00 0.000.051.42 1.2419.2628.02 0.016.201.836.35 0.96 0.14 dm-1 0.00 0.000.000.00 0.00 0.01 8.04 0.009.084.77 10.39 0.69 0.00 Device: rrqm/s wrqm/s r/s w/srkB/swkB/s avgrq-sz avgqu-sz await r_await w_await svctm %util sda 0.00 0.000.000.00 0.00 0.00 0.00 0.000.000.000.00 0.00 0.00 sdb 0.00 0.000.000.00 0.00 0.00 0.00 0.000.000.000.00 0.00 0.00 dm-0 0.00 0.000.000.00 0.00 0.00 0.00 0.000.000.000.00 0.00 0.00 dm-1 0.00 0.000.000.00 0.00 0.00 0.00 0.000.000.000.00 0.00 0.00 rrqm/s：每秒这个设备相关的读取请求有多少被 Merge 了（当系统调用需要读取数据的时候，VFS 将请求发到各个 FS，如果 FS 发现不同的读取请求读取的是相同 Block 的数据，FS 会将这个请求合并 Merge）； wrqm/s：每秒这个设备相关的写入请求有多少被 Merge 了。 rsec/s：每秒读取的扇区数； wsec/：每秒写入的扇区数。 rKB/s：The number of read requests that were issued to the device per second； wKB/s：The number of write requests that were issued to the device per second； avgrq-sz 平均请求扇区的大小 avgqu-sz 是平均请求队列的长度。毫无疑问，队列长度越短越好。 await： 每一个 IO 请求的处理的平均时间（单位是微秒毫秒）。这里可以理解为 IO 的响应时间，一般地系统 IO 响应时间应该低于 5ms，如果大于 10ms 就比较大了。这个时间包括了队列时间和服务时间，也就是说，一般情况下，await 大于 svctm，它们的差值越小，则说明队列时间越短，反之差值越大，队列时间越长，说明系统出了问题。 svctm 表示平均每次设备 I/O 操作的服务时间（以毫秒为单位）。如果 svctm 的值与 await 很接近，表示几乎没有 I/O 等待，磁盘性能很好，如果 await 的值远高于 svctm 的值，则表示 I/O 队列等待太长，系统上运行的应用程序将变慢。 %util： 在统计时间内所有处理 IO 时间，除以总共统计时间。例如，如果统计间隔 1 秒，该设备有 0.8 秒在处理 IO，而 0.2 秒闲置，那么该设备的 %util = 0.8/1 = 80%，所以该参数暗示了设备的繁忙程度。一般地，如果该参数是 100% 表示设备已经接近满负荷运行了（当然如果是多磁盘，即使 %util 是 100%，因为磁盘的并发能力，所以磁盘使用未必就到了瓶颈）。 常见用法iostat -d -k 1 10 #查看 TPS 和吞吐量信息(磁盘读写速度单位为 KB)iostat -d -m 2 #查看 TPS 和吞吐量信息(磁盘读写速度单位为 MB)iostat -d -x -k 1 10 #查看设备使用率（%util）、响应时间（await） iostat -c 1 10 #查看 cpu 状态 查找引起高 I/O wait 对应的进程 yum -y install iotop 查找引起高I/O wait 对应的进程 [root@localhost ~]# iotop iotop 30475 be/4 root2.80 K/s0.00 B/s 0.00 % 0.00 % [cifsd] Total DISK READ :1915.31 B/s | Total DISK WRITE : 22.44 K/s Actual DISK READ: 0.00 B/s | Actual DISK WRITE: 0.00 B/s TID PRIO USER DISK READ DISK WRITE SWAPIN IO&gt;COMMAND 20041 be/4 root0.00 B/s0.00 B/s 0.00 % 0.07 % java -Djava.util.logging.config.file=/home/rizhi/apache-tomcat-7.0.88/conf/logging.pro~pdir=/home/rizhi/apache-tomcat-7.0.88/temp org.apache.catalina.startup.Bootstrap start 12881 be/4 root0.00 B/s 14.96 K/s 0.00 % 0.00 % java -Djava.util.logging.config.file=/home/rizhi/apache-tomcat-7.0.88/conf/logging.pro~pdir=/home/rizhi/apache-tomcat-7.0.88/temp org.apache.catalina.startup.Bootstrap start 13150 be/4 agent 0.00 B/s3.74 K/s 0.00 % 0.00 % falcon-agent -c /open-falcon/agent/config/cfg.json 30475 be/4 root 1915.31 B/s0.00 B/s 0.00 % 0.00 % [cifsd] 20607 be/4 root0.00 B/s3.74 K/s 0.00 % 0.00 % java -Djava.util.logging.config.file=/home/rizhi/apache-tomcat-7.0.88/conf/logging.pro~pdir=/home/rizhi/apache-tomcat-7.0.88/temp org.apache.catalina.startup.Bootstrap start 4096 be/4 root0.00 B/s0.00 B/s 0.00 % 0.00 % dockerd --insecure-registry 192.168.16.184:5000 --log-opt max-size=100m --log-opt max-file=3 --insecure-registry 192.168.34.138:5000 1 be/4 root0.00 B/s0.00 B/s 0.00 % 0.00 % systemd --system --deserialize 26 2 be/4 root0.00 B/s0.00 B/s 0.00 % 0.00 % [kthreadd] 3 be/4 root0.00 B/s0.00 B/s 0.00 % 0.00 % [ksoftirqd/0] 4100 be/4 root0.00 B/s0.00 B/s 0.00 % 0.00 % docker-containerd -l unix:///var/run/docker/libcontainerd/docker-containerd.sock --met~un/docker/libcontainerd/containerd --shim docker-containerd-shim --runtime docker-runc 5 be/0 root0.00 B/s0.00 B/s 0.00 % 0.00 % [kworker/0:0H] 4102 be/4 root0.00 B/s0.00 B/s 0.00 % 0.00 % docker-containerd -l unix:///var/run/docker/libcontainerd/docker-containerd.sock --met~un/docker/libcontainerd/containerd --shim docker-containerd-shim --runtime docker-runc 7 rt/4 root0.00 B/s0.00 B/s 0.00 % 0.00 % [migration/0] iotop 命令简介iotop 根据 Linux 内核（需要 2.6.20 及以上）来监测 I/O，并且能显示当前进程 / 线程的 I/O 使用率。 Linux 内核 build 的事后哦，需要开启 CONFIG_TASK_DELAY_ACCT 和 CONFIG_TASK_IO_ACCOUNTING 选项，这些选项依赖于 CONFIG_TASKSTATS。在采样周期里，iotop 按列显示每个进程 / 线程的 I/O 读写带宽，同时也显示进程 / 线程做 swap 交换和等待 I/O 所占用的百分比。 每一个进程都会显示 I/O 优先级 (class/level)，另外在最上面显示每个采样周期内的读写带宽。 使用左右箭头来改变排序，r 用来改变排序顺序，o 用来触发–only 选项，p 用来触发–processes 选项。a 用来触发–accumulated 选项，q 用来退出，i 用来改变进程或线程的监测优先级，其它任继健是强制刷新。 选项 --version 显示版本号然后退出 -h, --help 显示帮助然后退出 -o, --only 只显示正在产生 I/O 的进程或线程。除了传参，可以在运行过程中按 o 生效。 -b, --batch 非交互模式，一般用来记录日志 -n NUM, --iter=NUM 设置监测的次数，默认无限。在非交互模式下很有用 -d SEC, --delay=SEC 设置每次监测的间隔，默认 1 秒，接受非×××数据例如 1.1 -p PID, --pid=PID 指定监测的进程 / 线程 -u USER, --user=USER 指定监测某个用户产生的 I/O -P, --processes 仅显示进程，默认 iotop 显示所有线程 -a, --accumulated 显示累积的 I/O，而不是带宽 -k, --kilobytes 使用 kB 单位，而不是对人友好的单位。在非交互模式下，脚本编程有用。 -t, --time 加上时间戳，非交互非模式。 -q, --quiet 禁止头几行，非交互模式。有三种指定方式。 -q 只在第一次监测时显示列名 -qq 永远不显示列名。 -qqq 永远不显示 I/O 汇总。 只显示有 I/O 行为的进程 iotop -oP Total DISK READ : 18.12 K/s | Total DISK WRITE : 7.63 K/s Actual DISK READ: 0.00 B/s | Actual DISK WRITE: 0.00 B/s PID PRIO USER DISK READ DISK WRITE SWAPIN IO&gt;COMMAND 12850 be/4 root0.00 B/s7.63 K/s 0.00 % 0.01 % java -Djava.util.logging.config.file=/home/rizhi/apache-tomcat-7.0.88/conf/logging.pro~pdir=/home/rizhi/apache-tomcat-7.0.88/temp org.apache.catalina.startup.Bootstrap start 30475 be/4 root 18.12 K/s0.00 B/s 0.00 % 0.00 % [cifsd] 总结 内存过高 查看占用内存最大的 10 个进程: top 也可以 ps -aux | sort -k4nr | head -n 10 io 过高 首先查看哪些进程的 io 过高 （用 top 查看是否很高，让后用iostat -x -d 1 定哪个设备 IO 负载高） 然后用 iotop -oP 查看哪一个进程的 io 高 也可以用iostat -x -d 1 定哪个设备 IO 负载高 确定的进程 pid 后，可以用ps -ef |grep pid 查看哪一个应用引起的，看看是否需要重启服务 减少 io 也可以用top -H -p pid 查看这个进程中线程消耗资源情况]]></content>
      <categories>
        <category>故障案例</category>
      </categories>
      <tags>
        <tag>故障案例</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ELK+Filebeat+Kafka+ZooKeeper 构建海量日志分析平台【转】]]></title>
    <url>%2F2018%2F09%2F12%2FELK%2BFilebeat%2BKafka%2BZooKeeper%20%E6%9E%84%E5%BB%BA%E6%B5%B7%E9%87%8F%E6%97%A5%E5%BF%97%E5%88%86%E6%9E%90%E5%B9%B3%E5%8F%B0%E3%80%90%E8%BD%AC%E3%80%91%2F</url>
    <content type="text"><![CDATA[ELK+Filebeat+Kafka+ZooKeeper 构建海量日志分析平台 原文链接：http://blog.51cto.com/tchuairen/1861167 什么要做日志分析平台？ 随着业务量的增长，每天业务服务器将会产生上亿条的日志，单个日志文件达几个 GB，这时我们发现用 Linux 自带工具，cat grep awk 分析越来越力不从心了，而且除了服务器日志，还有程序报错日志，分布在不同的服务器，查阅繁琐。 待解决的痛点: 1、大量不同种类的日志成为了运维人员的负担，不方便管理; 2、单个日志文件巨大，无法使用常用的文本工具分析，检索困难; 3、日志分布在多台不同的服务器上，业务一旦出现故障，需要一台台查看日志。 为了解决以上困扰: 接下来我们要一步步构建这个日志分析平台，架构图如下: 架构解读 : （整个架构从左到右，总共分为 5 层） 第一层、数据采集层 最左边的是业务服务器集群，上面安装了 filebeat 做日志采集，同时把采集的日志分别发送给两个 logstash 服务。 第二层、数据处理层，数据缓存层 logstash 服务把接受到的日志经过格式处理，转存到本地的 kafka broker+zookeeper 集群中。 第三层、数据转发层 这个单独的 Logstash 节点会实时去 kafka broker 集群拉数据，转发至 ES DataNode。 第四层、数据持久化存储 ES DataNode 会把收到的数据，写磁盘，建索引库。 第五层、数据检索，数据展示 ES Master + Kibana 主要协调 ES 集群，处理数据检索请求，数据展示。 笔者为了节约宝贵的服务器资源，把一些可拆分的服务合并在同一台主机。大家可以根据自己的实际业务环境自由拆分，延伸架构。 开 工 ! 操作系统环境 : CentOS release 6.5 各服务器角色分配 : jdk-8u101-linux-x64.rpm logstash-2.3.2.tar.gz filebeat-1.2.3-x86_64.rpm kafka_2.11-0.10.0.1.tgz zookeeper-3.4.9.tar.gz elasticsearch-2.3.4.rpm kibana-4.5.3-linux-x64.tar.gz 一、安装部署 Elasticsearch 集群 布置 ES Master 节点 10.10.1.244 1、安装 jdk1.8，elasticsearch-2.3.4oracle 官网 jdk 下载地址: http://www.oracle.com/technetwork/java/javase/downloads/index.html elasticsearch 官网: https://www.elastic.co/ 安装命令 yum install jdk-8u101-linux-x64.rpm elasticsearch-2.3.4.rpm -y ES 会被默认安装在 /usr/share/elasticsearch/ 2、系统调优，JVM 调优# 配置系统最大打开文件描述符数 vim /etc/sysctl.conf fs.file-max=65535 配置进程最大打开文件描述符 vim /etc/security/limits.conf # End of file \* soft nofile 65535 \* hard nofile 65535 # 配置 JVM 内存 vim /etc/sysconfig/elasticsearch ES\_HEAP\_SIZE=4g # 这台机器的可用内存为 8G 3、编写 ES Master 节点配置文件 # /etc/elasticsearch/elasticsearch.yml # ---------------------------------- Cluster ----------------------------------- # Use a descriptive name for your cluster: cluster.name: bigdata # ------------------------------------ Node ------------------------------------ node.name: server1 node.master: true node.data: false # ----------------------------------- Index ------------------------------------ index.number\_of\_shards: 5 index.number\_of\_replicas: 0 index.refresh\_interval: 120s # ----------------------------------- Paths ------------------------------------ path.data: /home/elk/data path.logs: /var/log/elasticsearch/elasticsearch.log # ----------------------------------- Memory ----------------------------------- bootstrap.mlockall: true indices.fielddata.cache.size: 50mb #------------------------------------ Network And HTTP -------------------------- network.host: 0.0.0.0 http.port: 9200 # ------------------------------------ Translog ---------------------------------- index.translog.flush\_threshold\_ops: 50000 # --------------------------------- Discovery ------------------------------------ discovery.zen.minimum\_master\_nodes: 1 discovery.zen.ping.timeout: 200s discovery.zen.fd.ping\_timeout: 200s discovery.zen.fd.ping.interval: 30s discovery.zen.fd.ping.retries: 6 discovery.zen.ping.unicast.hosts: [&amp;quot;10.10.1.60:9300&amp;quot;,&amp;quot;10.10.1.90:9300&amp;quot;,&amp;quot;10.10.1.244:9300&amp;quot;,] discovery.zen.ping.multicast.enabled: false # --------------------------------- merge ------------------------------------------ indices.store.throttle.max\_bytes\_per\_sec: 100mb 注: path.data、path.logs 这两个参数指定的路径，如果没有需要自己创建，还要赋予权限给 elasticsearch 用户。（后面的 ES DataNode 也同样） 4、安装 head、kopf、bigdesk 开源插件 安装方法有两种 : 1、使用 ES 自带的命令 plugin# head /usr/share/elasticsearch/bin/plugin install mobz/elasticsearch-head # kopf /usr/share/elasticsearch/bin/plugin install lmenezes/elasticsearch-kopf # bigdesk /usr/share/elasticsearch/bin/plugin install hlstudio/bigdesk 2、自行下载插件的源码包安装 我们通过 plugin 命令安装的插件，其实是安装到了这个路径:/usr/share/elasticsearch/plugins 而 plugin install 命令后面跟的这一串 mobz/elasticsearch-head 其实是 github 上的一个地址。 前面加上 github 的官网地址就是 https://github.com/mobz/elasticsearch-head 可以复制到浏览器中打开，找到该插件的源码仓库。 现在知道了，想要找插件自己可以去 github 上搜一下出来一大堆。随便选一个然后取后面那串路径，用 ES 自带的命令安装。 如果安装失败了，那么就手动下载该插件的源码包。 解压后直接整个目录 mv 到 ES 的插件安装路径下。 也就是这里: /usr/share/elasticsearch/plugins/ 那如何访问安装好的插件呢？ http://ES\_server\_ip:port/\_plugin/plugin\_name Example: http://127.0.0.1:9200/\_plugin/head/ http://127.0.0.1:9200/\_plugin/kopf/ 这时，ES Master 已经配置好了。 布置 ES DataNode 节点 10.10.1.60安装和系统调优方法同上，插件不用安装，只是配置文件不同。 编写配置文件 # ---------------------------------- Cluster ----------------------------------- # Use a descriptive name for your cluster: cluster.name: bigdata # ------------------------------------ Node ------------------------------------ node.name: server2 node.master: false node.data: true # ----------------------------------- Index ------------------------------------ index.number\_of\_shards: 5 index.number\_of\_replicas: 0 index.refresh\_interval: 120s # ----------------------------------- Paths ------------------------------------ path.data: /home/elk/data,/disk2/elk/data2 path.logs: /var/log/elasticsearch/elasticsearch.log # ----------------------------------- Memory ----------------------------------- bootstrap.mlockall: true indices.fielddata.cache.size: 50mb #------------------------------------ Network And HTTP -------------------------- network.host: 0.0.0.0 http.port: 9200 # ------------------------------------ Translog ---------------------------------- index.translog.flush\_threshold\_ops: 50000 # --------------------------------- Discovery ------------------------------------ discovery.zen.minimum\_master\_nodes: 1 discovery.zen.ping.timeout: 200s discovery.zen.fd.ping\_timeout: 200s discovery.zen.fd.ping.interval: 30s discovery.zen.fd.ping.retries: 6 discovery.zen.ping.unicast.hosts: [&amp;quot;10.10.1.244:9300&amp;quot;,] discovery.zen.ping.multicast.enabled: false # --------------------------------- merge ------------------------------------------ indices.store.throttle.max\_bytes\_per\_sec: 100mb 10.10.1.60 也准备好了。 布置另一台 ES DataNode 节点 10.10.1.90 编写配置文件 # ---------------------------------- Cluster ----------------------------------- # Use a descriptive name for your cluster: cluster.name: bigdata # ------------------------------------ Node ------------------------------------ node.name: server3 node.master: false node.data: true # ----------------------------------- Index ------------------------------------ index.number\_of\_shards: 5 index.number\_of\_replicas: 0 index.refresh\_interval: 120s # ----------------------------------- Paths ------------------------------------ path.data: /home/elk/single path.logs: /var/log/elasticsearch/elasticsearch.log # ----------------------------------- Memory ----------------------------------- bootstrap.mlockall: true indices.fielddata.cache.size: 50mb #------------------------------------ Network And HTTP -------------------------- network.host: 0.0.0.0 http.port: 9200 # ------------------------------------ Translog ---------------------------------- index.translog.flush\_threshold\_ops: 50000 # --------------------------------- Discovery ------------------------------------ discovery.zen.minimum\_master\_nodes: 1 discovery.zen.ping.timeout: 200s discovery.zen.fd.ping\_timeout: 200s discovery.zen.fd.ping.interval: 30s discovery.zen.fd.ping.retries: 6 discovery.zen.ping.unicast.hosts: [&amp;quot;10.10.1.244:9300&amp;quot;,] discovery.zen.ping.multicast.enabled: false # --------------------------------- merge ------------------------------------------ indices.store.throttle.max\_bytes\_per\_sec: 100mb 5、现在三台 ES 节点已经准备就绪，分别启动服务# 10.10.1.244 /etc/init.d/elasticsearch start # 10.10.1.60 /etc/init.d/elasticsearch start # 10.10.1.90 /etc/init.d/elasticsearch start 6、访问 head 插件，查看集群状态 此时 Elasticsearch 集群已经准备完成 二、配置位于架构图中第二层的 ZooKeeper 集群 配置 10.10.1.30 节点 1、安装，配置 zookeeper zookeeper 官网: http://zookeeper.apache.org/ # zookeeper 依赖 java，如果之前没安装过 JDK，则需要安装. rpm -ivh jdk-8u101-linux-x64.rpm # 解压程序 tar xf zookeeper-3.4.9.tar.gz 编写配置文件 # conf/zoo.cfg # The number of milliseconds of each tick tickTime=2000 # The number of ticks that the initial # synchronization phase can take initLimit=10 # The number of ticks that can pass between # sending a request and getting an acknowledgement syncLimit=5 # the directory where the snapshot is stored. # do not use /tmp for storage, /tmp here is just # example sakes. dataDir=/u01/zookeeper/zookeeper-3.4.9/data # the port at which the clients will connect clientPort=2181 # the maximum number of client connections. # increase this if you need to handle more clients #maxClientCnxns=60 server.11=10.10.1.30:2888:3888 server.12=10.10.1.31:2888:3888 server.13=10.10.1.32:2888:3888 # Be sure to read the maintenance section of the # administrator guide before turning on autopurge. # # http://zookeeper.apache.org/doc/current/zookeeperAdmin.html#sc\_maintenance # # The number of snapshots to retain in dataDir # autopurge.snapRetainCount=3 # Purge task interval in hours # Set to &amp;quot;0&amp;quot; to disable auto purge feature # autopurge.purgeInterval=1 同步配置文件到其他两台节点 注: zookeeper 集群，每个节点的配置文件都是一样的。所以直接同步过去，不需要做任何修改。 不熟悉 zookeeper 的朋友，可以参考这里: http://tchuairen.blog.51cto.com/3848118/1859494 scp zoo.cfg 10.10.1.31:/usr/local/zookeeper-3.4.9/conf/ scp zoo.cfg 10.10.1.32:/usr/local/zookeeper-3.4.9/conf/ 2、创建 myid 文件 # 10.10.1.30 echo 11 \&amp;gt;/usr/local/zookeeper-3.4.9/data/myid # 10.10.1.31 echo 12 \&amp;gt;/usr/local/zookeeper-3.4.9/data/myid # 10.10.1.32 echo 13 \&amp;gt;/usr/local/zookeeper-3.4.9/data/myid 3、启动服务 &amp; 查看节点状态 # 10.10.1.30 bin/zkServer.sh start bin/zkServer.sh status ZooKeeper JMX enabled by default Using config: /usr/local/zookeeper/zookeeper-3.4.9/bin/../conf/zoo.cfg Mode: leader # 10.10.1.31 bin/zkServer.sh start bin/zkServer.sh status ZooKeeper JMX enabled by default Using config: /usr/local/zookeeper/zookeeper-3.4.9/bin/../conf/zoo.cfg Mode: follower # 10.10.1.32 bin/zkServer.sh start bin/zkServer.sh status ZooKeeper JMX enabled by default Using config: /usr/local/zookeeper/zookeeper-3.4.9/bin/../conf/zoo.cfg Mode: follower 此时 zookeeper 集群配置完成 三、配置位于架构图中第二层的 Kafka Broker 集群Kafka 官网: http://kafka.apache.org/ 不熟悉 Kafka 的朋友可以参考: http://tchuairen.blog.51cto.com/3848118/1855090 配置 10.10.1.30 节点 1、安装，配置 kafka # 解压程序 tar xf kafka\_2.11-0.10.0.1.tgz 编写配置文件 ############################# Server Basics ############################# broker.id=1 ############################# Socket Server Settings ############################# num.network.threads=3 # The number of threads doing disk I/O num.io.threads=8 # The send buffer (SO\_SNDBUF) used by the socket server socket.send.buffer.bytes=102400 # The receive buffer (SO\_RCVBUF) used by the socket server socket.receive.buffer.bytes=102400 # The maximum size of a request that the socket server will accept (protection against OOM) socket.request.max.bytes=104857600 ############################# Log Basics ############################# log.dirs=/usr/local/kafka/kafka\_2.11-0.10.0.1/data num.partitions=6 num.recovery.threads.per.data.dir=1 ############################# Log Flush Policy ############################# # The number of messages to accept before forcing a flush of data to disk #log.flush.interval.messages=10000 # The maximum amount of time a message can sit in a log before we force a flush #log.flush.interval.ms=1000 ############################# Log Retention Policy ############################# log.retention.hours=60 log.segment.bytes=1073741824 log.retention.check.interval.ms=300000 ############################# Zookeeper ############################# zookeeper.connect=10.10.1.30:2181,10.10.1.31:2181,10.10.1.32:2181 zookeeper.connection.timeout.ms=6000 注: 其他两个节点的配置文件也基本相同，只有一个参数需要修改 broker.id 。 它用于唯一标识节点，所以绝对不能相同，不然会节点冲突。 同步配置文件到其他两台节点 scp server.properties 10.10.1.31:/usr/local/kafka/kafka\_2.11-0.10.0.1/config/ scp server.properties 10.10.1.32:/usr/local/kafka/kafka\_2.11-0.10.0.1/config/ # 修改 broker.id # 10.10.1.31 broker.id=2 # 10.10.1.32 broker.id=3 2、配置主机名对应 IP 的解析 vim /etc/hosts 10.10.1.30 server1 10.10.1.31 server2 10.10.1.32 server3 # 记得同步到其他两台节点 3、启动服务 bin/kafka-server-start.sh config/server.properties # 其他两台节点启动方式相同 Kafka+ZooKeeper 集群配置完成 四、配置位于架构图中第二层的 Logstash 服务 配置 10.10.1.30 节点 1、安装，配置 logstash # 解压程序 tar xf logstash-2.3.2.tar.gz 配置 GeoLiteCity ， 用于地图显示 IP 访问的城市 官网地址: http://dev.maxmind.com/geoip/legacy/geolite/ 下载地址: http://geolite.maxmind.com/download/geoip/database/GeoLiteCity.dat.gz 解压 gunzip GeoLiteCity.dat.gz 编写配置文件 input { beats { port =\&amp;gt; 5044 codec =\&amp;gt;&amp;quot;json&amp;quot; } } filter {if[type]==&amp;quot;nginxacclog&amp;quot;{ geoip { source=\&amp;gt;&amp;quot;clientip&amp;quot;# 与日志中访问地址的 key 要对应 target =\&amp;gt;&amp;quot;geoip&amp;quot; database =\&amp;gt;&amp;quot;/usr/local/logstash/GeoLiteCity.dat&amp;quot; add\_field =\&amp;gt;[&amp;quot;[geoip][coordinates]&amp;quot;,&amp;quot;%{[geoip][longitude]}&amp;quot;] add\_field =\&amp;gt;[&amp;quot;[geoip][coordinates]&amp;quot;,&amp;quot;%{[geoip][latitude]}&amp;quot;] } mutate {convert =\&amp;gt;[&amp;quot;[geoip][coordinates]&amp;quot;,&amp;quot;float&amp;quot;] } } } output { kafka { workers =\&amp;gt; 2 bootstrap\_servers =\&amp;gt;&amp;quot;10.10.1.30:9092,10.10.1.31:9092,10.10.1.32:9092&amp;quot; topic\_id =\&amp;gt;&amp;quot;peiyinlog&amp;quot; } } 2、启动服务 /usr/local/logstash/bin/logstash agent -f logstash\_in\_kafka.conf &amp;amp; 10.10.1.31 节点的这块配置，与上述完全相同。（略） 位于第二层、数据处理层的 Logstash 配置完成 五、配置数据采集层，业务服务器 +Filebeat1、定制 Nginx 日志格式 log\_format json &amp;#39;{&amp;quot;@timestamp&amp;quot;:&amp;quot;$time\_iso8601&amp;quot;,&amp;#39; &amp;#39;&amp;quot;slbip&amp;quot;:&amp;quot;$remote\_addr&amp;quot;,&amp;#39; &amp;#39;&amp;quot;clientip&amp;quot;:&amp;quot;$http\_x\_forwarded\_for&amp;quot;,&amp;#39; &amp;#39;&amp;quot;serverip&amp;quot;:&amp;quot;$server\_addr&amp;quot;,&amp;#39; &amp;#39;&amp;quot;size&amp;quot;:$body\_bytes\_sent,&amp;#39; &amp;#39;&amp;quot;responsetime&amp;quot;:$request\_time,&amp;#39; &amp;#39;&amp;quot;domain&amp;quot;:&amp;quot;$host&amp;quot;,&amp;#39; &amp;#39;&amp;quot;method&amp;quot;:&amp;quot;$request\_method&amp;quot;,&amp;#39; &amp;#39;&amp;quot;requesturi&amp;quot;:&amp;quot;$request\_uri&amp;quot;,&amp;#39; &amp;#39;&amp;quot;url&amp;quot;:&amp;quot;$uri&amp;quot;,&amp;#39; &amp;#39;&amp;quot;appversion&amp;quot;:&amp;quot;$HTTP\_APP\_VERSION&amp;quot;,&amp;#39; &amp;#39;&amp;quot;referer&amp;quot;:&amp;quot;$http\_referer&amp;quot;,&amp;#39; &amp;#39;&amp;quot;agent&amp;quot;:&amp;quot;$http\_user\_agent&amp;quot;,&amp;#39; &amp;#39;&amp;quot;status&amp;quot;:&amp;quot;$status&amp;quot;,&amp;#39; &amp;#39;&amp;quot;devicecode&amp;quot;:&amp;quot;$HTTP\_HA&amp;quot;}&amp;#39;; # 在虚拟主机配置中调用 access\_log /alidata/log/nginx/access/access.log json; 2、安装 Filebeat Filebeat 也是 Elasticsearch 公司的产品，在官网可以下载。 # rpm 包安装 yum install filebeat-1.2.3-x86\_64.rpm -y 3、编写 Filebeat 配置文件 ################### Filebeat Configuration Example ######################### ############################# Filebeat ###################################### filebeat: prospectors: - paths: - /var/log/messages input\_type: log document\_type: messages - paths: - /alidata/log/nginx/access/access.log input\_type: log document\_type: nginxacclog - paths: - /alidata/www/logs/laravel.log input\_type: log document\_type: larlog - paths: - /alidata/www/logs/500\_error.log input\_type: log document\_type: peiyinlar\_500error - paths: - /alidata/www/logs/deposit.log input\_type: log document\_type: lar\_deposit - paths: - /alidata/www/logs/call\_error.log input\_type: log document\_type: call\_error - paths: - /alidata/log/php/php-fpm.log.slow input\_type: log document\_type: phpslowlog multiline: pattern: &amp;#39;^[[:space:]]&amp;#39; negate: true match: after registry\_file: /var/lib/filebeat/registry ############################# Output ########################################## output: logstash: hosts: [&amp;quot;10.26.95.215:5044&amp;quot;] ############################# Shipper ######################################### shipper: name: &amp;quot;host\_6&amp;quot; ############################# Logging ######################################### logging: files: rotateeverybytes: 10485760 # = 10MB 4、启动服务 /etc/init.d/filebeat start 数据采集层，Filebeat 配置完成。 现在业务服务器上的日志数据已经在源源不断的写入缓存了。 六、配置位于架构图中的第三层，数据转发层Logstash 安装上面已经讲过（略） 编写 Logstash 配置文件 # kafka\_to\_es.conf input{ kafka { zk\_connect =\&amp;gt;&amp;quot;10.10.1.30:2181,10.10.1.31:2181,10.10.1.32:2181&amp;quot; group\_id =\&amp;gt;&amp;quot;logstash&amp;quot; topic\_id =\&amp;gt;&amp;quot;peiyinlog&amp;quot; reset\_beginning =\&amp;gt;false consumer\_threads =\&amp;gt; 50 decorate\_events =\&amp;gt;true } } # 删除一些不需要的字段 filter {if[type]==&amp;quot;nginxacclog&amp;quot;{ mutate {remove\_field =\&amp;gt;[&amp;quot;slbip&amp;quot;,&amp;quot;kafka&amp;quot;,&amp;quot;domain&amp;quot;,&amp;quot;serverip&amp;quot;,&amp;quot;url&amp;quot;,&amp;quot;@version&amp;quot;,&amp;quot;offset&amp;quot;,&amp;quot;input\_type&amp;quot;,&amp;quot;count&amp;quot;,&amp;quot;source&amp;quot;,&amp;quot;fields&amp;quot;,&amp;quot;beat.hostname&amp;quot;,&amp;quot;host&amp;quot;,&amp;quot;tags&amp;quot;] } } } output {if[type]==&amp;quot;nginxacclog&amp;quot;{# stdout {codec =\&amp;gt; rubydebug} elasticsearch {hosts =\&amp;gt;[&amp;quot;10.10.1.90:9200&amp;quot;,&amp;quot;10.10.1.60:9200&amp;quot;] index =\&amp;gt;&amp;quot;logstash-nginxacclog-%{+YYYY.MM.dd}&amp;quot; manage\_template =\&amp;gt;true flush\_size =\&amp;gt; 50000 idle\_flush\_time =\&amp;gt; 10 workers =\&amp;gt; 2 } } if[type]==&amp;quot;messages&amp;quot;{ elasticsearch {hosts =\&amp;gt;[&amp;quot;10.10.1.90:9200&amp;quot;,&amp;quot;10.10.1.60:9200&amp;quot;] index =\&amp;gt;&amp;quot;logstash-messages-%{+YYYY.MM.dd}&amp;quot; manage\_template =\&amp;gt;true flush\_size =\&amp;gt; 50000 idle\_flush\_time =\&amp;gt; 30 workers =\&amp;gt; 1 } } if[type]==&amp;quot;larlog&amp;quot;{ elasticsearch {hosts =\&amp;gt;[&amp;quot;10.10.1.90:9200&amp;quot;,&amp;quot;10.10.1.60:9200&amp;quot;] index =\&amp;gt;&amp;quot;logstash-larlog-%{+YYYY.MM.dd}&amp;quot; manage\_template =\&amp;gt;true flush\_size =\&amp;gt; 2000 idle\_flush\_time =\&amp;gt; 10 } } if[type]==&amp;quot;deposit&amp;quot;{ elasticsearch {hosts =\&amp;gt;[&amp;quot;10.10.1.90:9200&amp;quot;,&amp;quot;10.10.1.60:9200&amp;quot;] index =\&amp;gt;&amp;quot;logstash-deposit-%{+YYYY.MM.dd}&amp;quot; manage\_template =\&amp;gt;true flush\_size =\&amp;gt; 2000 idle\_flush\_time =\&amp;gt; 10 } } if[type]==&amp;quot;phpslowlog&amp;quot;{ elasticsearch {hosts =\&amp;gt;[&amp;quot;10.10.1.90:9200&amp;quot;,&amp;quot;10.10.1.60:9200&amp;quot;] index =\&amp;gt;&amp;quot;logstash-phpslowlog-%{+YYYY.MM.dd}&amp;quot; manage\_template =\&amp;gt;true flush\_size =\&amp;gt; 2000 idle\_flush\_time =\&amp;gt; 10 } } } 启动服务 /usr/local/logstash/bin/logstash agent -f kafka\_to\_es.conf &amp;amp; 数据转发层已经配置完成 这时数据已经陆陆续续的从 kafka 取出，转存到 ES DataNode。 我们登陆到任意一台 kafka 主机，查看数据的缓存和消费情况 七、修改 ES 的索引模版配置 为什么要做这一步呢？ 因为 logstash 写入数据到 ES 时，会自动选用一个索引模版。 我们可以看一下 这个模版其实也挺好，不过有一个参数，我标记出来了。 &quot;refresh_interval&quot;:&quot;5s&quot; 这个参数用于控制，索引的刷新频率。 索引的刷新频率越快，你搜索到的数据就实时。 这里是 5 秒。 一般我们日志场景不需要这么高的实时性。 可以适当降低该参数，提高 ES 索引库的写入速度。 上传自定义模版 curl -XPUT http://10.10.1.244:9200/\_template/logstash2 -d &amp;#39; { &amp;quot;order&amp;quot;:1, &amp;quot;template&amp;quot;:&amp;quot;logstash-\*&amp;quot;, &amp;quot;settings&amp;quot;:{ &amp;quot;index&amp;quot;:{&amp;quot;refresh\_interval&amp;quot;:&amp;quot;120s&amp;quot;} }, &amp;quot;mappings&amp;quot;:{ &amp;quot;\_default\_&amp;quot;:{ &amp;quot;\_all&amp;quot;:{&amp;quot;enabled&amp;quot;:false} } } }&amp;#39; 由于这个自定义模版，我把优先级 order 定义的比 logstash 模版高，而模版的匹配规则又一样，所以这个自定义模版的配置会覆盖原 logstash 模版。 我这里只是简单描述。 如果要详细理解其中道理，请查看我的 ES 调优篇。 八、配置 Kibana 数据展示层10.10.1.244 节点 Kibana 是 ELK 套件中的一员，也属于 elasticsearch 公司，在官网提供下载。 安装tar xf kibana-4.5.3-linux-x64.tar.gz # 很简单，只要解压就可以用。 修改配置文件 # vim kibana-4.5.3-linux-x64/config/kibana.yml # Kibana is served by a back end server. This controls which port to use. server.port: 5601 # The host to bind the server to. server.host: &amp;quot;0.0.0.0&amp;quot; # The Elasticsearch instance to use for all your queries. elasticsearch.url: &amp;quot; # 修改这三个参数就好了 启动服务 打开浏览器访问: http://10.10.1.244:5601/ 定制 Elasticsearch 索引的 Index pattern 默认情况下，Kibana 认为你要访问的是通过 Logstash 导入 Elasticsearch 的数据，这时候你可以用默认的 logstash-* 作为你的 index pattern。 通配符（*）匹配索引名中任意字符任意个数。 选择一个包含了时间戳的索引字段（字段类型为 date 的字段），可以用来做基于时间的处理。Kibana 会读取索引的 映射，然后列出所有包含了时间戳的字段。如果你的索引没有基于时间的数据. 关闭 Index contains time-based events 参数。 如果一个新索引是定期生成，而且索引名中带有时间戳，选择 Use event times to create index names 选项， 然后再选择 Index pattern interval 。这可以提高搜索性能，Kibana 会至搜索你指定的时间范围内的索引。在你用 Logstash 输出数据给 Elasticsearch 的情况下尤其有效。 由于我们的索引是用日期命名，按照每天分割的。 index pattern 如下 数据展示 完 工 !]]></content>
      <categories>
        <category>ELK</category>
      </categories>
      <tags>
        <tag>ELK</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一个思维习惯，让你成为架构师]]></title>
    <url>%2F2018%2F08%2F29%2F%E4%B8%80%E4%B8%AA%E6%80%9D%E7%BB%B4%E4%B9%A0%E6%83%AF%EF%BC%8C%E8%AE%A9%E4%BD%A0%E6%88%90%E4%B8%BA%E6%9E%B6%E6%9E%84%E5%B8%88%2F</url>
    <content type="text"><![CDATA[一个思维习惯，让你成为架构师 程序员的迷茫不仅仅是面对技术繁杂的无力感，更重要的是因为长期埋没于软件 世界的浩大的分工体系中，无法看清从业务到软件架构的价值链条，无法清楚定位自 己在分工体系的位置，处理不好自身与技术、业务的关系所致。 很多程序员打心底不喜欢业务，这一点我曾经也经历过，我更宁愿从事框架工 具、技术组件研究的相关事情。我有个朋友经常吐槽我说：”你们天天加班加点写了 那么多代码，然后呢？有改变什么吗？还不是写出了一堆垃圾。”仔细想想很多时候 业务在我们脑海中存留的只是逻辑和流程，我们丢失的是对业务场景的感受，对用 户痛点的体会，对业务发展的思考。这些都是与价值紧密相关的部分。我们很自然的 用战术的勤快掩盖战略的懒惰！那么这样的后果就是我们把自己限死在流水线的工位 上，阉割了自己能够发现业务价值的能力，而过多关注新技术对职场竞争力的价值。 这也就是我们面对繁杂技术，而产生技术学习焦虑症的根本原因。 那么什么是业务呢？ 就是指某种有目的的工作或工作项目，业务的目的就是解 决人类社会与吃喝住行息息相关的领域问题，包括物质的需求和精神的需求，使开展 业务活动的主体和受众都能得到利益。通俗的讲业务就是用户的痛点，是业务提供方 （比如公司）的盈利点。而技术则是解决问题的工具和手段。比如为了解决用户随时随 地购物的业务问题时，程序员利用 web 技术构建电子商务 App，而当需求升级为帮 助用户快速选购商品时，程序员会利用数据算法等技术手段构建推荐引擎。 技术如果 脱离了业务，那么技术应用就无法很好的落地，技术的研究也将失去场景和方向。而 业务脱离了技术，那么业务的开展就变得极其昂贵和低效。 所以回过头来我们想想自己没日没夜写了那么多的代码从而构建起来的软件系 统，它的价值何在呢？说白了就是为了解决业务问题，所以当你所从事的工作内容并 不能为解决业务问题带来多大帮助的时候，你应该要及时做出调整。那么软件系统又 是如何体现它自身的价值呢？在我看来有如下几个方面的体现： 业务领域与功能：比如支付宝立足支付领域而推出的转账、收款功能等，比如人 工智能自动驾驶系统等。 服务能力：这就好比火车站购票窗口，评判它的服务能力的标准就是它能够同时 处理多少用户的购票业务，能不能在指定时间内完成购票业务，能不能 7*8 小时持续 工作。对应到软件系统领域，则表现为以下三个方面： 系统正确性 (程序能够正确表述业务流程，没有 Bug) 可用性（可以 7 ＊ 24 小时＊ 365 不间歇工作） 大规模（高并发，高吞吐量） 互联网公司正是借助大规模的软件系统承载着繁多的业务功能，使其拥有巨大的 服务能力并借助互联网技术突破了空间限制，高效低廉解决了业务问题，创造了丰厚 的利润，这是人肉所不可比拟的。 理解了这一层面的概念，你就可以清楚这个价值链条：公司依靠软件系统提供业 务服务而创造价值，程序员则是通过构建并持续演进软件系统服务能力以及业务功能 以支撑公司业务发展从而创造价值。 有了这个价值链条，我们就可以反思自己的工作学习对软件系统的服务能力提升 起到了多大的推动作用？可以反思自己的工作学习是否切实在解决领域的业务问题， 还是只是做一些意义不大的重复性工作。 什么是架构？ 在我看来软件架构就是将人员、技术等资源组织起来以解决业务问题，支撑业务 增长的一种活动。可能比较抽象，我想我们可以从架构师的一些具体工作任务来理解 这句话含义： 组织业务：架构师通过探索和研究业务领域的知识，构建自身看待业务的”世界 观”。他会基于这种认识拆分业务生命周期，确立业务边界，构建出了一套解决特定 业务问题的领域模型，并且确认模型之间、领域之间的关系与协作方式，完成了对业 务领域内的要素的组织工作。 组织技术：为了能在计算机世界中运作人类社会的业务模型，架构师需要选用计 算机世界中合适的框架、中间件、编程语言、网络协议等技术工具依据之前设计方 案组织起来形成一套软件系统方案，在我看来软件系统就像是一种技术组织，即技 术组件、技术手段依据某种逻辑被组织起来了，这些技术工具被确定了职责，有了明 确分工，并以实现业务功能为目标集合在了一起。比如 RPC 框架或消息队列被用 于内部系统之间的通信服务就如同信使一般，而数据库则负责记录结果，它更像是 一名书记员。 组织人员：为了能够实现利用软件系统解决业务问题的目标，架构师还需要关注 软件系统的构建过程，他以实现软件系统为号召，从公司组织中聚集一批软件工程 师，并将这些人员按不同工种、不同职责、不同系统进行组织，确定这些人员之间的 协作方式，并关注这个组织系统是否运作良好比如沟通是否顺畅、产出是否达到要 求、能否按时间完成等。 组织全局，对外输出：架构师的首要目标是解决业务问题，推动业务增长。所以 他非常关心软件的运行状况。因为只有在软件系统运行起来后，才能对外提供服务， 才能在用户访问的过程中，解决业务问题。架构师需要关注运行过程中产生的数据比 如业务成功率，系统运行资源占用数据、用户反馈信息、业务增长情况等，这些信息 将会帮助架构师制定下一步架构目标和方向。 所以软件架构不仅仅只是选用什么框架、选用什么技术组件这么简单。它贯穿了 对人的组织、对技术的组织、对业务的组织，并将这三种组织以解决业务问题这一目 标有机的结合在了一起。 很多面试的候选人在被问及他所开发的系统采用什么架构的问题时，只会罗列出 一些技术组件、技术框架等技术要素，这样看来其根本没有理清架构的深层含义。也 有一些架构师只专注对底层技术的研究，以为打造一个卓越的系统是非常牛逼的事 情，可是他忽略了软件系统的价值是以解决业务问题的能力、支撑业务增长的能力为 衡量标准，所以最后生产出了很多对组织，对业务没有帮助的系统。 成本与收益 正如之前所说软件系统只有在运行的时候才能创造价值，也就是说软件系统能否 7*24 小时＊ 365 天稳定的工作关系到公司的收益水平。所以开发团队对生产环境的 发布总是小心翼翼，对解决生产环境的问题总是加班加点。而软件系统的成本则体现 在软件构建过程，这时候我们就能理解那些工程技术如项目管理、敏捷开发、 单元测 试、持续集成、持续构建，版本管理等的价值了，他们有的是保证软件系统正确性， 有的是为了降低沟通成本，有的是为了提升开发效率等但总的来说就是为了降低软件 的构建成本。所以在提升系统服务能力，创造更多业务收益的同时，降低构建成本也 是一种提升收益的有效手段。 作为一名软件工程师而言，我们往往处在软件构建过程体系中的某个环节，我们 可以基于成本与收益的关系去思考自己每一项技能的价值，学习新的有价值的技能， 甚至在工作中基于成本与收益的考量选择合适的技术。比如在逻辑不大发生变化的地 方，没有必要去做过多的设计，应用各种花俏的设计模式等浪费时间。这样我们才能 成为技术的主人。 架构目标需要适应业务的发展 架构的目标就是为了支撑业务增长，就是提升软件系统的服务能力。可是话虽说 如此，但真实却要做很多取舍。比如对初创团队而言，其产品是否解决业务问题这一 设想还没得到确认，就立即去构造一个高性能、高可用的分布式系统，这样的架构目 标远超出业务发展的需求，最后的结果就是浪费大量人力物力，却得不到任何起色。 架构师需要审时度势，仔细衡量正确性、大规模、可用性三者的关系，比如今年业务 蓬勃发展日均订单 300 万，基于对未来的可能预测，明年可能有 3000 万的订单，那 么架构师应该要着重考虑大规模和可用性。而且每一点提升的程度，也需要架构师衡 量把握，比如可用性要达到 2 个 9 还是 3 个 9。 回顾自己以往的工作很多时候就是因为没有确立架构目标导致浪费了组织很多资 源，比如在之前的创业团队中，由于本人有一定的代码洁癖，经常会花费很多时间和 同事计较代码质量，这样本可以更快上线的功能却需要被延迟，当时过度追求正确性 的行为是与创业团队快速验证想法的业务需求不匹配的。 从价值出发－找寻学习与工作的新思路 向前一步，为更大的价值负责：不要因为自己是开发人员就不去关注软件运维， 不要因为只是测试就不关注软件开发，因为你关注的越多你越能看清全局的价值目 标。如果只关注一亩三分地，那么注定这辈子只能困守在这一亩三分地里，成为一名 流水线上焦虑至死的码农。试着转变思维，从架构师的角度思考价值问题，看看能否 将技术贯穿到业务、到用户、到最终的价值去。之前我的朋友说过要把产品经理踢到 运营位置去，把程序员踢到产品经理位置去，这样才是正确做事方式。这句话也是类 似的意思，向前一步才能懂得怎么做的更好。 像架构师一样思考，用价值找寻重心：人的迷茫是因为找不到重心，而价值的意 义在于引导我们思考做哪些事情才能实现价值，先做哪些事情会比后做哪些事情更能 创造收益。像架构师那样全局性思考，把遇到问题进行拆分，把学习到的事物串联起 来，努力构成完整的价值链条。 转自：一个思维习惯，让你成为架构师]]></content>
      <categories>
        <category>架构</category>
      </categories>
      <tags>
        <tag>架构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何用 9 条命令在一分钟内检查 Linux 服务器性能？]]></title>
    <url>%2F2018%2F08%2F15%2F%E5%A6%82%E4%BD%95%E7%94%A89%E6%9D%A1%E5%91%BD%E4%BB%A4%E5%9C%A8%E4%B8%80%E5%88%86%E9%92%9F%E5%86%85%E6%A3%80%E6%9F%A5Linux%E6%9C%8D%E5%8A%A1%E5%99%A8%E6%80%A7%E8%83%BD%EF%BC%9F%2F</url>
    <content type="text"><![CDATA[一、uptime 命令 这个命令可以快速查看机器的负载情况。在 Linux 系统中，这些数据表示等待 CPU 资源的进程和阻塞在不可中断 IO 进程（进程状态为 D）的数量。这些数据可以让我们对系统资源使用有一个宏观的了解。 命令的输出分别表示 1 分钟、5 分钟、15 分钟的平均负载情况。通过这三个数据，可以了解服务器负载是在趋于紧张还是趋于缓解。如果 1 分钟平均负载很高，而 15 分钟平均负载很低，说明服务器正在命令高负载情况，需要进一步排查 CPU 资源都消耗在了哪里。反之，如果 15 分钟平均负载很高，1 分钟平均负载较低，则有可能是 CPU 资源紧张时刻已经过去。 上面例子中的输出，可以看见最近 1 分钟的平均负载非常高，且远高于最近 15 分钟负载，因此我们需要继续排查当前系统中有什么进程消耗了大量的资源。可以通过下文将会介绍的 vmstat、mpstat 等命令进一步排查。 二、dmesg 命令 该命令会输出系统日志的最后 10 行。示例中的输出，可以看见一次内核的 oom kill 和一次 TCP 丢包。这些日志可以帮助排查性能问题。千万不要忘了这一步。 三、vmstat 命令 vmstat(8) 命令，每行会输出一些系统核心指标，这些指标可以让我们更详细的了解系统状态。后面跟的参数 1，表示每秒输出一次统计信息，表头提示了每一列的含义，这几介绍一些和性能调优相关的列： • r：等待在 CPU 资源的进程数。这个数据比平均负载更加能够体现 CPU 负载情况，数据中不包含等待 IO 的进程。如果这个数值大于机器 CPU 核数，那么机器的 CPU 资源已经饱和。 • free：系统可用内存数（以千字节为单位），如果剩余内存不足，也会导致系统性能问题。下文介绍到的 free 命令，可以更详细的了解系统内存的使用情况。 • si，so：交换区写入和读取的数量。如果这个数据不为 0，说明系统已经在使用交换区（swap），机器物理内存已经不足。 • us, sy, id, wa, st：这些都代表了 CPU 时间的消耗，它们分别表示用户时间（user）、系统（内核）时间（sys）、空闲时间（idle）、IO 等待时间（wait）和被偷走的时间（stolen，一般被其他虚拟机消耗）。 上述这些 CPU 时间，可以让我们很快了解 CPU 是否出于繁忙状态。一般情况下，如果用户时间和系统时间相加非常大，CPU 出于忙于执行指令。如果 IO 等待时间很长，那么系统的瓶颈可能在磁盘 IO。 示例命令的输出可以看见，大量 CPU 时间消耗在用户态，也就是用户应用程序消耗了 CPU 时间。这不一定是性能问题，需要结合 r 队列，一起分析。 四、mpstat 命令 该命令可以显示每个 CPU 的占用情况，如果有一个 CPU 占用率特别高，那么有可能是一个单线程应用程序引起的。 五、pidstat 命令 pidstat 命令输出进程的 CPU 占用率，该命令会持续输出，并且不会覆盖之前的数据，可以方便观察系统动态。如上的输出，可以看见两个 JAVA 进程占用了将近 1600% 的 CPU 时间，既消耗了大约 16 个 CPU 核心的运算资源。 六、iostat 命令 • r/s, w/s, rkB/s, wkB/s：分别表示每秒读写次数和每秒读写数据量（千字节）。读写量过大，可能会引起性能问题。 • await：IO 操作的平均等待时间，单位是毫秒。这是应用程序在和磁盘交互时，需要消耗的时间，包括 IO 等待和实际操作的耗时。如果这个数值过大，可能是硬件设备遇到了瓶颈或者出现故障。 • avgqu-sz：向设备发出的请求平均数量。如果这个数值大于 1，可能是硬件设备已经饱和（部分前端硬件设备支持并行写入）。 • %util：设备利用率。这个数值表示设备的繁忙程度，经验值是如果超过 60，可能会影响 IO 性能（可以参照 IO 操作平均等待时间）。如果到达 100%，说明硬件设备已经饱和。 如果显示的是逻辑设备的数据，那么设备利用率不代表后端实际的硬件设备已经饱和。值得注意的是，即使 IO 性能不理想，也不一定意味这应用程序性能会不好，可以利用诸如预读取、写缓存等策略提升应用性能。 七、free 命令 free 命令可以查看系统内存的使用情况，-m 参数表示按照兆字节展示。最后两列分别表示用于 IO 缓存的内存数，和用于文件系统页缓存的内存数。需要注意的是，第二行 -/+ buffers/cache，看上去缓存占用了大量内存空间。 这是 Linux 系统的内存使用策略，尽可能的利用内存，如果应用程序需要内存，这部分内存会立即被回收并分配给应用程序。因此，这部分内存一般也被当成是可用内存。 如果可用内存非常少，系统可能会动用交换区（如果配置了的话），这样会增加 IO 开销（可以在 iostat 命令中提现），降低系统性能。 八、sar 命令 sar 命令在这里可以查看网络设备的吞吐率。在排查性能问题时，可以通过网络设备的吞吐量，判断网络设备是否已经饱和。如示例输出中，eth0 网卡设备，吞吐率大概在 22 Mbytes/s，既 176 Mbits/sec，没有达到 1Gbit/sec 的硬件上限。 sar 命令在这里用于查看 TCP 连接状态，其中包括：• active/s：每秒本地发起的 TCP 连接数，既通过 connect 调用创建的 TCP 连接；• passive/s：每秒远程发起的 TCP 连接数，即通过 accept 调用创建的 TCP 连接；• retrans/s：每秒 TCP 重传数量；TCP 连接数可以用来判断性能问题是否由于建立了过多的连接，进一步可以判断是主动发起的连接，还是被动接受的连接。TCP 重传可能是因为网络环境恶劣，或者服务器压力过大导致丢包。 九、top 命令 top 命令包含了前面好几个命令的检查的内容。比如系统负载情况（uptime）、系统内存使用情况（free）、系统 CPU 使用情况（vmstat）等。因此通过这个命令，可以相对全面的查看系统负载的来源。同时，top 命令支持排序，可以按照不同的列排序，方便查找出诸如内存占用最多的进程、CPU 占用率最高的进程等。 但是，top 命令相对于前面一些命令，输出是一个瞬间值，如果不持续盯着，可能会错过一些线索。这时可能需要暂停 top 命令刷新，来记录和比对数据。]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL 高性能优化实战总结！]]></title>
    <url>%2F2018%2F07%2F21%2FMySQL%E9%AB%98%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E5%AE%9E%E6%88%98%E6%80%BB%E7%BB%93%EF%BC%81%2F</url>
    <content type="text"><![CDATA[一、前言MySQL 对于很多 Linux 从业者而言，是一个非常棘手的问题，多数情况都是因为对数据库出现问题的情况和处理思路不清晰。在进行 MySQL 的优化之前必须要了解的就是 MySQL 的查询过程，很多的查询优化工作实际上就是遵循一些原则让 MySQL 的优化器能够按照预想的合理方式运行而已。 今天给大家体验 MySQL 的优化实战，助你高薪之路顺畅！ 二、优化的哲学 注意：优化有风险，涉足需谨慎！ 2.1 优化可能带来的问题 优化不总是对一个单纯的环境进行，还很可能是一个复杂的已投产的系统。 优化手段本来就有很大的风险，只不过你没能力意识到和预见到！ 任何的技术可以解决一个问题，但必然存在带来一个问题的风险！ 对于优化来说解决问题而带来的问题, 控制在可接受的范围内才是有成果。 保持现状或出现更差的情况都是失败！ 2.2、优化的需求 稳定性和业务可持续性, 通常比性能更重要！ 优化不可避免涉及到变更，变更就有风险！ 优化使性能变好，维持和变差是等概率事件！ 切记优化, 应该是各部门协同，共同参与的工作，任何单一部门都不能对数据库进行优化！ 所以优化工作, 是由业务需要驱使的！！！ 2.3、优化由谁参与 在进行数据库优化时，应由数据库管理员、业务部门代表、应用程序架构师、应用程序设计人员、应用程序开发人员、硬件及系统管理员、存储管理员等，业务相关人员共同参与。 三、优化思路 3.1、优化什么 在数据库优化上有两个主要方面：即安全与性能。 安全 —&gt; 数据可持续性 性能 —&gt; 数据的高性能访问 3.2、优化的范围有哪些 存储、主机和操作系统方面: 主机架构稳定性 I/O 规划及配置 Swap 交换分区 OS 内核参数和网络问题 应用程序方面: 应用程序稳定性 SQL 语句性能 串行访问资源 性能欠佳会话管理 这个应用适不适合用 MySQL 数据库优化方面: 内存 数据库结构(物理 &amp; 逻辑) 实例配置 说明：不管是在，设计系统，定位问题还是优化，都可以按照这个顺序执行。 3.3、优化维度 数据库优化维度有四个： 硬件、系统配置、数据库表结构、SQL 及索引。 优化选择： 优化成本: 硬件 &gt; 系统配置 &gt; 数据库表结构 &gt;SQL 及索引 优化效果: 硬件 &lt; 系统配置 &lt; 数据库表结构 &lt;SQL 及索引 四、优化工具有啥？4.1、数据库层面 检查问题常用工具： 不常用但好用的工具： 4.2、数据库层面问题解决思路 一般应急调优的思路： 针对突然的业务办理卡顿，无法进行正常的业务处理！需要立马解决的场景！ 1、show processlist 2、explain select id ,name from stu where name=’clsn’; # ALL id name age sexselect id,name from stu where id=2-1 函数 结果集 &gt;30;show index from table; 3、通过执行计划判断，索引问题（有没有、合不合理）或者语句本身问题 4、show status like ‘%lock%’; # 查询锁状态kill SESSION_ID; # 杀掉有问题的 session 常规调优思路： 针对业务周期性的卡顿，例如在每天 10-11 点业务特别慢，但是还能够使用，过了这段时间就好了。 1、查看 slowlog，分析 slowlog，分析出查询慢的语句。 2、按照一定优先级，进行一个一个的排查所有慢语句。 3、分析 top sql，进行 explain 调试，查看语句执行时间。 4、调整索引或语句本身。 4.3、系统层面cpu 方面： vmstat、sar top、htop、nmon、mpstat 内存： free 、ps -aux 、 IO 设备（磁盘、网络）： iostat 、 ss 、 netstat 、 iptraf、iftop、lsof、 vmstat 命令说明： Procs：r 显示有多少进程正在等待 CPU 时间。b 显示处于不可中断的休眠的进程数量。在等待 I/O Memory：swpd 显示被交换到磁盘的数据块的数量。未被使用的数据块，用户缓冲数据块，用于操作系统的数据块的数量 Swap：操作系统每秒从磁盘上交换到内存和从内存交换到磁盘的数据块的数量。s1 和 s0 最好是 0 Io：每秒从设备中读入 b1 的写入到设备 b0 的数据块的数量。反映了磁盘 I/O System：显示了每秒发生中断的数量 (in) 和上下文交换 (cs) 的数量 Cpu：显示用于运行用户代码，系统代码，空闲，等待 I/O 的 CPU 时间 iostat 命令说明 实例命令： iostat -dk 1 5 iostat -d -k -x 5 （查看设备使用率（%util）和响应时间（await）） tps：该设备每秒的传输次数。“一次传输”意思是“一次 I/O 请求”。多个逻辑请求可能会被合并为“一次 I/O 请求”。 iops ：硬件出厂的时候，厂家定义的一个每秒最大的 IO 次数,”一次传输”请求的大小是未知的。 kB_read/s：每秒从设备（drive expressed）读取的数据量； KB_wrtn/s：每秒向设备（drive expressed）写入的数据量； kB_read：读取的总数据量； kB_wrtn：写入的总数量数据量；这些单位都为 Kilobytes。 4.4、系统层面问题解决办法 你认为到底负载高好，还是低好呢？ 在实际的生产中，一般认为 cpu 只要不超过 90% 都没什么问题 。 当然不排除下面这些特殊情况： 问题一：cpu 负载高，IO 负载低 内存不够 磁盘性能差 SQL 问题 ——&gt; 去数据库层，进一步排查 sql 问题 IO 出问题了（磁盘到临界了、raid 设计不好、raid 降级、锁、在单位时间内 tps 过高） tps 过高: 大量的小数据 IO、大量的全表扫描 问题二：IO 负载高，cpu 负载低 大量小的 IO 写操作： autocommit ，产生大量小 IO IO/PS, 磁盘的一个定值，硬件出厂的时候，厂家定义的一个每秒最大的 IO 次数。 大量大的 IO 写操作 SQL 问题的几率比较大 问题三：IO 和 cpu 负载都很高 硬件不够了或 sql 存在问题 五、基础优化 5.1、优化思路 定位问题点: 硬件 –&gt; 系统 –&gt; 应用 –&gt; 数据库 –&gt; 架构（高可用、读写分离、分库分表） 处理方向： 明确优化目标、性能和安全的折中、防患未然 5.2、硬件优化 主机方面： 根据数据库类型，主机 CPU 选择、内存容量选择、磁盘选择 平衡内存和磁盘资源 随机的 I/O 和顺序的 I/O 主机 RAID 卡的 BBU(Battery Backup Unit)关闭 cpu 的选择： cpu 的两个关键因素：核数、主频 根据不同的业务类型进行选择： cpu 密集型：计算比较多，OLTP 主频很高的 cpu、核数还要多 IO 密集型：查询比较，OLAP 核数要多，主频不一定高的 内存的选择： OLAP 类型数据库，需要更多内存，和数据获取量级有关。 OLTP 类型数据一般内存是 cpu 核心数量的 2 倍到 4 倍，没有最佳实践。 存储方面： 根据存储数据种类的不同，选择不同的存储设备 配置合理的 RAID 级别(raid5、raid10、热备盘) 对与操作系统来讲，不需要太特殊的选择，最好做好冗余（raid1）（ssd、sas 、sata） raid 卡：主机 raid 卡选择： 实现操作系统磁盘的冗余（raid1） 平衡内存和磁盘资源 随机的 I/O 和顺序的 I/O 主机 RAID 卡的 BBU(Battery Backup Unit)要关闭。 网络设备方面： 使用流量支持更高的网络设备（交换机、路由器、网线、网卡、HBA 卡） 注意：以上这些规划应该在初始设计系统时就应该考虑好。 5.3、服务器硬件优化 1、物理状态灯： 2、自带管理设备：远程控制卡（FENCE 设备：ipmi ilo idarc），开关机、硬件监控。 3、第三方的监控软件、设备（snmp、agent）对物理设施进行监控 4、存储设备：自带的监控平台。EMC2（hp 收购了）， 日立（hds），IBM 低端 OEM hds，高端存储是自己技术，华为存储 5.4、系统优化Cpu： 基本不需要调整，在硬件选择方面下功夫即可。 内存： 基本不需要调整，在硬件选择方面下功夫即可。 SWAP： MySQL 尽量避免使用 swap。阿里云的服务器中默认 swap 为 0 IO ： raid、no lvm、 ext4 或 xfs、ssd、IO 调度策略 Swap 调整(不使用 swap 分区) 这个参数决定了 Linux 是倾向于使用 swap，还是倾向于释放文件系统 cache。在内存紧张的情况下，数值越低越倾向于释放文件系统 cache。当然，这个参数只能减少使用 swap 的概率，并不能避免 Linux 使用 swap。修改 MySQL 的配置参数 innodb_flush_method，开启 O_DIRECT 模式。这种情况下，InnoDB 的 buffer pool 会直接绕过文件系统 cache 来访问磁盘，但是 redo log 依旧会使用文件系统 cache。值得注意的是，Redo log 是覆写模式的，即使使用了文件系统的 cache，也不会占用太多 IO 调度策略： 5.5、系统参数调整Linux 系统内核参数优化： 用户限制参数（mysql 可以不设置以下配置）： 5.6、应用优化 业务应用和数据库应用独立, 防火墙：iptables、selinux 等其他无用服务(关闭)： 安装图形界面的服务器不要启动图形界面 runlevel 3, 另外，思考将来我们的业务是否真的需要 MySQL，还是使用其他种类的数据库。用数据库的最高境界就是不用数据库。 六、数据库优化SQL 优化方向： 执行计划、索引、SQL 改写 架构优化方向： 高可用架构、高性能架构、分库分表 6.1、数据库参数优化 调整： 实例整体（高级优化，扩展） 连接层（基础优化） 设置合理的连接客户和连接方式 SQL 层（基础优化） query_cache_size： 查询缓存 OLAP 类型数据库, 需要重点加大此内存缓存. 但是一般不会超过 GB. 对于经常被修改的数据，缓存会立马失效。 我们可以实用内存数据库（redis、memecache），替代他的功能。 6.2、存储引擎层（innodb 基础优化参数） 推荐一本书籍《高性能 MySQL》，每一个程序员必看的一本经典书籍！]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>K8S</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CentOS 7 安装最新版 docker]]></title>
    <url>%2F2018%2F07%2F15%2FCentOS%207%20%E5%AE%89%E8%A3%85%E6%9C%80%E6%96%B0%E7%89%88docker%2F</url>
    <content type="text"><![CDATA[安装最新版 docker1、Docker 要求 CentOS 系统的内核版本高于 3.10 ，查看本页面的前提条件来验证你的 CentOS 版本是否支持 Docker 。 通过 uname -r 命令查看你当前的内核版本 uname -r 2、使用 root 权限登录 Centos。确保 yum 包更新到最新。 yum update 3、卸载旧版本(如果安装过旧版本的话) yum remove docker docker-common docker-selinux docker-engine 4、安装需要的软件包， yum-util 提供 yum-config-manager 功能，另外两个是 devicemapper 驱动依赖的 yum install -y yum-utils device-mapper-persistent-data lvm2 5、设置 yum 源 yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo 6、可以查看所有仓库中所有 docker 版本，并选择特定版本安装 yum list docker-ce --showduplicates | sort -r 7、安装 docker yum install docker-ce #由于 repo 中默认只开启 stable 仓库，故这里安装的是最新稳定版 18.06.0 或者指定 docker 版本安装： yum install &lt;FQPN&gt; # 例如：sudo yum install docker-ce-18.06.0.ce 8、启动并加入开机启动 systemctl start docker systemctl enable docker 9、验证安装是否成功(有 client 和 service 两部分表示 docker 安装启动都成功了) docker version 10、添加国内镜像 添加国内 docker 镜像地址：vim /etc/docker/daemon.json {&quot;registry-mirrors&quot;: [&quot;https://registry.docker-cn.com&quot;,&quot;http://hub-mirror.c.163.com&quot;,&quot;https://docker.mirrors.ustc.edu.cn&quot;] } 11、启动 docker systemctl daemon-reload systemctl restart docker]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Maven 私库 Nexus3 搭建使用]]></title>
    <url>%2F2018%2F06%2F20%2FMaven%E7%A7%81%E5%BA%93Nexus3%E6%90%AD%E5%BB%BA%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[Maven 私库 nexus3 搭建使用 docker 安装 sonatype/nexus31. 创建挂载目录 mkdir -p v-nexus/data 并修改目录权限 chown -R 200 v-nexus/data 2. 创建部署脚本 # 默认用户名 admin/admin123 version: &apos;3.2&apos; services: nexus: restart: always image: sonatype/nexus3 ports: #自定义端口 - target: 8081 published: 18081 #只有 worker 能访问该端口 protocol: tcp mode: host #版本要求 3.2 volumes: - &quot;/dockerdata/v-nexus/data:/nexus-data&quot; deploy: replicas: 1 restart_policy: condition: on-failure placement: constraints: [node.hostname == lfadmin] 3. 测试访问http://192.168.1.213:18081/ 然后输入 admin 和 admin123 进行登陆即可 ##win10 下 maven 安装 1. 下载 apache-maven-3.5.4-bin.zip 然后解压 2. 添加环境变量, 新建系统环境变量 Maven_HOME 值为解压路径，编辑 path 环境变量添加 %Maven_HOME%\bin 3. 命令窗口测试 mvn -v，只支持 cmd 4. 修改 apache-maven-3.5.4\conf\settings.xml 文件 &lt;!--jar 本地缓存地址 --&gt; &lt;localRepository&gt;D:\MavenRepository&lt;/localRepository&gt; 完整的 setting.xml 设置 &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;settings xmlns=&quot;http://maven.apache.org/SETTINGS/1.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/SETTINGS/1.0.0 http://maven.apache.org/xsd/settings-1.0.0.xsd&quot;&gt; &lt;!-- jar 本地缓存地址 --&gt; &lt;localRepository&gt;D:\MavenRepository&lt;/localRepository&gt; &lt;pluginGroups&gt; &lt;/pluginGroups&gt; &lt;proxies&gt; &lt;/proxies&gt; &lt;servers&gt; &lt;!-- 配置权限, 使用默认用户 --&gt; &lt;server&gt; &lt;!-- 这里的 id 要和项目里的 pom.xml 的 id 一致 --&gt; &lt;id&gt;nexus-releases&lt;/id&gt; &lt;username&gt;admin&lt;/username&gt; &lt;password&gt;admin123&lt;/password&gt; &lt;/server&gt; &lt;server&gt; &lt;id&gt;nexus-snapshots&lt;/id&gt; &lt;username&gt;admin&lt;/username&gt; &lt;password&gt;admin123&lt;/password&gt; &lt;/server&gt; &lt;/servers&gt; &lt;mirrors&gt; &lt;/mirrors&gt; &lt;profiles&gt; &lt;profile&gt; &lt;id&gt;MyNexus&lt;/id&gt; &lt;activation&gt; &lt;jdk&gt;1.4&lt;/jdk&gt; &lt;/activation&gt; &lt;repositories&gt; &lt;!-- 私有库地址 --&gt; &lt;repository&gt; &lt;id&gt;nexus&lt;/id&gt; &lt;name&gt;&gt;Nexus3 Repository&lt;/name&gt; &lt;!-- 注意修改成对应的 IP, 在 nexus 里面复制 public 里面的地址 --&gt; &lt;url&gt;http://192.168.1.213:18081/repository/maven-public/&lt;/url&gt; &lt;releases&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/releases&gt; &lt;!-- snapshots 默认是关闭的，需要手动开启 --&gt; &lt;snapshots&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/snapshots&gt; &lt;/repository&gt; &lt;/repositories&gt; &lt;pluginRepositories&gt; &lt;!-- 插件库地址 --&gt; &lt;pluginRepository&gt; &lt;id&gt;nexus&lt;/id&gt; &lt;url&gt;http://192.168.1.213:18081/repository/maven-public/&lt;/url&gt; &lt;releases&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/releases&gt; &lt;snapshots&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/snapshots&gt; &lt;/pluginRepository&gt; &lt;/pluginRepositories&gt; &lt;/profile&gt; &lt;/profiles&gt; &lt;!-- 激活 profile--&gt; &lt;activeProfiles&gt; &lt;activeProfile&gt;MyNexus&lt;/activeProfile&gt; &lt;/activeProfiles&gt; &lt;/settings&gt; 6. 在项目的 pom.xml 修改或添加如下配置 &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;project ...&gt; .... &lt;!-- 配置 maven 地址 --&gt; &lt;distributionManagement&gt; &lt;repository&gt; &lt;!-- 这里的 id 要和 maven 里的的 settings.xml 的 id 一致 --&gt; &lt;id&gt;nexus-releases&lt;/id&gt; &lt;name&gt;Nexus Release Repository&lt;/name&gt; &lt;url&gt;http://192.168.1.213:18081/repository/maven-releases/&lt;/url&gt; &lt;/repository&gt; &lt;snapshotRepository&gt; &lt;id&gt;nexus-snapshots&lt;/id&gt; &lt;name&gt;Nexus Snapshot Repository&lt;/name&gt; &lt;url&gt;http://192.168.1.213:18081/repository/maven-snapshots/&lt;/url&gt; &lt;/snapshotRepository&gt; &lt;/distributionManagement&gt; ... &lt;/project&gt; 7. 编译在 cmd 执行 mvn install 发布上传 jar 执行 mvn deploy，可以到 nexus 地址进行检查 8. 使用私库下载和上传是一样的 nexus3 配置阿里云代理仓库1. 点击Create Repository-&gt;maven2(proxy) 2. 添加名字 aliyun-proxy 设置阿里云 url 地址http://maven.aliyun.com/nexus/content/groups/public 3. 设置阿里云优先级，在 maven-public 里面的 group 把刚刚创建的添加过去并移到 maven-central 上面 4. 设置允许发布 release, 在 maven-release 的 hosted 里面选择 allow redeploy 发布上传 jar 包到 nexus语法： mvn deploy:deploy-file \ -DgroupId=&lt;group-id&gt; \ -DartifactId=&lt;artifact-id&gt; \ -Dversion=&lt;version&gt; \ -Dpackaging=&lt;type-of-packaging&gt; \ -Dfile=&lt;path-to-file&gt; \ -DrepositoryId=&lt; 这里的 id 要和 maven 里的的 settings.xml 的 id 一致 &gt; \ -Durl=&lt;url-of-the-repository-to-deploy&gt; 实战 mvn deploy:deploy-file \ -Dfile=spring-boot-starter-druid-0.0.1-SNAPSHOT.jar \ -DgroupId=cn.binux \ -DartifactId=spring-boot-starter-druid \ -Dversion=0.0.1-SNAPSHOT \ -Dpackaging=jar \ -DpomFile=spring-boot-starter-druid-0.0.1-SNAPSHOT.pom \ -DrepositoryId=nexus-snapshots \ -Durl=http://192.168.1.213:18081/repository/maven-snapshots/ 上传 jar 包到私有 maven 仓库 mvn deploy:deploy-file -Dfile=spring-boot-starter-druid-0.0.1-SNAPSHOT.jar -DgroupId=cn.binux -DartifactId=spring-boot-starter-druid -Dversion=0.0.1-SNAPSHOT -Dpackaging=jar -DpomFile=spring-boot-starter-druid-0.0.1-SNAPSHOT.pom -DrepositoryId=nexus-snapshots -Durl=http://192.168.1.213:18081/repository/maven-snapshots/ mvn deploy:deploy-file -Dfile=spring-boot-starter-dubbox-0.0.1-SNAPSHOT.jar -DgroupId=cn.binux -DartifactId=spring-boot-starter-dubbox -Dversion=0.0.1-SNAPSHOT -Dpackaging=jar -DpomFile=spring-boot-starter-dubbox-0.0.1-SNAPSHOT.pom -DrepositoryId=nexus-snapshots -Durl=http://192.168.1.213:18081/repository/maven-snapshots/ mvn deploy:deploy-file -Dfile=spring-boot-starter-redis-0.0.1-SNAPSHOT.jar -DgroupId=cn.binux -DartifactId=spring-boot-starter-redis -Dversion=0.0.1-SNAPSHOT -Dpackaging=jar -DpomFile=spring-boot-starter-redis-0.0.1-SNAPSHOT.pom -DrepositoryId=nexus-snapshots -Durl=http://192.168.1.213:18081/repository/maven-snapshots/ #这个不是 snapshots 要发布到 releases，注意设置 nexus 为允许发布，看 jar 报后缀，没有 `SNAPSHOT` 就是 release mvn deploy:deploy-file -Dfile=dubbo-2.8.4.jar -DgroupId=com.alibaba -DartifactId=dubbo -Dversion=2.8.4 -Dpackaging=jar -DrepositoryId=nexus-releases -Durl=http://192.168.1.213:18081/repository/maven-releases/ mvn deploy:deploy-file -Dfile=fastdfs-1.24.jar -DgroupId=org.csource -DartifactId=fastdfs -Dversion=1.24 -Dpackaging=jar -DrepositoryId=nexus-releases -Durl=http://192.168.1.213:18081/repository/maven-releases/ mvn deploy:deploy-file -Dfile=examples-1.0.jar -DgroupId=com.haikang -DartifactId=examples -Dversion=1.0 -Dpackaging=jar -DrepositoryId=nexus-releases -Durl=http://192.168.1.230:18081/repository/maven-releases/ 本地安装 jar 包到本地 maven 仓库 mvn install:install-file -Dfile=spring-boot-starter-druid-0.0.1-SNAPSHOT.jar -DgroupId=cn.binux -DartifactId=spring-boot-starter-druid -Dversion=0.0.1-SNAPSHOT -Dpackaging=jar mvn install:install-file -Dfile=spring-boot-starter-dubbox-0.0.1-SNAPSHOT.jar -DgroupId=cn.binux -DartifactId=spring-boot-starter-dubbox -Dversion=0.0.1-SNAPSHOT -Dpackaging=jar mvn install:install-file -Dfile=spring-boot-starter-redis-0.0.1-SNAPSHOT.jar -DgroupId=cn.binux -DartifactId=spring-boot-starter-redis -Dversion=0.0.1-SNAPSHOT -Dpackaging=jar mvn install:install-file -Dfile=dubbo-2.8.4.jar -DgroupId=com.alibaba -DartifactId=dubbo -Dversion=2.8.4 -Dpackaging=jar mvn install:install-file -Dfile=fastdfs-1.24.jar -DgroupId=org.csource -DartifactId=fastdfs -Dversion=1.24 -Dpackaging=jar 问题 下载了找不到包，解决，删除项目重新导入，重新 maven 依赖 刚上传或添加了新的 jar 到私库，无法下载，解决，删除本地仓库的该包目录]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>工具</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一起学 docker(十) --jenkins 的使用]]></title>
    <url>%2F2018%2F06%2F02%2F%E4%B8%80%E8%B5%B7%E5%AD%A6docker(%E5%8D%81)%20--jenkins%E7%9A%84%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[什么是 Jenkins?jenkins 是一个广泛用于持续构建的可视化 web 工具，持续构建说得更直白点，就是各种项目的”自动化”编译、打包、分发部署。jenkins 可以很好的支持各种语言（比如：java, c#, php 等）的项目构建，也完全兼容 ant、maven、gradle 等多种第三方构建工具，同时跟 svn、git 能无缝集成，也支持直接与知名源代码托管网站，比如 github、bitbucket 直接集成。 jenkins 官网地址为https://jenkins.io/，jenkins 本身是用 java 语言开发的，所以安装 jenkins 的机器至少要有 jdk，另外建议 git、ant、maven、gradle、groovy 等工具也一并安装好，方便与这些构建工具集成。 Jenkins 使用流程 1. 安装 安装 java: sudo yum install -y java 设置 Jenkins 源： 参考：http://pkg.jenkins-ci.org/redhat/ 下载 jenkins.repo 定义源： sudo wget -O /etc/yum.repos.d/jenkins.repohttp://pkg.jenkins.io/redhat/jenkins.repo 可查看定义的源： 导入 jenkins key： sudo rpm --import http://pkg.jenkins.io/redhat/jenkins.io.key 安装 jenkins: yum install jenkins 参考地址：http://pkg.jenkins-ci.org/redhat/ 2. 启动 jenkinssudo service jenkins start 3. 查看 jenkins 进程状态systemctl status jenkins 4. 配置文件jenkins 默认配置文件是 /etc/sysconfig/jenkins 日志目录 tail –f /var/log/jenkins/jenkins.log 5. 访问 jenkins 服务http://IP:8080 注意：如果第一次启动的时候访问失败，可以执行 systemctl restart jenkins 重启 Jenkins 就可以了 6. 初始化权限配置 执行提示命令获取管理员密码： Jenkins 实例离线可参考我的另一篇文章：Jenkins 实例离线问题 7. 熟悉基本组件Jenkins 集成了很多组建，我们默认选择系统建议的组建安装。 创建新用户： 安全配置 创建演示任务(利用 maven 编译项目) 任务名称：hello-jenkins 任务配置： 保存并构建： 构建并执行成功：]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一起学 docker(九) -- 持续系统集成了解下 git]]></title>
    <url>%2F2018%2F05%2F28%2F%E4%B8%80%E8%B5%B7%E5%AD%A6docker(%E4%B9%9D)%20%20--%E6%8C%81%E7%BB%AD%E7%B3%BB%E7%BB%9F%E9%9B%86%E6%88%90%E4%BA%86%E8%A7%A3%E4%B8%8Bgit%2F</url>
    <content type="text"><![CDATA[什么是持续集成？ 持续集成（Continuous integration，简称 CI）。 根据敏捷大师 Martin Fowler 的定义，“持续集成是一种软件开发实践。在持续集成中，团队成员频繁集成他们的工作成果，一般每人每天至少集成一次，也可以多次。每次集成会经过自动构建（包括自动测试）的检验，以尽快发现集成错误。许多团队发现这种方法可以显著减少集成引起的问题，并可以加快团队合作软件开发的速度。 为什么要持续集成？1 快速发现错误：每完成一点更新，就集成到主干，可以快速发现错误，定位错误也比较容易。 2 防止分支大幅偏离主干：如果不是经常集成，主干又在不断更新，会导致以后集成的难度变大，甚至难以集成。 下面是持续集成的图谱介绍： 1 将更改提交到代码管理仓库 2 持续集成服务器收到请求拉取变更代码 3 持续集成服务器编译代码 4 持续集成服务器跑代码相关测试 5 持续集成服务器测试结束 6 持续集成服务器对结果进行反馈 Docker 在持续集成中的作用：Docker 提供代码编译、打包、测试的相关环境。 优势： 1 环境可以是是任意版本 2 节省空间 3 环境相对隔离 什么是 Git? Git 是一个开源的分布式版本控制系统，可以有效、高速的处理从很小到非常大的项目版本管理。Git 是 Linus Torvalds 为了帮助管理 Linux 内核开发而开发的一个开放源码的版本控制软件。 GitHub 官网注册：https://github.com/ Git 客户端安装：yum install –y git Git 常用命令:1 初始化 git 项目 git init 2 查看当前项目状态 git status 3 新建文件并再次查看状态 echo “# My Project” &gt; README.md git status 4 记录当前操作，记录新加入的文件并再次查看状态 git add README.md git status 5 记录当前更改并加以信息描述 git commit 文件名 -m’add my first project’ 6 查看提交历史 git log 7 新建远程仓库 git remote add origin https://github.com/limingios/git-test.git 8 同步到远程仓库 git push -u origin master 9 从远程代码库同步到本地 git pull origin master 10 与同步前对比变更 git diff HEAD 11 查看当前更改变更 git diff --staged 12 恢复到为更改状态 git reset README.md 13 覆盖本地文件 git checkout octocat.txt 14 新建分支 git branch feature1 15 切换分支 git checkout feature1 16 删除本地分支 git branch –d feature1 Git hook 配置 Git 也具有在特定事件发生之前或之后执行特定脚本代码功能（从概念上类比，就与监听事件、触发器之类的东西类似）。Git Hooks 就是那些在 Git 执行特定事件（如 commit、push、receive 等）后触发运行的脚本。 按照 Git Hooks 脚本所在的位置可以分为两类： 本地 Hooks，触发事件如 commit、merge 等。 服务端 Hooks，触发事件如 receive 等。 下图为 github 的 webhooks: 下图为 gitlab 的 webhooks: 两者在使用上并没有什么不同，使用命令也是完全兼容的。]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一起学 docker(八) --Dockerfile]]></title>
    <url>%2F2018%2F05%2F25%2F%E4%B8%80%E8%B5%B7%E5%AD%A6docker%20%20%20(%E5%85%AB)%20%20--Dockerfile%2F</url>
    <content type="text"><![CDATA[在 Docker 的运用中，从下载镜像，启动容器，在容器中输入命令来运行程序，这些命令都是手工一条条往里输入的，无法重复利用，而且效率很低。所以就需要一 种文件或脚本，我们把想执行的操作以命令的方式写入其中，然后让 docker 读取并分析、执行，那么重复构建、更新将变得很方便，所以 Dockerfile 就此诞生了。Docker 提供了 Dockerfile 作为构建 Docker 镜像脚本，避免人们一行一行的输入，真是善莫大焉。Dockerfile 脚本可以做到随时维护修改，即可以分享，更有利于在模板化，更不用说传输了，好处那是一大箩筐！下面就详细介绍下 Dockfile 的使用： Dockfile 是一种被 Docker 程序解释的脚本，它由一条条的指令组成，每条指令对应 Linux 下面的一条命令。Docker 程序将这些 Dockerfile 指令翻译成真正的 Linux 命令。Dockerfile 有自己书写格式和支持的命令，Docker 程序解决这些命令间的依赖关系，类似于 Makefile。Docker 程序将读取 Dockerfile，根据指令生成定制的 image。相比 image 这种黑盒子，Dockerfile 这种显而易见的脚本更容易被使用者接受，它明确的表明 image 是怎么产生的。有了 Dockerfile，当我们需要定制自己额外的需求时，只需在 Dockerfile 上添加或者修改指令，重新生成 image 即可，省去了敲命令的麻烦。 总的来说：Dockerfile 分为四部分：基础镜像信息、镜像创建者信息、镜像操作指令、容器启动执行指令。 一开始必须要指明所基于的镜像名称，接下来一般会说明镜像创建者信息。后面则是镜像操作指令 Dockerfile 的书写规则及指令使用方法 Dockerfile 的指令是忽略大小写的，建议使用大写，使用 # 作为注释，每一行只支持一条指令，每条指令可以携带多个参数。Dockerfile 的指令根据作用可以分为两种：构建指令和设置指令。 构建指令用于构建 image，其指定的操作不会在运行 image 的容器上执行；设置指令用于设置 image 的属性，其指定的操作将在运行 image 的容器中执行。 FROM（指定基础 image）构建指令，必须指定且需要在 Dockerfile 其他指令的前面。后续的指令都依赖于该指令指定的 image。FROM 指令指定的基础 image 可以是官方远程仓库中的，也可以位于本地仓库。 FROM 命令告诉 docker 我们构建的镜像是以哪个 (发行版) 镜像为基础的。第一条指令必须是 FROM 指令。并且，如果在同一个 Dockerfile 中创建多个镜像时，可以使用多个 FROM 指令。 该指令有两种格式： FROM &lt;image&gt; 指定基础 image 为该 image 的最后修改的版本。或者： FROM &lt;image&gt;:&lt;tag&gt; 指定基础 image 为该 image 的一个 tag 版本。 RUN 后面接要执行的命令，比如，我们想在镜像中安装vim，只需在 Dockfile 中写入RUN yum install -y vim MAINTAINER（用来指定镜像创建者信息）构建指令，用于将 image 的制作者相关的信息写入到 image 中。当我们对该 image 执行 docker inspect 命令时，输出中有相应的字段记录该信息。 格式： MAINTAINER &lt;name&gt; RUN（安装软件用）构建指令，RUN 可以运行任何被基础 image 支持的命令。如基础 image 选择了 ubuntu，那么软件管理部分只能使用 ubuntu 的命令。 该指令有两种格式： RUN &lt;command&gt; RUN [&quot;executable&quot;, &quot;param1&quot;, &quot;param2&quot; ...] CMD（设置 container 启动时执行的操作）设置指令，用于 container 启动时指定的操作。该操作可以是执行自定义脚本，也可以是执行系统命令。该指令只能在文件中存在一次，如果有多个，则只执行最后一条。 该指令有三种格式： CMD [&quot;executable&quot;,&quot;param1&quot;,&quot;param2&quot;] CMD command param1 param2 当 Dockerfile 指定了 ENTRYPOINT，那么使用下面的格式： CMD [&quot;param1&quot;,&quot;param2&quot;] 其中： ENTRYPOINT 指定的是一个可执行的脚本或者程序的路径，该指定的脚本或者程序将会以 param1 和param2作为参数执行。 所以如果 CMD 指令使用上面的形式，那么 Dockerfile 中必须要有配套的ENTRYPOINT。 ENTRYPOINT（设置 container 启动时执行的操作）设置指令，指定容器启动时执行的命令，可以多次设置，但是只有最后一个有效。 两种格式: ENTRYPOINT [&quot;executable&quot;, &quot;param1&quot;, &quot;param2&quot;] ENTRYPOINT command param1 param2 该指令的使用分为两种情况，一种是独自使用，另一种和 CMD 指令配合使用。当独自使用时，如果你还使用了 CMD 命令且 CMD 是一个完整的可执行的命令，那么 CMD 指令和 ENTRYPOINT 会互相覆盖，只有最后一个 CMD 或者 ENTRYPOINT 有效。CMD 指令将不会被执行，只有 ENTRYPOINT 指令被执行 CMD echo “Hello, World!” ENTRYPOINT ls -l 另一种用法和 CMD 指令配合使用来指定 ENTRYPOINT 的默认参数，这时 CMD 指令不是一个完整的可执行命令，仅仅是参数部分；ENTRYPOINT 指令只能使用 JSON 方式指定执行命令，而不能指定参数。 FROM ubuntu CMD [&quot;-l&quot;] ENTRYPOINT [&quot;/usr/bin/ls&quot;] USER（设置 container 容器的用户）设置指令，设置启动容器的用户，默认是 root 用户。例如下面指定 memcached 的运行用户 ENTRYPOINT [&quot;memcached&quot;] USER daemon 或者 ENTRYPOINT [&quot;memcached&quot;, &quot;-u&quot;, &quot;daemon&quot;] EXPOSE（指定容器需要映射到宿主机器的端口）设置指令，该指令会将容器中的端口映射成宿主机器中的某个端口。当你需要访问容器的时候，可以不是用容器的 IP 地址而是使用宿主机器的 IP 地址和映射后的端口。 要完成整个操作需要两个步骤，首先在 Dockerfile 使用 EXPOSE 设置需要映射的容器端口，然后在运行容器的时候指定 -p 选项加上 EXPOSE 设置的端口，这样 EXPOSE 设置的端口号会被随机映射成宿主机器中的一个端口号。 也可以指定需要映射到宿主机器的那个端口，这时要确保宿主机器上的端口号没有被使用。EXPOSE 指令可以一次设置多个端口号，相应的运行容器的时候，可以配套的多次使用 -p 选项。 格式: EXPOSE &lt;port&gt; [&lt;port&gt;...] # 映射一个端口 EXPOSE port1 # 相应的运行容器使用的命令 docker run -p port1 image # 映射多个端口 EXPOSE port1 port2 port3 # 相应的运行容器使用的命令 docker run -p port1 -p port2 -p port3 image # 还可以指定需要映射到宿主机器上的某个端口号 docker run -p host_port1:port1 -p host_port2:port2 -p host_port3:port3 image 端口映射是 docker 比较重要的一个功能，原因在于我们每次运行容器的时候容器的 IP 地址不能指定而是在桥接网卡的地址范围内随机生成的。 宿主机器的 IP 地址是固定的，我们可以将容器的端口的映射到宿主机器上的一个端口，免去每次访问容器中的某个服务时都要查看容器的 IP 的地址。 对于一个运行的容器，可以使用 docker port 加上容器中需要映射的端口和容器的 ID 来查看该端口号在宿主机器上的映射端口。 ENV（用于设置环境变量）主要用于设置容器运行时的环境变量 格式: ENV &lt;key&gt; &lt;value&gt; 设置了后，后续的 RUN 命令都可以使用，container 启动后，可以通过 docker inspect 查看这个环境变量，也可以通过在 docker run –env key=value 时设置或修改环境变量。 假如你安装了 JAVA 程序，需要设置 JAVA_HOME，那么可以在 Dockerfile 中这样写：ENV JAVA_HOME /path/to/java/dirent ADD（从 src 复制文件到 container 的 dest 路径）主要用于将宿主机中的文件添加到镜像中构建指令，所有拷贝到 container 中的文件和文件夹权限为 0755，uid 和 gid 为 0；如果是一个目录，那么会将该目录下的所有文件添加到 container 中，不包括目录； 如果文件是可识别的压缩格式，则 docker 会帮忙解压缩（注意压缩格式）；如果 是文件且 中不使用斜杠结束，则会将 视为文件，的内容会写入 ；如果 是文件且 中使用斜杠结束，则会 文件拷贝到 目录下。 格式: ADD &lt;src&gt; &lt;dest&gt; 是相对被构建的源目录的相对路径，可以是文件或目录的路径，也可以是一个远程的文件 url; 是 container 中的绝对路径 VOLUME（指定挂载点)）设置指令，使容器中的一个目录具有持久化存储数据的功能，该目录可以被容器本身使用，也可以共享给其他容器使用。我们知道容器使用的是 AUFS，这种文件系统不能持久化数据，当容器关闭后，所有的更改都会丢失。当容器中的应用有持久化数据的需求时可以在 Dockerfile 中使用该指令。 格式: VOLUME [&quot;&lt;mountpoint&gt;&quot;] 例如： FROM base VOLUME [&quot;/tmp/data&quot;] 运行通过该 Dockerfile 生成 image 的容器，/tmp/data 目录中的数据在容器关闭后，里面的数据还存在。例如另一个容器也有持久化数据的需求，且想使用上面容器共享的 /tmp/data 目录，那么可以运行下面的命令启动一个容器： docker run -t -i -rm -volumes-from container1 image2 bash 其中：container1为第一个容器的 ID，image2 为第二个容器运行 image 的名字。 WORKDIR（切换目录）设置指令，可以多次切换(相当于 cd 命令)，对 RUN,CMD,ENTRYPOINT 生效。 格式: WORKDIR /path/to/workdir # 在 /p1/p2 下执行 vim a.txt WORKDIR /p1 WORKDIR p2 RUN vim a.txt ONBUILD（在子镜像中执行）格式： ONBUILD &lt;Dockerfile 关键字 &gt; ONBUILD 指定的命令在构建镜像时并不执行，而是在它的子镜像中执行。 Dockerfile 使用实例1）利用 dockerfile 部署 jdk1.7+tomcat7 服务环境1）查看 docker 宿主机镜像 [root@localhost ~]# docker images REPOSITORY TAG IMAGE IDCREATED SIZE docker.io/ubuntu latest 0ef2e08ed3fa6 weeks ago 130 MB docker.io/centos latest 67591570dd293 months ago191.8 MB docker.io/registry 2.2 ad379b517aa614 months ago 224.5 MB 2）编写 Dockerfile（注意里面的 ubuntu 的源要换成国内的，这里我换成了国内阿里的 ubuntu 源） [root@localhost ~]# vim Dockerfile # Pull base image FROM docker.io/ubuntu:latest MAINTAINER yhaing &quot;yhaing@sina.com&quot; # update source RUN echo &quot;deb http://mirrors.aliyun.com/ubuntu/ xenial main restricted universe multiverse&quot;&gt; /etc/apt/sources.list RUN apt-get update # Install curl RUN apt-get -y install curl # Install JDK 7 RUN cd /tmp &amp;&amp; curl -L &apos;http://download.oracle.com/otn-pub/java/jdk/7u65-b17/jdk-7u65-linux-x64.tar.gz&apos; -H &apos;Cookie: oraclelicense=accept-securebackup-cookie; gpw_e24=Dockerfile&apos; | tar -xz RUN mkdir -p /usr/lib/jvm RUN mv /tmp/jdk1.7.0_65/ /usr/lib/jvm/java-7-oracle/ # Set Oracle JDK 7 as default Java RUN update-alternatives --install /usr/bin/java java /usr/lib/jvm/java-7-oracle/bin/java 300 RUN update-alternatives --install /usr/bin/javac javac /usr/lib/jvm/java-7-oracle/bin/javac 300 ENV JAVA_HOME /usr/lib/jvm/java-7-oracle/ # Install tomcat7 RUN cd /tmp &amp;&amp; curl -L &apos;http://archive.apache.org/dist/tomcat/tomcat-7/v7.0.8/bin/apache-tomcat-7.0.8.tar.gz&apos; | tar -xz RUN mv /tmp/apache-tomcat-7.0.8/ /opt/tomcat7/ ENV CATALINA_HOME /opt/tomcat7 ENV PATH $PATH:$CATALINA_HOME/bin ADD tomcat7.sh /etc/init.d/tomcat7 RUN chmod 755 /etc/init.d/tomcat7 # Expose ports. EXPOSE 8080 # Define default command. ENTRYPOINT service tomcat7 start &amp;&amp; tail -f /opt/tomcat7/logs/catalina.out 3）编写 tomcat7.sh [root@localhost ~]# vim tomcat7.sh export JAVA_HOME=/usr/lib/jvm/java-7-oracle/ export TOMCAT_HOME=/opt/tomcat7 case $1 in start) sh $TOMCAT_HOME/bin/startup.sh ;; stop) sh $TOMCAT_HOME/bin/shutdown.sh ;; restart) sh $TOMCAT_HOME/bin/shutdown.sh sh $TOMCAT_HOME/bin/startup.sh ;; esac exit 0 4）构建镜像DOckerfile 脚本写好了，需要转换成镜像： [root@localhost ~]# docker build -t yhaing/jdk-tomcat --rm=true . ........ Removing intermediate container 09cfba8ebc6b Successfully built 76b10dd9923f 其中： -t 表示选择指定生成镜像的用户名，仓库名和 tag –rm=true 表示指定在生成镜像过程中删除中间产生的临时容器。 注意：上面构建命令中最后的. 符号不要漏了，表示使用当前目录下的 Dockerfile 构建镜像 以上构建命令执行后，可以查看下镜像是否构建成功 [root@localhost ~]# docker images REPOSITORY TAG IMAGE IDCREATED SIZE yhaing/jdk-tomcat latest 76b10dd9923f21 minutes ago 771.5 MB docker.io/ubuntu latest 0ef2e08ed3fa6 weeks ago 130 MB docker.io/centos latest 67591570dd293 months ago191.8 MB docker.io/registry 2.2 ad379b517aa614 months ago 224.5 MB 最后利用这个镜像启动容器 [root@localhost ~]# docker run -ti -d --name yhaing-tomcat -p 8899:8080 yhaing/jdk-tomcat /bin/bash c0812ad20bed2f27787565d273f7b02d860de5afab88e853e591dde7d3b0dfc9 [root@localhost ~]# docker ps CONTAINER IDIMAGE COMMAND CREATED STATUS PORTSNAMES c0812ad20bed yhaing/jdk-tomcat&quot;/bin/sh -c &apos;service &quot; 7 seconds ago Up 6 seconds0.0.0.0:8899-&gt;8080/tcp yhaing-tomcat 进入容器，查看 tomcat 进程起来了没 [root@localhost ~]# docker exec -ti yhaing-tomcat /bin/bash root@c0812ad20bed:/# ps -ef|grep tomcat root 1 0 0 06:49 ?00:00:00 /bin/sh -c service tomcat7 start &amp;&amp; tail -f /opt/tomcat7/logs/catalina.out /bin/bash root23 1 11 06:49 ?00:00:04 /usr/lib/jvm/java-7-oracle//bin/java -Djava.util.logging.config.file=/opt/tomcat7/conf/logging.properties -Djava.util.logging.manager=org.apache.juli.ClassLoaderLogManager -Djava.endorsed.dirs=/opt/tomcat7/endorsed -classpath /opt/tomcat7/bin/bootstrap.jar:/opt/tomcat7/bin/tomcat-juli.jar -Dcatalina.base=/opt/tomcat7 -Dcatalina.home=/opt/tomcat7 -Djava.io.tmpdir=/opt/tomcat7/temp org.apache.catalina.startup.Bootstrap start root24 1 0 06:49 ?00:00:00 tail -f /opt/tomcat7/logs/catalina.out root6953 0 06:49 ?00:00:00 grep --color=auto tomcat 最后访问 http:// 本机 ip:8899 就能打开容器的 tomcat 页面了 Docker 容器创建好之后，尽量不要直接登陆容器内去修改。所以最好容器创建的时候进行目录映射。这样就可以通过映射到宿主机上的文件或目录去共享到容器内。 则上面的 yhaing-tomcat 容器可以如下调整操作： [root@localhost ~]# docker ps CONTAINER IDIMAGE COMMAND CREATED STATUS PORTSNAMES c0812ad20bed yhaing/jdk-tomcat&quot;/bin/sh -c &apos;service &quot; 7 seconds ago Up 6 seconds0.0.0.0:8899-&gt;8080/tcp yhaing-tomcat [root@localhost ~]# docker cp yhaing-tomcat:/opt/tomcat7/webapps /opt/ [root@localhost ~]# docker run -ti -d --name yhaing-tomcat -v /opt/webapps:/opt/tomcat7/webapps -p 8899:8080 yhaing/jdk-tomcat /bin/bash 1373d1496c2a6226fe5bb6b4877e854bde68ec3653c04966a1b5d22b98486f7d [root@localhost ~]# docker ps CONTAINER IDIMAGE COMMAND CREATED STATUS PORTSNAMES 1373d1496c2a yhaing/jdk-tomcat&quot;/bin/sh -c &apos;service &quot; 4 seconds ago Up 2 seconds0.0.0.0:8899-&gt;8080/tcp yhaing-tomcat 这样让需要修改 yhaing-tomcat 容器的代码或上线代码时，只需要操作宿主机的 /opt/webapps 目录即可。 删除 docker images 中为 none 的镜像 经常使用 Dockerfile 制作镜像，Docker build 命令执行后，由于版本更新需要重新创建，那么以前那个版本的镜像就会成为临时镜像，这就是 none 标签的镜像。，如下： [root@localhost ~]# docker images REPOSITORY TAG IMAGE IDCREATED SIZE yhaing/jdk-tomcat latest 76b10dd9923fAbout an hour ago 771.5 MB &lt;none&gt; &lt;none&gt; 571685f2919c2 hours ago 130 MB &lt;none&gt; &lt;none&gt; affbf24261bd2 hours ago 130 MB &lt;none&gt; &lt;none&gt; 8eee83c3b41d10 days ago 8.096 MB &lt;none&gt; &lt;none&gt; b94d96e1464410 days ago 8.096 MB docker.io/ubuntu latest 0ef2e08ed3fa6 weeks ago 130 MB docker.io/centos latest 67591570dd293 months ago191.8 MB docker.io/registry 2.2 ad379b517aa614 months ago 224.5 MB 对于这些 none 标签的 images，可以通过下面的脚本进行删除（如果无法删除 none 的 images，一般重启 docker 服务后即可解决）： [root@localhost ~]# vim none_images_rm.sh #!/bin/bash docker ps -a | grep &quot;Exited&quot; | awk &apos;{print $1}&apos;|xargs docker stop docker ps -a | grep &quot;Exited&quot; | awk &apos;{print $1}&apos;|xargs docker rm docker images|grep none|awk &apos;{print $3}&apos;|xargs docker rmi [root@localhost ~]# chmod 755 none_images_rm.sh [root@localhost ~]# sh none_images_rm.sh [root@localhost ~]# docker images REPOSITORY TAG IMAGE IDCREATED SIZE yhaing/jdk-tomcat latest 76b10dd9923fAbout an hour ago 771.5 MB docker.io/centos latest 67591570dd293 months ago191.8 MB docker.io/registry 2.2 ad379b517aa614 months ago 224.5 MB 2）再看一例 tomcat 容器镜像的 Dockerfile 制作过程（centos 为 base 镜像）[root@localhost ~]# docker images REPOSITORY TAG IMAGE IDCREATED SIZE docker.io/centos latest 67591570dd293 months ago191.8 MB 提前下载好 tomcat 和 java 安装包，放在 Docker 宿主机的 /usr/local/src 目录下： [root@localhost src]# ls apache-tomcat-7.0.67.tar.gz jdk-7u79-linux-x64.tar.gz 在 /usr/local/src 当前目录下编辑 Dockerfile。如下：即将宿主机本地的 tomcat 和 java 安装包拷贝到容器内，并自动解压。 [root@localhost src]# vim Dockerfile #pull down centos image FROM docker.io/centos MAINTAINER yhaing yhaing@163.com #copy jdk and tomcat into image ADD ./apache-tomcat-7.0.67.tar.gz /usr/local ADD ./jdk-7u79-linux-x64.tar.gz /usr/local #set environment variable ENV JAVA_HOME /usr/local/jdk1.7.0_79 ENV PATH $JAVA_HOME/bin:$PATH #define entry point which will be run first when the container starts up ENTRYPOINT /usr/local/apache-tomcat-7.0.67/bin/startup.sh &amp;&amp; tail -F /usr/local/apache-tomcat-7.0.67/logs/catalina.out 接着构建镜像 [root@localhost src]# docker build -t kevin_tomcat7 --rm=true . [root@localhost src]# docker images REPOSITORY TAG IMAGE IDCREATED SIZE kevin_tomcat7 latest 8f242680d94011 seconds ago 511.6 MB docker.io/centos latest 67591570dd293 months ago191.8 MB 根据制作的镜像启动 tomcat 容器 [root@localhost src]# docker run -ti -d --name tomcat-test -p 8899:8080 kevin_tomcat7 /bin/bash 22dece8d6660b61677bf89137d5d21548c2f0b0fd337ce5a1e12ef6d2000091a [root@localhost src]# docker ps CONTAINER IDIMAGE COMMAND CREATED STATUS PORTSNAMES 22dece8d6660kevin_tomcat7 &quot;/bin/sh -c &apos;/usr/loc&quot; 4 seconds ago Up 4 seconds0.0.0.0:8899-&gt;8080/tcp tomcat-test [root@localhost src]# docker exec -ti tomcat-test /bin/bash [root@22dece8d6660 /]# ps -ef|grep tomcat root 1 0 0 08:59 ?00:00:00 /bin/sh -c /usr/local/apache-tomcat-7.0.67/bin/startup.sh &amp;&amp; tail -F /usr/local/apache-tomcat-7.0.67/logs/catalina.out /bin/bash root20 1 9 08:59 ?00:00:05 /usr/local/jdk1.7.0_79/bin/java -Djava.util.logging.config.file=/usr/local/apache-tomcat-7.0.67/conf/logging.properties -Djava.util.logging.manager=org.apache.juli.ClassLoaderLogManager -Djava.endorsed.dirs=/usr/local/apache-tomcat-7.0.67/endorsed -classpath /usr/local/apache-tomcat-7.0.67/bin/bootstrap.jar:/usr/local/apache-tomcat-7.0.67/bin/tomcat-juli.jar -Dcatalina.base=/usr/local/apache-tomcat-7.0.67 -Dcatalina.home=/usr/local/apache-tomcat-7.0.67 -Djava.io.tmpdir=/usr/local/apache-tomcat-7.0.67/temp org.apache.catalina.startup.Bootstrap start root21 1 0 08:59 ?00:00:00 tail -F /usr/local/apache-tomcat-7.0.67/logs/catalina.out root6747 0 09:00 ?00:00:00 grep --color=auto tomcat 3）使用 Dockerfile 制作 nginx 镜像[root@localhost mnt]# docker images REPOSITORY TAG IMAGE IDCREATED SIZE docker.io/centos latest 67591570dd293 months ago191.8 MB 制作 Dockerfile [root@localhost mnt]# vim Dockerfile #pull down centos image FROM docker.io/centos MAINTAINER yhaing yhaing@163.com #install nginx RUN yum install -y pcre pcre-devel openssl openssl-devel gcc gcc+ wget vim net-tools RUN useradd www -M -s /sbin/nologin RUN cd /usr/local/src &amp;&amp; wget http://nginx.org/download/nginx-1.8.0.tar.gz &amp;&amp; tar -zxvf nginx-1.8.0.tar.gz RUN cd /usr/local/src/nginx-1.8.0 &amp;&amp; ./configure --prefix=/usr/local/nginx --user=www --group=www --with-http_stub_status_module --with-http_ssl_module &amp;&amp; make &amp;&amp; make install ENTRYPOINT /usr/local/nginx/sbin/nginx &amp;&amp; tail -f /usr/local/nginx/logs/access.log 特别需要注意的：在 Docker daemon 模式下，无论你是使用 ENTRYPOINT，还是 CMD，最后的命令，一定要是当前进程需要一直运行的，才能够防容器退出。也就是说，上面 Dockerfile 脚本中最后一行： 以下无效方式： ENTRYPOINT /usr/local/nginx/sbin/nginx #运行几秒钟之后，容器就会退出 或者 CMD /usr/local/nginx/sbin/nginx #运行几秒钟之后，容器就会退出 以下才是有效方式： ENTRYPOINT /usr/local/nginx/sbin/nginx &amp;&amp; tail -f /usr/local/nginx/logs/access.log #确保容器内的进程一直运行 或者 CMD /usr/local/nginx/sbin/nginx &amp;&amp; tail -f /usr/local/nginx/logs/access.log #确保容器内的进程一直运行 其他应用程序镜像创建的 Dockerfile 配置类似 Dockerfile 写好了，需要转换成镜像： 构建镜像 [root@localhost mnt]# docker build -t kevin_nginx --rm=true . [root@localhost mnt]# docker images REPOSITORY TAG IMAGE IDCREATED SIZE kevin_nginxlatest e4967a39dd5423 seconds ago 411.1 MB docker.io/centos latest 67591570dd293 months ago191.8 MB 根据 Dockerfile 构建的镜像启动 nginx 容器 [root@localhost mnt]# docker run -ti -d --name test_nginx -p 8899:80 kevin_nginx /bin/bash 8725aceba170722cd57a4f20fd843634ee5c5d75f1c2726c1e98f66b8102a179 [root@localhost mnt]# docker ps CONTAINER IDIMAGE COMMAND CREATED STATUS PORTSNAMES 8725aceba170kevin_nginx &quot;/bin/sh -c &apos;/usr/loc&quot; 2 seconds ago Up 1 seconds0.0.0.0:8899-&gt;80/tcp test_nginx 进入容器，检查容器内的 nginx 程序是否已启动 [root@localhost mnt]# docker exec -ti test_nginx /bin/bash [root@8725aceba170 /]# ps -ef|grep nginx root 1 0 0 11:15 ?00:00:00 /bin/sh -c /usr/local/nginx/sbin/nginx &amp;&amp; tail -f /usr/local/nginx/logs/access.log /bin/bash root13 1 0 11:15 ?00:00:00 nginx: master process /usr/local/nginx/sbin/nginx www 1413 0 11:15 ?00:00:00 nginx: worker process root15 1 0 11:15 ?00:00:00 tail -f /usr/local/nginx/logs/access.log root3816 0 11:16 ?00:00:00 grep --color=auto nginx 通过映射到 Docker 宿主机的端口 8080 去访问容器的 nginx 创建好的镜像，可以保存到索引仓库中，便于下次使用（当然，我们直接共享 Dockerfile，是最简单的事情，:)) ），但毕竟镜像可以做到开箱即用。 登陆https://hub.docker.com/ 注册一个账号 然后登陆 [root@localhost mnt]# docker login Username: yhaing Password: Email: yhaing@163.com WARNING: login credentials saved in /root/.docker/config.json Login Succeeded 提交到 Docker 索引仓库 注意下面提交的镜像路径，即 &quot; 用户名 / 镜像 &quot;，只有这样才能成功提交。 所以在 Dockerfile 制作镜像的时候，仓库名最好用 docker 索引仓库的用户名，也即”用户名 / 镜像” [root@localhost mnt]# docker push yhaing/jdk-tomcat The push refers to a repository [docker.io/yhaing/jdk-tomcat] 8363b5ccd5b3: Pushed a619e846ce29: Pushed 578822b85971: Pushing [================================&gt;] 6.859 MB/10.57 MB b979d350733e: Pushed a6b994571424: Pushed eb80782ddf9c: Pushed ....... 这样下次想用的时候，可以直接从 Docker 索引仓库里下载docker pull yhaing/jdk-tomcat 需要注意几点：1）Docker 宿主机必须要有 base 镜像以供 Dockerfile 文件使用 2）注意 Dockerfile 实例文件中的 base 镜像，这个引用的 base 镜像一定要是存在的3）可以切换到不同的目录路径下编写 Dockerfile，然后构建，构建的时候直接使用. 表示在当前路径下。 镜像构建成功后，可以放到自己的私有仓库里，然后 Dockerfile 文件可以选择删除。 Dockerfile 最佳实践1 错误定位 每个 Dockerfile 的指令可以生成新的一层镜像，如果通过 Dockerfile 创建镜像出错，可以根据出错所在步骤的上一层启动容器，然后手工执行出错层的命令，以达到调试目的。 2 好的使用习惯 http://dockone.io/article/131 http://dockone.io/article/132]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[容器间网络通信设置 (Pipework 和 Open vSwitch)]]></title>
    <url>%2F2018%2F05%2F24%2F%E5%AE%B9%E5%99%A8%E9%97%B4%E7%BD%91%E7%BB%9C%E9%80%9A%E4%BF%A1%E8%AE%BE%E7%BD%AE(Pipework%E5%92%8COpen%20vSwitch)%2F</url>
    <content type="text"><![CDATA[自从 Docker 容器出现以来，容器的网络通信就一直是被关注的焦点，也是生产环境的迫切需求。容器的网络通信又可以分为两大方面：单主机容器上的相互通信，和跨主机的容器相互通信。下面将分别针对这两方面，对容器的通信原理进行简单的分析，帮助大家更好地使用 docker。 docker 单主机容器通信 基于对 net namespace 的控制，docker 可以为在容器创建隔离的网络环境，在隔离的网络环境下，容器具有完全独立的网络栈，与宿主机隔离，也可以使容器共享主机或者其他容器的网络命名空间，基本可以满足开发者在各种场景下的需要。 按 docker 官方的说法，docker 容器的网络有五种模式： 1）bridge 模式，–net=bridge(默认) 这是 dokcer 网络的默认设置，为容器创建独立的网络命名空间，容器具有独立的网卡等所有单独的网络栈，是最常用的使用方式。在 docker run 启动容器的时候，如果不加–net 参数，就默认采用这种网络模式。安装完 docker，系统会自动添加一个供 docker 使用的网桥 docker0，我们创建一个新的容器时，容器通过 DHCP 获取一个与 docker0 同网段的 IP 地址，并默认连接到 docker0 网桥，以此实现容器与宿主机的网络互通。 2）host 模式，–net=host 这个模式下创建出来的容器，直接使用容器宿主机的网络命名空间。将不拥有自己独立的 Network Namespace，即没有独立的网络环境。它使用宿主机的 ip 和端口。 3）none 模式，–net=none 为容器创建独立网络命名空间，但不为它做任何网络配置，容器中只有 lo，用户可以在此基础上，对容器网络做任意定制。这个模式下，dokcer 不为容器进行任何网络配置。需要我们自己为容器添加网卡，配置 IP。因此，若想使用 pipework 配置 docker 容器的 ip 地址，必须要在 none 模式下才可以。 4）其他容器模式（即container 模式），–net=container:NAME_or_ID 与 host 模式类似，只是容器将与指定的容器共享网络命名空间。这个模式就是指定一个已有的容器，共享该容器的 IP 和端口。除了网络方面两个容器共享，其他的如文件系统，进程等还是隔离开的。 5）用户自定义 ：docker 1.9 版本以后新增的特性，允许容器使用第三方的网络实现或者创建单独的 bridge 网络，提供网络隔离能力。 这些网络模式在相互网络通信方面的对比如下所示： 南北向通信指容器与宿主机外界的访问机制，东西向流量指同一宿主机上，与其他容器相互访问的机制。 host 模式 由于容器和宿主机共享同一个网络命名空间，换言之，容器的 IP 地址即为宿主机的 IP 地址。所以容器可以和宿主机一样，使用宿主机的任意网卡，实现和外界的通信。其网络模型可以参照下图 采用 host 模式的容器，可以直接使用宿主机的 IP 地址与外界进行通信，若宿主机具有公有 IP，那么容器也拥有这个公有 IP。同时容器内服务的端口也可以使用宿主机的端口， 无需额外进行 NAT 转换，而且由于容器通信时，不再需要通过 linuxbridge 等方式转发或者数据包的拆封，性能上有很大优势。 当然，这种模式有优势，也就有劣势，主要包括以下几个方面： 1）最明显的就是容器不再拥有隔离、独立的网络栈。容器会与宿主机竞争网络栈的使用，并且容器的崩溃就可能导致宿主机崩溃，在生产环境中，这种问题可能是不被允许的。 2）容器内部将不再拥有所有的端口资源，因为一些端口已经被宿主机服务、bridge 模式的容器端口绑定等其他服务占用掉了。 bridge 模式bridge 模式是 docker 默认的，也是开发者最常使用的网络模式。在这种模式下，docker 为容器创建独立的网络栈，保证容器内的进程使用独立的网络环境， 实现容器之间、容器与宿主机之间的网络栈隔离。同时，通过宿主机上的 docker0 网桥，容器可以与宿主机乃至外界进行网络通信。 其网络模型可以参考下图： 从上面的网络模型可以看出，容器从原理上是可以与宿主机乃至外界的其他机器通信的。同一宿主机上，容器之间都是连接掉 docker0 这个网桥上的，它可以作为虚拟交换机使容器可以相互通信。 然而，由于宿主机的 IP 地址与容器 veth pair 的 IP 地址均不在同一个网段，故仅仅依靠 veth pair 和 namespace 的技术，还不足以使宿主机以外的网络主动发现容器的存在。为了使外界可以方位容器中的进程，docker 采用了端口绑定的方式，也就是通过 iptables 的 NAT，将宿主机上的端口 端口流量转发到容器内的端口上。 举一个简单的例子，使用下面的命令创建容器，并将宿主机的 3306 端口绑定到容器的 3306 端口： docker run -tid --name db -p 3306:3306 MySQL 在宿主机上，可以通过 iptables -t nat -L -n，查到一条 DNAT 规则： DNAT tcp -- 0.0.0.0/0 0.0.0.0/0 tcp dpt:3306 to:172.17.0.5:3306 上面的 172.17.0.5 即为 bridge 模式下，创建的容器 IP。 很明显，bridge 模式的容器与外界通信时，必定会占用宿主机上的端口，从而与宿主机竞争端口资源，对宿主机端口的管理会是一个比较大的问题。同时，由于容器与外界通信是基于三层上 iptables NAT，性能和效率上的损耗是可以预见的。 none 模式 在这种模式下，容器有独立的网络栈，但不包含任何网络配置，只具有 lo 这个 loopback 网卡用于进程通信。也就是说，none 模式为容器做了最少的网络设置，但是俗话说得好“少即是多”，在没有网络配置的情况下，通过第三方工具或者手工的方式，开发这任意定制容器的网络，提供了最高的灵活性 其他容器（container）模式 其他网络模式是 docker 中一种较为特别的网络的模式。在这个模式下的容器，会使用其他容器的网络命名空间，其网络隔离性会处于 bridge 桥接模式与 host 模式之间。 当容器共享其他容器的网络命名空间，则在这两个容器之间不存在网络隔离，而她们又与宿主机以及除此之外其他的容器存在网络隔离。其网络模型可以参考下图： 在这种模式下的容器可以通过 localhost 来同一网络命名空间下的其他容器，传输效率较高。而且这种模式还节约了一定数量的网络资源，但它并没有改变容器与外界通信的方式。在一些特殊的场景中非常有用，例如，kubernetes 的 pod，kubernetes 为 pod 创建一个基础设施容器，同一 pod 下的其他容器都以其他容器模式共享这个基础设施容器的网络命名空间，相互之间以 localhost 访问，构成一个统一的整体。 用户定义网络模式 在用户定义网络模式下，开发者可以使用任何 docker 支持的第三方网络 driver 来定制容器的网络。并且，docker 1.9 以上的版本默认自带了 bridge 和 overlay 两种类型的自定义网络 driver。可以用于集成 calico、weave、openvswitch 等第三方厂商的网络实现。 除了 docker 自带的 bridge driver，其他的几种 driver 都可以实现容器的跨主机通信。而基于 bdrige driver 的网络，docker 会自动为其创建 iptables 规则， 保证与其他网络之间、与 docker0 之间的网络隔离。 例如，使用下面的命令创建一个基于 bridge driver 的自定义网络： docker network create bri1 则 docker 会自动生成如下的 iptables 规则，保证不同网络上的容器无法互相通信。 -A DOCKER-ISOLATION -i br-8dba6df70456 -o docker0 -j DROP -A DOCKER-ISOLATION -i docker0 -o br-8dba6df70456 -j DROP 除此之外，bridge driver 的所有行为都和默认的 bridge 模式完全一致。而 overlay 及其他 driver，则可以实现容器的跨主机通信。 docker 跨主机容器通信 早期大家的跨主机通信方案主要有以下几种： 1）容器使用 host 模式：容器直接使用宿主机的网络，这样天生就可以支持跨主机通信。虽然可以解决跨主机通信问题，但这种方式应用场景很有限，容易出现端口冲突，也无法做到隔离网络环境，一个容器崩溃很可能引起整个宿主机的崩溃。 2）端口绑定：通过绑定容器端口到宿主机端口，跨主机通信时，使用主机 IP+ 端口的方式访问容器中的服务。显而易见，这种方式仅能支持网络栈的四层及以上的应用，并且容器与宿主机紧耦合，很难灵活的处理，可扩展性不佳。 3）docker 外定制容器网络：在容器通过 docker 创建完成后，然后再通过修改容器的网络命名空间来定义容器网络。典型的就是很久以前的 pipework，容器以 none 模式创建，pipework 通过进入容器的网络命名空间为容器重新配置网络，这样容器网络可以是静态 IP、vxlan 网络等各种方式，非常灵活，容器启动的一段时间内会没有 IP，明显无法在大规模场景下使用，只能在实验室中测试使用。 4）第三方 SDN 定义容器网络：使用 Open vSwitch 或 Flannel 等第三方 SDN 工具，为容器构建可以跨主机通信的网络环境。这些方案一般要求各个主机上的 docker0 网桥的 cidr 不同，以避免出现 IP 冲突的问题，限制了容器在宿主机上的可获取 IP 范围。并且在容器需要对集群外提供服务时，需要比较复杂的配置，对部署实施人员的网络技能要求比较高。 上面这些方案有各种各样的缺陷，同时也因为跨主机通信的迫切需求，docker 1.9 版本时，官方提出了基于 vxlan 的 overlay 网络实现，原生支持容器的跨主机通信。同时，还支持通过 libnetwork 的 plugin 机制扩展各种第三方实现，从而以不同的方式实现跨主机通信。就目前社区比较流行的方案来说，跨主机通信的基本实现方案有以下几种： 1）基于隧道的 overlay 网络：按隧道类型来说，不同的公司或者组织有不同的实现方案。docker 原生的 overlay 网络就是基于 vxlan 隧道实现的。ovn 则需要通过 geneve 或者 stt 隧道来实现的。flannel 最新版本也开始默认基于 vxlan 实现 overlay 网络。 2）基于包封装的 overlay 网络：基于 UDP 封装等数据包包装方式，在 docker 集群上实现跨主机网络。典型实现方案有 weave、flannel 的早期版本。 3）基于三层实现 SDN 网络：基于三层协议和路由，直接在三层上实现跨主机网络，并且通过 iptables 实现网络的安全隔离。典型的方案为 Project Calico。同时对不支持三层路由的环境，Project Calico 还提供了基于 IPIP 封装的跨主机网络实现 Dokcer 通过使用 Linux 桥接提供容器之间的通信，docker0 桥接接口的目的就是方便 Docker 管理。当 Docker daemon 启动时需要做以下操作 a）如果 docker0 不存在则创建 b）搜索一个与当前路由不冲突的 ip 段 c）在确定的范围中选择 ip d）绑定 ip 到 docker0 列出当前主机网桥 [root@localhost ~]# brctl showbridge namebridge id STP enabled interfacesdocker08000.02426f15541e novethe833b02 查看当前 docker0 ip [root@localhost ~]# ifconfig docker0: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt; mtu 1500 inet 172.17.0.1 netmask 255.255.0.0 broadcast 0.0.0.0 inet6 fe80::42:6fff:fe15:541e prefixlen 64 scopeid 0x20&lt;link&gt; ether 02:42:6f:15:54:1e txqueuelen 0 (Ethernet) RX packets 120315 bytes 828868638 (790.4 MiB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 132565 bytes 100884398 (96.2 MiB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 ................ 在容器运行时，每个容器都会分配一个特定的虚拟机口并桥接到 docker0。每个容器都会配置同 docker0 ip 相同网段的专用 ip 地址，docker0 的 IP 地址被用于所有容器的默认网关。 一般启动的容器中 ip 默认是 172.17.0.1/24 网段的。 [root@linux-node2 ~]# docker images REPOSITORY TAG IMAGE IDCREATED SIZE centos latest 67591570dd293 months ago191.8 MB [root@linux-node2 ~]# docker run -t -i --name my-test centos /bin/bash [root@c5217f7bd44c /]# [root@linux-node2 ~]# docker ps CONTAINER IDIMAGE COMMAND CREATED STATUS PORTSNAMES c5217f7bd44ccentos &quot;/bin/bash&quot; 10 seconds ago Up 10 secondsmy-test [root@linux-node2 ~]# docker inspect c5217f7bd44c|grep IPAddress &quot;SecondaryIPAddresses&quot;: null, &quot;IPAddress&quot;: &quot;172.17.0.2&quot;, &quot;IPAddress&quot;: &quot;172.17.0.2&quot;, 那么能不能在创建容器的时候指定特定的 ip 呢？这是当然可以实现的！ 注意：宿主机的 ip 路由转发功能一定要打开，否则所创建的容器无法联网！ [root@localhost ~]# cat /proc/sys/net/ipv4/ip_forward 1 [root@localhost ~]# [root@localhost ~]# docker ps CONTAINER IDIMAGE COMMAND CREATED STATUS PORTS NAMES 6e64eade06d1docker.io/centos&quot;/bin/bash&quot; 10 seconds ago Up 9 secondsmy-centos [root@localhost ~]# docker run -itd --net=none --name=container1 docker.io/centos 5e5bdbc4d9977e6bcfa40e0a9c3be10806323c9bf5a60569775903d345869b09 [root@localhost ~]# docker attach container1 [root@5e5bdbc4d997 /]# ping www.baidu.com PING www.a.shifen.com (61.135.169.121) 56(84) bytes of data. 64 bytes from 61.135.169.121 (61.135.169.121): icmp_seq=1 ttl=53 time=2.09 ms 64 bytes from 61.135.169.121 (61.135.169.121): icmp_seq=2 ttl=53 time=2.09 ms 关闭 ip 路由转发功能，容器即不能联网 [root@localhost ~]# echo 0 &gt; /proc/sys/net/ipv4/ip_forward [root@localhost ~]# cat /proc/sys/net/ipv4/ip_forward 0 [root@5e5bdbc4d997 /]# ping www.baidu.com//ping 不通~ 创建容器使用特定范围的 IPDocker 会尝试寻找没有被主机使用的 ip 段，尽管它适用于大多数情况下，但是它不是万能的，有时候我们还是需要对 ip 进一步规划。 Docker 允许你管理 docker0 桥接或者通过 -b 选项自定义桥接网卡，需要安装 bridge-utils 软件包。操作流程如下： a）确保 docker 的进程是停止的 b）创建自定义网桥 c）给网桥分配特定的 ip d）以 -b 的方式指定网桥 具体操作过程如下（比如创建容器的时候，指定 ip 为 192.168.5.1/24 网段的）： [root@localhost ~]# service docker stop [root@localhost ~]# ip link set dev docker0 down [root@localhost ~]# brctl delbr docker0 [root@localhost ~]# brctl addbr bridge0 [root@localhost ~]# ip addr add 192.168.5.1/24 dev bridge0 // 注意，这个 192.168.5.1 就是所建容器的网关地址。通过 docker inspect container_id 能查看到 [root@localhost ~]# ip link set dev bridge0 up [root@localhost ~]# ip addr show bridge0 [root@localhost ~]# vim /etc/sysconfig/docker // 即将虚拟的桥接口由默认的 docker0 改为 bridge0 将 OPTIONS=&apos;--selinux-enabled --log-driver=journald&apos; 改为 OPTIONS=&apos;--selinux-enabled --log-driver=journald -b=bridge0&apos;// 即添加 -b=bridge0 [root@localhost ~]# service docker restart 上面是 centos7 下的操作步骤, 下面提供下 ubuntu 下的操作步骤： $ sudo service docker stop $ sudo ip link set dev docker0 down $ sudo brctl delbr docker0 $ sudo brctl addbr bridge0 $ sudo ip addr add 192.168.5.1/24 dev bridge0 $ sudo ip link set dev bridge0 up $ ip addr show bridge0 $ echo &apos;DOCKER_OPTS=&quot;-b=bridge0&quot;&apos; &gt;&gt; /etc/default/docker $ sudo service docker start 然后创建容器，查看下容器 ip 是否为设定的 192.168.5.1/24 网段的 [root@localhost ~]# docker images REPOSITORY TAG IMAGE IDCREATED SIZE docker.io/ubuntulatest 0ef2e08ed3fa2 weeks ago 130 MB centos7 7.3.1611d5ebea14da543 weeks ago 311 MB [root@localhost ~]# docker run -t -i --name test2 centos7:7.3.1611 /bin/bash [root@224facf8e054 /]# [root@localhost ~]# docker run -t -i --name test1 docker.io/ubuntu /bin/bash root@f5b1bfc2811a:/# [root@localhost ~]# docker ps CONTAINER IDIMAGE COMMAND CREATED STATUS PORTS NAMES 224facf8e054centos7:7.3.1611&quot;/bin/bash&quot; 46 minutes ago Up 46 minutes test2 f5b1bfc2811adocker.io/ubuntu&quot;/bin/bash&quot; 47 minutes ago Up 5 minutestest1 [root@localhost ~]# docker inspect --format=&apos;{{.NetworkSettings.IPAddress}}&apos; f5b1bfc2811a 192.168.5.2 [root@localhost ~]# docker inspect --format=&apos;{{.NetworkSettings.IPAddress}}&apos; 224facf8e054 192.168.5.3 [root@localhost ~]# brctl show bridge name bridge id STP enabled interfaces bridge0 8000.ba141fa20c91 no vethe7e227b vethf382771 使用 pipework 给容器设置一个固定的 ip可以利用 pipework 为容器指定一个固定的 ip，操作方法非常简单，如下： [root@node1 ~]# brctl addbr br0 [root@node1 ~]# ip link set dev br0 up [root@node1 ~]# ip addr add 192.168.114.1/24 dev br0// 这个 ip 相当于 br0 网桥的网关 ip，可以随意设定。 [root@node1 ~]# docker run -ti -d --net=none --name=my-test1 docker.io/nginx /bin/bash [root@node1 ~]# pipework br0 -i eth0 my-test1 192.168.114.100/24@192.168.114.1 [root@node1 ~]# docker exec -ti my-test1 /bin/bash root@cf370a090f63:/# ip addr 1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 57: eth0@if58: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000 link/ether b2:c1:8d:92:33:e2 brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 192.168.114.100/24 brd 192.168.114.255 scope global eth0 valid_lft forever preferred_lft forever inet6 fe80::b0c1:8dff:fe92:33e2/64 scope link valid_lft forever preferred_lft forever 再启动一个容器 [root@node1 ~]# docker run -ti -d --net=none --name=my-test2 docker.io/nginx /bin/bash [root@node1 ~]# pipework br0 -i eth0 my-test12 192.168.114.200/24@192.168.114.1 [root@node1 ~]# pipework br0 -i eth0 my-test2 192.168.114.200/24@192.168.114.1 这样，my-test1 容器和 my-test2 容器在同一个宿主机上，所以它们固定后的 ip 是可以相互 ping 通的，如果是在不同的宿主机上，则就无法 ping 通！ 所以说： 这样使用 pipework 指定固定 ip 的容器，在同一个宿主机下的容器间的 ip 是可以相互 ping 通的，但是跨主机的容器通过这种方式固定 ip 后就不能 ping 通了。跨主机的容器间的通信可以看下面的介绍。 不同主机间的容器通信（pipework config docker container ip）我的 centos7 测试机上的 docker 是 yum 安装的，默认自带 pipework 工具，所以就不用在另行安装它了。 如果没有 pipework 工具，可以安装下面步骤进行安装： # git clone https://github.com/jpetazzo/pipework.git # sudo cp -rp pipework/pipework /usr/local/bin/ 安装相应依赖软件(网桥) #sudo apt-get install iputils-arping bridge-utils -y 查看 Docker 宿主机上的桥接网络 [root@linux-node2 ~]# brctl show bridge name bridge id STP enabled interfaces docker0 8000.02426f15541e noveth92d132f 有两种方式做法： 1）可以选择删除 docker0，直接把 docker 的桥接指定为 br0； 2）也可以选择保留使用默认 docker0 的配置，这样单主机容器之间的通信可以通过 docker0；跨主机不同容器之间通过 pipework 将容器的网卡桥接到 br0 上，这样跨主机容器之间就可以通信了。 如果保留了 docker0，则容器启动时不加–net=none 参数，那么本机容器启动后就是默认的 docker0 自动分配的 ip（默认是 172.17.1.0/24 网段），它们之间是可以通信的；跨宿主机的容器创建时要加–net=none 参数，待容器启动后通过 pipework 给容器指定 ip，这样跨宿主机的容器 ip 是在同一网段内的同网段地址，因此可以通信。 一般来说：最好在创建容器的时候加上–net=none，防止自动分配的 IP 在局域网中有冲突。若是容器创建后自动获取 ip，下次容器启动会 ip 有变化，可能会和物理网段中的 ip 冲突 实践 实例说明如下： 宿主机信息 ip：192.168.1.23 （网卡设备为 eth0） gateway：192.168.1.1 netmask：255.255.255.0 1）删除虚拟桥接卡 docker0 的配置 [root@localhost ~]# service docker stop [root@localhost ~]# ip link set dev docker0 down [root@localhost ~]# brctl delbr docker0 [root@localhost ~]# brctl addbr br0 [root@localhost ~]# ip link set dev br0 up [root@localhost ~]# ip addr del 192.168.1.23/24 dev eth0 // 删除宿主机网卡的 IP（如果是使用这个地址进行的远程连接，这一步操作后就会断掉；如果是使用外网地址连接的话，就不会断开） [root@localhost ~]# ip addr add 192.168.1.23/24 dev br0// 将宿主主机的 ip 设置到 br0 [root@localhost ~]# brctl addif br0 eth0// 将宿主机网卡挂到 br0 上 [root@localhost ~]# ip route del default // 删除默认的原路由，其实就是 eth0 上使用的原路由 192.168.1.1（这步小心，注意删除后要保证机器能远程连接上，最好是通过外网 ip 远程连的。别删除路由后，远程连接不上，中断了） [root@localhost ~]# ip route add default via 192.168.1.1 dev br0 // 为 br0 设置路由 [root@localhost ~]# vim /etc/sysconfig/docker // 即将虚拟的桥接口由默认的 docker0 改为 bridge0 将 OPTIONS=&apos;--selinux-enabled --log-driver=journald&apos; 改为 OPTIONS=&apos;--selinux-enabled --log-driver=journald -b=br0&apos;// 即添加 -b=br0 [root@localhost ~]# service docker start 启动一个手动设置网络的容器 [root@localhost ~]# docker ps CONTAINER IDIMAGE COMMAND CREATED STATUS PORTS NAMES 6e64eade06d1docker.io/centos&quot;/bin/bash&quot; 10 seconds ago Up 9 secondsmy-centos [root@localhost ~]# docker run -itd --net=none --name=my-test1 docker.io/centos 为 my-test1 容器设置一个与桥接物理网络同地址段的 ip（如下，”ip@gateway”）默认不指定网卡设备名，则默认添加为 eth0。可以通过 -i 参数添加网卡设备名 [root@localhost ~]# pipework br0 -i eth0 my-test1 192.168.1.190/24@192.168.1.1 同理，在其他机器上启动容器，并类似上面用 pipework 设置一个同网段类的 ip，这样跨主机的容器就可以相互 ping 通了！ 2）保留默认虚拟桥接卡 docker0 的配置 [root@localhost ~]# cd /etc/sysconfig/network-scripts/ [root@localhost network-scripts]# cp ifcfg-eth0 ifcfg-eth0.bak [root@localhost network-scripts]# cp ifcfg-eth0 ifcfg-br0 [root@localhost network-scripts]# vim ifcfg-eth0// 增加 BRIDGE=br0，删除 IPADDR,NETMASK,GATEWAY,DNS 的设置 ...... BRIDGE=br0 [root@localhost network-scripts]# vim ifcfg-br0// 修改 DEVICE 为 br0,Type 为 Bridge, 把 eth0 的网络设置设置到这里来（里面应该有 ip，网关，子网掩码或 DNS 设置） ...... TYPE=Bridge DEVICE=br0 [root@localhost network-scripts]# service network restart [root@localhost network-scripts]# service docker restart 开启一个容器并指定网络模式为 none（这样，创建的容器就不会通过 docker0 自动分配 ip 了，而是根据 pipework 工具自定 ip 指定） [root@localhost network-scripts]# docker images REPOSITORY TAG IMAGE IDCREATED SIZE docker.io/centoslatest 67591570dd293 months ago191.8 MB [root@localhost network-scripts]# docker run -itd --net=none --name=my-centos docker.io/centos /bin/bash 6e64eade06d1eb20be3bd22ece2f79174cd033b59182933f7bbbb502bef9cb0f 接着给容器配置网络 [root@localhost network-scripts]# pipework br0 -i eth0 my-centos 192.168.1.150/24@192.168.1.1 [root@localhost network-scripts]# docker attach 6e64eade06d1 [root@6e64eade06d1 /]# ifconfig eth0 // 若没有 ifconfig 命令，可以 yum 安装 net-tools 工具 eth0 Link encap:Ethernet HWaddr 86:b6:6b:e8:2e:4d inet addr:192.168.1.150 Bcast:0.0.0.0 Mask:255.255.255.0 inet6 addr: fe80::84b6:6bff:fee8:2e4d/64 Scope:Link UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 RX packets:8 errors:0 dropped:0 overruns:0 frame:0 TX packets:9 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1000 RX bytes:648 (648.0 B) TX bytes:690 (690.0 B) [root@6e64eade06d1 /]# route -n Kernel IP routing table Destination Gateway Genmask Flags Metric RefUse Iface 0.0.0.0 192.168.1.1 0.0.0.0 UG0 00 eth0 192.168.115.0 0.0.0.0 255.255.255.0 U 0 00 eth0 另外 pipework 不能添加静态路由，如果有需求则可以在 run 的时候加上–privileged=true 权限在容器中手动添加，但这种方法安全性有缺陷。 除此之外，可以通过 ip netns（–help 参考帮助）添加静态路由，以避免创建容器使用–privileged=true 选项造成一些不必要的安全问题： 如下获取指定容器的 pid [root@localhost network-scripts]# docker inspect --format=&quot;{{.State.Pid}}&quot; 6e64eade06d1 7852 [root@localhost network-scripts]# ln -s /proc/7852/ns/net /var/run/netns/7852 [root@localhost network-scripts]# ip netns exec 7852 ip route add 192.168.0.0/16 dev eth0 via 192.168.1.1 [root@localhost network-scripts]# ip netns exec 7852 ip route// 添加成功 192.168.0.0/16 via 192.168.1.1 dev eth0 同理，在其它宿主机进行相应的配置，新建容器并使用 pipework 添加虚拟网卡桥接到 br0，如此创建的容器间就可以相互通信了。 报错处理1）重启网卡报错如下： # systemctl restart network ...... Nov 23 22:09:08 hdcoe02 systemd[1]: network.service: control process exited, code=exited status=1 Nov 23 22:09:08 hdcoe02 systemd[1]: Failed to start LSB: Bring up/down networking. Nov 23 22:09:08 hdcoe02 systemd[1]: Unit network.service entered failed state.&lt;/span&gt; 解决办法： # systemctl enable NetworkManager-wait-online.service # systemctl stop NetworkManager # systemctl restart network.service 2）创建容器，出现下面告警 WARNING: IPv4 forwarding is disabled. Networking will not work. 解决办法： #vim /usr/lib/sysctl.d/00-system.conf 添加如下代码： net.ipv4.ip_forward=1 重启 network 服务 # systemctl restart network Open vSwitch 的使用 其实除了上面使用的 pipework 工具还，还可以使用虚拟交换机 (Open vSwitch) 进行 docker 容器间的网络通信，废话不多说，下面说下 Open vSwitch 的使用： 在 Server1 和 Server2 上分别安装 open vswitch[root@Slave1 ~]# # yum -y install wget openssl-devel kernel-devel [root@Slave1 ~]# yum groupinstall &quot;Development Tools&quot; [root@Slave1 ~]# adduser ovswitch [root@Slave1 ~]# su - ovswitch [root@Slave1 ~]$ wget http://openvswitch.org/releases/openvswitch-2.3.0.tar.gz [root@Slave1 ~]$ tar -zxvpf openvswitch-2.3.0.tar.gz [root@Slave1 ~]$ mkdir -p ~/rpmbuild/SOURCES [root@Slave1 ~]$ sed &apos;s/openvswitch-kmod, //g&apos; openvswitch-2.3.0/rhel/openvswitch.spec &gt; openvswitch-2.3.0/rhel/openvswitch_no_kmod.spec [root@Slave1 ~]$ cp openvswitch-2.3.0.tar.gz rpmbuild/SOURCES/ [root@Slave1 ~]$ rpmbuild -bb --without check ~/openvswitch-2.3.0/rhel/openvswitch_no_kmod.spec [root@Slave1 ~]$ exit [root@Slave1 ~]# yum localinstall /home/ovswitch/rpmbuild/RPMS/x86_64/openvswitch-2.3.0-1.x86_64.rpm [root@Slave1 ~]# mkdir /etc/openvswitch [root@Slave1 ~]# setenforce 0 [root@Slave1 ~]# systemctl start openvswitch.service [root@Slave1 ~]# systemctl status openvswitch.service -l 在 Slave1 和 Slave2 上建立 OVS Bridge 并配置路由1）在 Slave1 宿主机上设置 docker 容器内网 ip 网段 172.17.1.0/24 [root@Slave1 ~]# vim /proc/sys/net/ipv4/ip_forward 1 [root@Slave1 ~]# ovs-vsctl add-br obr0 [root@Slave1 ~]# ovs-vsctl add-port obr0 gre0 -- set Interface gre0 type=gre options:remote_ip=192.168.115.5 [root@Slave1 ~]# brctl addbr kbr0 [root@Slave1 ~]# brctl addif kbr0 obr0 [root@Slave1 ~]# ip link set dev docker0 down [root@Slave1 ~]# ip link del dev docker0 [root@Slave1 ~]# vim /etc/sysconfig/network-scripts/ifcfg-kbr0 ONBOOT=yes BOOTPROTO=static IPADDR=172.17.1.1 NETMASK=255.255.255.0 GATEWAY=172.17.1.0 USERCTL=no TYPE=Bridge IPV6INIT=no [root@Slave1 ~]# vim /etc/sysconfig/network-scripts/route-ens32 172.17.2.0/24 via 192.168.115.6 dev ens32 [root@Slave1 ~]# systemctl restart network.service 2）在 Slave2 宿主机上设置 docker 容器内网 ip 网段 172.17.2.0/24 [root@Slave2 ~]# vim /proc/sys/net/ipv4/ip_forward 1 [root@Slave2 ~]# ovs-vsctl add-br obr0 [root@Slave2 ~]# ovs-vsctl add-port obr0 gre0 -- set Interface gre0 type=gre options:remote_ip=192.168.115.6 [root@Slave2 ~]# brctl addbr kbr0 [root@Slave2 ~]# brctl addif kbr0 obr0 [root@Slave2 ~]# ip link set dev docker0 down [root@Slave2 ~]# ip link del dev docker0 [root@Slave2 ~] vim /etc/sysconfig/network-scripts/ifcfg-kbr0 ONBOOT=yes BOOTPROTO=static IPADDR=172.17.2.1 NETMASK=255.255.255.0 GATEWAY=172.17.2.0 USERCTL=no TYPE=Bridge IPV6INIT=no [root@Slave2 ~]# vim /etc/sysconfig/network-scripts/route-ens32 172.17.1.0/24 via 192.168.115.5 dev ens32 [root@Slave2 ~]# systemctl restart network.service 启动容器测试Server1 和 Server2 上修改 docker 启动的虚拟网卡绑定为 kbr0，重启 docker 进程 1）在 Server1 宿主机上启动容器, 然后登陆容器内查看 ip，就会发现 ip 是上面设定额 172.17.1.0/24 网段的 [root@Slave1 ~]# docker run -idt --name my-server1 daocloud.io/library/centos/bin/bash 2）在 Server2 宿主机上启动容器，然后登陆容器内查看 ip，就会发现 ip 是上面设定额 172.17.2.0/24 网段的 [root@Slave2 ~]#docker run -idt --name my-server1 daocloud.io/library/centos /bin/bash 然后在上面启动的容内互 ping 对方容器，发现是可以 ping 通的]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python 3 爬取百度经验数据]]></title>
    <url>%2F2018%2F05%2F24%2FPython%203%E7%88%AC%E5%8F%96%E7%99%BE%E5%BA%A6%E7%BB%8F%E9%AA%8C%E6%95%B0%E6%8D%AE%2F</url>
    <content type="text"><![CDATA[安装环境准备 直接使用 win10 的 wsl 沙盒 Ubuntu 系统，自带 python3.5 安装 apt install python3-pip pip3 install rsa 注意事项IndentationError: unexpected indent 检查缩进是否一致，空格和 Tab 符号注意区分 ## 实战 通过 cookie 爬百度数据1. 登陆百度，通过浏览器设置 - 内容管理 -cookie，找到百度的 BDUSS 的内容复制 2. 编写脚本 login.py import requests #需要爬数据的 url url = &apos;http://i.baidu.com/&apos; #浏览器访问网站的 cookie 信息 cookie = {&quot;BDUSS&quot;:&quot;----------------------------------------------------AAAAAAAAAAA----------------AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA--&quot;} #requests 请求，获取登录网站页面的内容 html = requests.get(url,cookies=cookie).content #print(html) #把内容保存为文件 with open(&quot;baidu.html&quot;, &apos;wb&apos;) as f: f.write(html) f.close() 3. 在 Ubuntu bash 执行 python3 login.py，会生成一个文件 baidu.html 在当前目录, 打开如果能看到个人信息就证明获取成功 爬百度翻页数据 上面已经登陆成功了，下面直接用 cookie 进行爬数据会被重定向，还需要添加请求头，以及翻页参数 import requests #需要爬数据的 url url = &apos;https://jingyan.baidu.com/user/nucpage/content&apos; #浏览器访问网站的 cookie 信息 cookie = {&quot;BDUSS&quot;:&quot;-----QAAAAAAAAAAAEAAA--1QAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA---&quot;} #提供你所使用的浏览器类型、操作系统及版本、CPU 类型、浏览器渲染引擎、浏览器语言、浏览器插件等信息的标识 user_agent=&quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/67.0.3396.99 Safari/537.36&quot; # 从那个连接来的 referer=&quot;https://jingyan.baidu.com/user/nucpage/content&quot; # 设置请求头 headers = { &quot;User-Agent&quot;: user_agent, &quot;Referer&quot;: referer } # url 参数 # https://jingyan.baidu.com/user/nucpage/content?tab=exp&amp;expType=published&amp;pn=20 params = { &apos;tab&apos;: &apos;exp&apos;, &apos;expType&apos;: &apos;published&apos;, &apos;pn&apos;: &apos;30&apos; } #requests 请求，获取登录网站页面的内容 html = requests.get(url,cookies=cookie,headers=headers).content #print(html) #把内容保存为文件 with open(&quot;baidu.html&quot;, &apos;wb&apos;) as f: f.write(html) f.close() 最终版爬百度经验的个人经验数据import requests #正则 import re #需要爬数据的 url url = &apos;https://jingyan.baidu.com/user/nucpage/content&apos; #浏览器访问网站的 cookie 信息 cookie = {&quot;BDUSS&quot;:&quot;--AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA-&quot;} #提供你所使用的浏览器类型、操作系统及版本、CPU 类型、浏览器渲染引擎、浏览器语言、浏览器插件等信息的标识 user_agent=&quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/67.0.3396.99 Safari/537.36&quot; # 从那个连接来的 referer=&quot;https://jingyan.baidu.com/user/nucpage/content&quot; # 设置请求头 headers = { &quot;User-Agent&quot;: user_agent, &quot;Referer&quot;: referer } #requests 请求, 获取发布数量 published = requests.get(url,cookies=cookie,headers=headers).content #&lt;li&gt;&lt;a class=&quot;on&quot; href=&quot;/user/nucpage/content&quot;&gt; 已发布 (505)&lt;/a&gt;&lt;/li&gt; reg=r&apos;&lt;li&gt;&lt;a class=&quot;on&quot; href=&quot;/user/nucpage/content&quot;&gt; 已发布 \((.*?)\)&lt;/a&gt;&lt;/li&gt;&apos; publishedNum=re.search(reg,published.decode(),re.I|re.M|re.S).group(1) #group(0) 匹配的串，group(1) 匹配的串中第一个括号 print(publishedNum) #算页数, 实际篇数 -1 pages=int((int(publishedNum)-1)/20)+1 print(pages) #把内容保存为文件,&apos;w&apos; 是写，&apos;wb&apos; 是写入 byte with open(&quot;jingyan.md&quot;, &apos;w&apos;) as f: for page in range(0,pages): pn=page*20 print(pn) # url 参数 # https://jingyan.baidu.com/user/nucpage/content?tab=exp&amp;expType=published&amp;pn=20 params = { &apos;tab&apos;: &apos;exp&apos;, &apos;expType&apos;: &apos;published&apos;, &apos;pn&apos;: pn } #requests 请求，获取登录网站页面的内容 html = requests.get(url,cookies=cookie,headers=headers,params=params).content #过滤 reg=r&apos;&lt;a class=&quot;f14&quot; target=&quot;_blank&quot; title=(.*?)&gt;&apos; #re.I 使匹配对大小写不敏感 #re.M 多行匹配，影响 ^ 和 $ #re.S 使 . 匹配包括换行在内的所有字符 #这个是查找此字符串中所有符合条件的内容并返回一个列表 list=re.findall(reg,html.decode(),re.I|re.M|re.S) #写入文件并替换为 markdown 格式 for item in list: item=item.replace(&apos;&quot; href=&quot;&apos;,&apos;](https://jingyan.baidu.com&apos;) item=item.replace(&apos;.html&quot;&apos;,&apos;.html)&apos;) item=item.replace(&apos;&quot;&apos;,&apos;[&apos;) f.write(&quot;%s\n&quot; % item) f.close() 以上整理主要参照下面的文档，如涉及侵权请联系本人，进行删除。 参考： Python：网页的抓取、过滤和保存]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一起学 Docker（七）-- 容器的网络]]></title>
    <url>%2F2018%2F05%2F24%2F%E4%B8%80%E8%B5%B7%E5%AD%A6Docker%EF%BC%88%E4%B8%83%EF%BC%89--%E5%AE%B9%E5%99%A8%E7%9A%84%E7%BD%91%E7%BB%9C%2F</url>
    <content type="text"><![CDATA[容器对外服务 当容器内运行一些网络应用，要让外部访问这些应用时，可以通过 -P 或 -p 参数来指定端口映射。 使用 -P 映射时，Docker 会随机映射一个 49000 ～49900 的端口至容器内部开放的端口： docker run -d -P --namemysql mysql:5.6 通过 docker ps 可以看到端口映射关系。可以通过映射在宿主机的端口来访问对应容器内的服务。 进入 docker 的官网下载 mysql 镜像 tag 是 mysql 的版本号，这里咱们选择 mysql5.6 映射到指定宿主机的端口： docker run --name some-mysql -e MYSQL_ROOT_PASSWORD=my-secret-pw -d mysql:tag -e 是环境变量的意思 运行容器内的环境变量设置 mysql 的密码 容器里面的 mysql 已经启动了现在咱们为了映射端口的话删除这个 mysql 容器 映射到指定地址的指定端口，为例： docker run -d -p 3306:3306 --name mysql mysql:5.6 外部访问虚拟机的 3306 直接映射到容器的 3306 连接到数据库 映射到指定地址的指定端口，以 127.0.0.1 为例： docker run -d -p127.0.0.1:3306:3306 --name mysql mysql:5.6 映射到指定地址的任意端口，以 127.0.0.1 为例： docker run -d -p 127.0.0.1::3306 --name mysql mysql:5.6 查看映射端口配置： docker port mysql 3306 容器间相互通信 通过映射宿主机的端口实现容器的互联。 容器的连接 (link) 除了端口映射外的另一种可以与容器中应用进行交互的方式。 使用 –link 参数可以让容器之间安全的进行交互。 创建一个数据库容器： docker run -d --namemysqldb mysql:5.6 创建一个 web 容器并和数据库容器建立连接： docker run -d --name Webapp–p 8000:8080 --link mysqldb:MySQL tomcat 上边的 MySQL 别名就类似 dns 解析的方式，我给这个容器起了个别名叫 MySQL，我就通过这个别名就可以找到对应的这个 mysqldb 容器 mysqldb 容器和 web 容器建立互联关系。 --link 参数的格式为 --link name:alias, 其中 name 是要连接的容器名称，alias 是这个连接的别名。 可以使用 docker ps（PORT 字段）来查看容器的连接。 Docker 在两个容器之间创建了安全隧道，而且不用映射它们的端口到宿主机上。在启动 mysqldb 的时候并没有使用 -p 和 -P 标记，从而避免的了暴露数据库的端口到外部的网络上。 link 就是容器直接互相通信的 Docker 通过两种方式为容器公开连接信息： 1 环境变量： 使用 env 命令来查看。 EX: docker run --rm --name test--link dblink:dblink ubuntu env 2 更新 /etc/hosts 文件 查看 /etc/hosts 文件。]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一起学 Docker（六）-- 数据管理]]></title>
    <url>%2F2018%2F05%2F23%2F%E4%B8%80%E8%B5%B7%E5%AD%A6Docker%EF%BC%88%E5%85%AD%EF%BC%89--%E6%95%B0%E6%8D%AE%E7%AE%A1%E7%90%86%2F</url>
    <content type="text"><![CDATA[数据卷Docker volume 介绍Docker 中的数据可以存储在类似于虚拟机磁盘的介质中，在 Docker 中称为数据卷（Data Volume）。数据卷可以用来存储 Docker 应用的数据，也可以用来在 Docker 容器间进行数据共享。 数据卷呈现给 Docker 容器的形式就是一个目录，支持多个容器间共享，修改也不会影响镜像。使用 Docker 的数据卷，类似在系统中使用 mount 挂载一个文件系统。 1）一个数据卷是一个特别指定的目录，该目录利用容器的 UFS 文件系统可以为容器提供一些稳定的特性或者数据共享。数据卷可以在多个容器之间共享。 2）创建数据卷，只要在 docker run 命令后面跟上 -v 参数即可创建一个数据卷，当然也可以跟多个 -v 参数来创建多个数据卷，当创建好带有数据卷的容器后， 就可以在其他容器中通过–volumes-froms 参数来挂载该数据卷了，而不管该容器是否运行。也可以在 Dockerfile 中通过 VOLUME 指令来增加一个或者多个数据卷。 3）如果有一些数据想在多个容器间共享，或者想在一些临时性的容器中使用该数据，那么最好的方案就是你创建一个数据卷容器，然后从该临时性的容器中挂载该数据卷容器的数据。 这样，即使删除了刚开始的第一个数据卷容器或者中间层的数据卷容器，只要有其他容器使用数据卷，数据卷都不会被删除的。 4）不能使用 docker export、save、cp 等命令来备份数据卷的内容，因为数据卷是存在于镜像之外的。备份的方法可以是创建一个新容器，挂载数据卷容器，同时挂载一个本地目录， 然后把远程数据卷容器的数据卷通过备份命令备份到映射的本地目录里面。如下： docker run -rm --volumes-from DATA -v $(pwd):/backup busybox tar cvf /backup/backup.tar /data 5）也可以把一个本地主机的目录当做数据卷挂载在容器上，同样是在 docker run 后面跟 -v 参数，不过 -v 后面跟的不再是单独的目录了，它是 [host-dir]:[container-dir]:[rw|ro] 这样格式的， host-dir 是一个绝对路径的地址，如果 host-dir 不存在，则 docker 会创建一个新的数据卷，如果 host-dir 存在，但是指向的是一个不存在的目录，则 docker 也会创建该目录，然后使用该目录做数据源。 Docker Volume 数据卷可以实现： 1）绕过“拷贝写”系统，以达到本地磁盘 IO 的性能，（比如运行一个容器，在容器中对数据卷修改内容，会直接改变宿主机上的数据卷中的内容，所以是本地磁盘 IO 的性能，而不是先在容器中写一份，最后还要将容器中的修改的内容拷贝出来进行同步。） 2）绕过“拷贝写”系统，有些文件不需要在 docker commit 打包进镜像文件。 3）数据卷可以在容器间共享和重用数据 4）数据卷可以在宿主和容器间共享数据 5）数据卷数据改变是直接修改的，数据卷修改会立即生效， 数据卷的更新不会影响镜像 6）数据卷是持续性的，直到没有容器使用它们。即便是初始的数据卷容器或中间层的数据卷容器删除了，只要还有其他的容器使用数据卷，那么里面的数据都不会丢失。 Docker 数据持久化： 容器在运行期间产生的数据是不会写在镜像里面的，重新用此镜像启动新的容器就会初始化镜像，会加一个全新的读写入层来保存数据。 如果想做到数据持久化，Docker 提供数据卷（Data volume）或者数据容器卷来解决问题，另外还可以通过 commit 提交一个新的镜像来保存产生的数据。 准备工作 创建一个目录，并在目录里面创建文件，文件内写入内容。 在容器内创建数据卷 在使用 docker run 的命令时，使用 -v 标记可以在容器内创建一个数据卷，并且可以指定挂在一个本地已有的目录到容器中作为数据卷： docker run -d --name app1-it -v ${PWD}/webapp:/root/webapp ubuntu bash 通过目录跟容器内建立了一层关系，数据卷发生变化后，容器内和容器外都会随之发生改变。例如容器挂载一个文件，当容器挂了后，文件不会丢失。 注意：默认挂载的数据卷的权限是 rw（可读写），如果要求 ro（只读），则需要加上对应的 ro 参数，命令可改为： docker run -d --name app1-it -v ${PWD}/webapp:/root/webapp:ro ubuntu bash 下面我们一起来操作一下： 创建 webapp 目录，在目录下新建文件 file，并在文件 file 中写入“hello my docker!!”。 echo ${PWD} 命令标识当前目录。 创建启动 app1 容器并挂载数据卷 进入容器找到 root 目录可查看到已挂载的数据卷。 数据卷目录与容器内目录有映射关系，所以不管是在容器内部修改数据卷还是在外部修改数据卷，相对应的数据卷都会发生改变。 只读的演示 数据卷容器 数据卷容器用于用户需要在容器间共享一些持续更新的数据，数据卷容器专门提供数据卷供其它容器挂载使用。 Example: 创建数据卷容器 db1 docker run -d --name db1 -v/dbdata -ti ubuntu bash 创建容器 db2 与 db1 共享 dbdata 的数据 docker run -d --name db2 --volumes-fromdb1 -ti ubuntu bash 在容器 db1 和容器 db2 任意一个容器修改 dbdata 的内容，在两个容器内均生效 数据卷容器的删除： 如果删除了挂载的容器，数据卷并不会被自动删除，如果要删除一个数据卷，必须在删除最后一个还挂载它的容器时显示使用 docker rm -v 命令指定同时删除关联的容器。在下图可看到即使删除 db1，db2 中仍然有 file 文件。 在 db1 中创建了文件 db1_file，db2 可以看到 db1_file，删除 db1 容器后，db1_file 在 db2 中还可以看到。 可以利用数据卷容器对其中的数据卷进行备份、恢复，以实现数据的迁移。 备份： 使用下面的命令来备份 dbdata 数据卷容器内的数据卷： docker run --volumes-from dbdata -v ${PWD}:/backup --name worker ubuntu \tar cvf /backup/backup.tar /dbdata 说明： 利用 ubuntu 镜像创建一个容器 worker。使用–volumes-from dbdata 参数来让 worker 容器挂载 dbdata 的数据卷；使用 ${pwd}:/backup 参数来挂载本地目录到 worker 容器的 /backup 目录。 worker 启动后，使用 tar 命令将 /dbdata 下的内容备份为容器内的 /backup/backup.tar。 创建 dbdata 数据卷容器并写入文件：fileA、fileB、fileC 执行备份命令创建备份 tar 包： 恢复： 如果恢复数据到一个容器，可以参照下面的操作。首先创建一个带有数据卷的容器 dbdata2: docker run -d -v /dbdata--name dbdata2 ubuntu /bin/bash 然后创建另一个新的容器，挂载 dbdata2 的容器，并使用 tar 命令解压备份文件到挂载的容器卷中即可： docker run --volumes-fromdbdata2 -v ${pwd}:/backup ubuntu tar xvf /backup/backup.tar ***当你发现自己的才华撑不起野心时，就请安静下来学习吧*** 参考资料：https://www.cnblogs.com/kevingrace/p/6238195.html]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一起学 Docker（五）-- 仓库]]></title>
    <url>%2F2018%2F05%2F22%2F%E4%B8%80%E8%B5%B7%E5%AD%A6Docker%EF%BC%88%E4%BA%94%EF%BC%89--%E4%BB%93%E5%BA%93%2F</url>
    <content type="text"><![CDATA[1.Docker Hub仓库是集中存放镜像的地方。 目前 Docker 官方仓库维护了一个公共仓库https://hub.docker.com，其中已经包括 15000 多个的镜像。 大部分需求都可以通过在 Docker Hub 中直接下来镜像来实现。 登录 可以通过执行 docker login 命令来输入用户名、密码和邮箱来完成注册登录。 基本操作 用户无需登录可以通过 docker search 命令来查找官方仓库中的镜像，并利用 docker pull 下载到本地，可以通过 docker push 命令将本地镜像推送到 docker hub。 先 tag 一下复制一个镜像，然后把镜像 push 到服务器上 2. 创建和使用私有仓库 使用 registry 镜像创建私有仓库 可以通过 docker 官方提供的 registry 镜像来搭建一套本地私有仓库。 镜像地址：https://hub.docker.com/_/registry/ 命令： docker run -e SEARCH_BACKEND=sqlalchemy -e SQLALCHEMY_INDEX_DATABASE=sqlite:////tmp/docker-registry.db -d --name registry -p 5000:5000 registry -e 设定环境变量 -d 从后台启动的方式镜像启动 -name 启动的容器起个名字 -p 暴露端口，容器内部的 5000 绑定到宿主机的 5000 端口上。 registry 镜像本身 SEARCH_BACKEND=sqlalchemy 默认索引是可以查询的 参考地址： https://github.com/docker/docker-registry#search-engine-options https://hub.docker.com/_/registry/ 自动下载并启动一个 registry 容器，创建本地的私有仓库服务。 默认仓库创建在 /tmp/registry 目录下。 上传到本地的私有仓库中, 报错了：http:server gave HTTP response to HTTPS client 后面会告诉你如何解决往下看。 docker 启动参数配置： 环境：centos7 解决上边的问题 配置文件：/lib/systemd/system/docker.service 修改成： #ExecStart=/usr/bin/dockerd ExecStart=/usr/bin/dockerd-H tcp://0.0.0.0:2375 -H unix:///var/run/docker.sock --insecure-registry192.168.100.146:5000 （此处默认 2375 为主管理端口，unix:///var/run/docker.sock 用于本地管理，7654 是备用的端口） 重启服务，在启动一个私有仓库的容器，然后 push 到私有仓库中 参考地址：https://docs.docker.com/engine/admin/configuring/ 3. 仓库加速服务 加速下载官方镜像。 推荐服务：https://dashboard.daocloud.io/ 点击加速器：https://dashboard.daocloud.io/mirror 配置 Docker 加速器： 下载第三方官方仓库。 4. 仓库 web 管理工具 DockerUIDocker 提供一个平台来把应用程序当作容器来打包、分发、共享和运行，它已经通过节省工作时间来拯救了成千上万的系统管理员和开发人员。Docker 不用关注主机上运行的操作系统是什么，它没有开发语言、框架或打包系统的限制，并且可以在任何时间、任何地点运行，从小型计算机到高端服务器都可以。由于运行 Docker 容器和管理它们可能会花费一点点努力和时间，因为基于 web 的应用程序－DockerUI 应运而生，它可以让管理和运行容器变得很简单。DockerUI 是一个开源的基于 Docker API 的 web 应用程序，提供等同 Docker 命令行的大部分功能，支持 container 管理，image 管理。它最值得称道的是它华丽的设计和用来运行和管理 docker 的简洁的操作界面。 DockerUI 优点： 1）支持 container 批量操作； 2）支持 image 管理（虽然比较薄弱） DockerUI 缺点： 不支持多主机。 下面记录在 DockerUI 管理环境的部署过程： 1）首先拉去 dockerUI 镜像，现在 dockerUI 镜像位置变了，如下： [root@localhost ~]# docker pull uifd/ui-for-docker // 之前镜像位置为 dockerui/dockerui Using default tag: latest latest: Pulling from uifd/ui-for-docker 841194d080c8: Pull complete Digest: sha256:3f015313831d8b777760ea5b17a92d9372c1ef450bff9df5bd5f9fca56718215 Status: Downloaded newer image for uifd/ui-for-docker:latest [root@localhost ~]# docker images REPOSITORY TAG IMAGE IDCREATED SIZE nginx latest be1f31be9a875 days ago 109MB registrylatest 2e2f252f3c883 weeks ago 33.3MB hello-world latest 4ab4c602aa5e4 weeks ago 1.84kB yhaing/hello-world latest 4ab4c602aa5e4 weeks ago 1.84kB 192.168.56.47:5000/yh/ubuntu_test latest cd6d8154f1e14 weeks ago 84.1MB ubuntu latest cd6d8154f1e14 weeks ago 84.1MB uifd/ui-for-docker latest 965940f98fa52 years ago 8.1MB 2）启动 DockerUI 容器。如果服务器开启了 Selinux，那么就得使用–privileged 标志。这里我在本机关闭了 Selinux，所以不用添加–privileged [root@localhost ~]# docker run -it -d --name registry_ui -p 9000:9000 -v /var/run/docker.sock:/var/run/docker.sock uifd/ui-for-docker [root@localhost ~]# docker ps CONTAINER IDIMAGE COMMAND CREATED STATUS PORTSNAMES c078f0f41285docker.io/uifd/ui-for-docker &quot;/ui-for-docker&quot;4 seconds ago Up 2 seconds0.0.0.0:9000-&gt;9000/tcp docker-web [root@localhost ~]# vim /etc/sysconfig/iptables ...... -A INPUT -p tcp -m state --state NEW -m tcp --dport 9000 -j ACCEPT [root@localhost ~]# systemctl restart iptables.service 接着就可以在浏览器访问 DockerUI 管理界面了。启动了 DockerUI 容器之后，就可以用它来执行启动、暂停、终止、删除以及 DockerUI 提供的其它操作 Docker 容器的命令。在浏览器里面输入 http://ip-address:9000，默认情况下登录不需要认证，但是可以配置我们的 web 服务器来要求登录认证。 DockerUI 的管理： 1）Dashboard 控制台 。点击Running Containers 下面活跃的容器，进入容器的管理界面进行相关操作，比如修改容器名，commit 提交容器为新的镜像等。 2）container 容器管理。点击Display All ，可以显示所有创建了的容器，包括没有启动的。然后点击 Action，可以对容器进行启动，关闭，重启，删除，挂起等操作。 3）images 镜像管理。点击Action，可以对已有的镜像镜像移除操作。点击Pull，可以拉取镜像。点击镜像 ID 进去后可以添加或移除镜像 tag 如下截图，Pull 镜像的时候，Registry 为空，默认从 docker hub 上拉取镜像。 点击镜像 ID 进入，可以添加或删除镜像 tag 标识。 *当你发现自己的才华撑不起野心时，就请安静下来学习吧*]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker 集中化 web 界面管理平台 -Shipyard(官网已经停止维护)]]></title>
    <url>%2F2018%2F05%2F22%2FDocker%E9%9B%86%E4%B8%AD%E5%8C%96web%E7%95%8C%E9%9D%A2%E7%AE%A1%E7%90%86%E5%B9%B3%E5%8F%B0-Shipyard%E9%83%A8%E7%BD%B2%E8%AE%B0%E5%BD%95%2F</url>
    <content type="text"><![CDATA[今天在网上流量 docker 文章时偶然发现了一个 Docker 的 web 管理工具 shipyard，今天想起来介绍下，然而却发现官网无法打开，去作者的 GitHub 一看缺发现这个项目已经放弃维护了，也是相当可惜啊。 PS.shipyard 是船坞或者船厂的意思，结合 Docker 一直以来是条类似货轮的鲸鱼，其实这个名字挺合适的。 作者由于没有时间与精力继续维护下去，在去年八月份就开始询问是否有人感兴趣接手这个项目，可惜过了几个月依旧没有人出现，只能无奈的决定停止这个项目，官网也被关掉了。开源届每隔一阵子就会出现一个这样的项目(当然我指的是比较大的项目，小的估计每几天都有凉凉的)，也是希望能有更多有能力的人能够加入为开源项目贡献代码的行列。 之前介绍了DOcker 的 web 管理工具 DockerUI，下面介绍下 Docker 的另一个 web 界面管理工具 Shipyard 的使用。Shipyard（github）是建立在 docker 集群管理工具 Citadel 之上的可以管理容器、主机等资源的 web 图形化工具, 包括 core 和 extension 两个版本，core 即 shipyard 主要是把多个 Docker host 上的 containers 统一管理（支持跨越多个 host），extension 即 shipyard-extensions 添加了应用路由和负载均衡、集中化日志、部署等;Shipyard 是在 Docker Swarm 实现对容器、镜像、docker 集群、仓库、节点进行管理的 web 系统。 ———————DockerUI——————— 功能：通过 Web 浏览器的命令行来管理的任务。 DockerUI 的优点 1）可以对运行着的容器进行批量操作 2）在容器网络中 - 会显示容器与容器间的网络关系 3）在 Volumes 中显示了所有挂载目录 DockerUI 一个致命的缺点：不支持多主机 试想一下，如果有 N 台 docker 主机时 - 我就需要一台台的用 dockerui 进行管理，想想都觉得麻烦。就单台主机而言，dockerui 是一款不错的管理工具。 ———————Shipyard——————— 功能：简化对横跨多个主机的 Docker 容器集群进行管理 通过 Web 用户界面，你可以大致浏览相关信息，比如你的容器在使用多少处理器和内存资源、在运行哪些容器，还可以检查所有集群上的事件日志。 其特性主要包括： 1）支持节点动态集群，可扩展节点的规模（swarm、etcd 方案） 2）支持镜像管理、容器管理、节点管理等功能 3）可视化的容器管理和监控管理 4）在线容 console 终端 了解 Shipyard 几个概念 1）engine 一个 shipyard 管理的 docker 集群可以包含一个或多个 engine（引擎），一个 engine 就是监听 tcp 端口的 docker daemon。shipyard 管理 docker daemon、images、containers 完全基于 Docker API，不需要做其他的修改。另外，shipyard 可以对每个 engine 做资源限制，包括 CPU 和内存；因为 TCP 监听相比 Unix socket 方式会有一定的安全隐患，所以 shipyard 还支持通过 SSL 证书与 docker 后台进程安全通信。 2）rethinkdbRethinkDB 是一个 shipyard 项目的一个 docker 镜像，用来存放账号（account）、引擎（engine）、服务密钥（service key）、扩展元数据（extension metadata）等信息，但不会存储任何有关容器或镜像的内容。 Shipyard 生态 shipyard 是由 shipyard 控制器以及周围生态系统构成，以下按照 deploy 启动顺序进行介绍（下面几个就是 shipyard 使用脚本安装后，启动的几个容器名） 1）RethinkDB deploy 首先启动的就是 RethinkDB 容器，shipyard 采用 RethinkDB 作为数据库来保存用户等信息 2）Discovery 为了使用 Swarm，我们需要一个外部的密钥值存储群容器，shipyard 默认是采用了 etcd。 3）shipyard_certs 证书管理容器，实现证书验证功能 4）Proxy 默认情况下，Docker 引擎只监听 Socket，我们可以重新配置引擎使用 TLS 或者使用一个代理容器，转发请求从 TCP 到 Docker 监听的 UNIX Socket。 5）Swarm Manager Swarm 管理器 6）Swarm Agent Swarm 代理，运行在每个节点上。 7）Controller shipyard 控制器，Remote API 的实现和 web 的实现。 Shipyard 部署过程（下面是 centos7 下的安装记录）好了，不说其他了，虽然官网挂了，项目也停止了，但是还是能用的，来体验下吧 服务器 ip：182.48.115.233主机名：node-1centos7.2 1）安装 docker 并配置加速 [root@node-1 ~]# yum install docker [root@node-1 ~]# systemctl start docker 修改 docker 配置文件，添加下面一行，加速设置 [root@node-1 ~]# vim /etc/sysconfig/docker ...... ADD_REGISTRY=&apos;--add-registry xxx.mirror.aliyuncs.com&apos; 然后重启 docker [root@node-1 ~]# systemctl restart docker 2）下载镜像（这些镜像其实可以不用提前下载，执行下面安装 shipyard 的命令后就会自动下载这些镜像） [root@node-1 ~]# docker pull alpine [root@node-1 ~]# docker pull library/rethinkdb [root@node-1 ~]# docker pull microbox/etcd [root@node-1 ~]# docker pull shipyard/docker-proxy [root@node-1 ~]# docker pull swarm [root@node-1 ~]# docker pull shipyard/shipyard [root@node-1 ~]# docker images REPOSITORY TAG IMAGE IDCREATED SIZE docker.io/alpinelatest 02674b9cb1796 days ago 3.984 MB xxx.mirror.aliyuncs.com/library/rethinkdb latest b66f932ecd3c6 days ago 182.6 MB docker.io/nginx latest 3448f27c273f7 days ago 109.4 MB docker.io/swarm latest 36b1e23becab3 months ago15.85 MB xxx.mirror.aliyuncs.com/shipyard/shipyard latest 36fb3dc0907d7 months ago58.82 MB xxx.mirror.aliyuncs.com/shipyard/docker-proxy latest cfee14e5d6f216 months ago 9.464 MB xxx.mirror.aliyuncs.com/microbox/etcd latest 6aef84b9ec5a21 months ago 17.86 MB 3）安装 shipyard shipyard 的部署非常简单，因为官网已经停止维护了，只好通过下面的方式安装： # 安装主节点(管理端) curl -s https://raw.githubusercontent.com/shipyard/shipyard-project.com/master/site/themes/shipyard/static/deploy | bash -s #添加节点 curl -sSL https://raw.githubusercontent.com/shipyard/shipyard-project.com/master/site/themes/shipyard/static/deploy | ACTION=node DISCOVERY=etcd:// 主节点 IP:4001 bash -s 注意：由于 deploy 在执行时需要访问 /var/run/docker.sock，所以需要 root 权限，或者为该文件添加权限。 [root@localhost ~]# curl -s https://raw.githubusercontent.com/shipyard/shipyard-project.com/master/site/themes/shipyard/static/deploy | bash -s Deploying Shipyard -&gt; Starting Database -&gt; Starting Discovery -&gt; Starting Cert Volume -&gt; Starting Proxy -&gt; Starting Swarm Manager -&gt; Starting Swarm Agent -&gt; Starting Controller Waiting for Shipyard on 182.48.115.233:8080 Shipyard available at http://182.48.115.233:8080 Username: admin Password: shipyard 至此，shipyard 就已经安装完成了。使http://182.48.115.233:8080 就可以访问了（iptables 防火墙要是开启了，要记得打开 8080 端口访问），默认用户名为 admin 密码为 shipyard。 登录进去后，可以在”ACCOUNTS”选项里管理用户，可以添加用户，并对用户进行角色授权。 其他需要注意的几点 注意一下： 1）最好关闭防火墙 2）添加 Node 节点可能失败，可以进行多次尝试 添加 node 节点的操作curl -sSL https://raw.githubusercontent.com/shipyard/shipyard-project.com/master/site/themes/shipyard/static/deploy | ACTION=node DISCOVERY=etcd:// 主节点 IP:4001 bash -s 节点添加之后，访问 shipyard 页面，发现节点已经添加上了，如下： 可以在 shipyard 页面里做所添加的节点机里的 docker 容器进行关闭、删除、重启、重命名、提交新镜像、状态和日志查看能操作，如下： 可以在 shipyard 页面里添加镜像和容器，还可以添加私有仓库 Registry。 另外，在 shipyard 访问页面里，还提供了容器的 console 控制台，如下： ***当你发现自己的才华撑不起野心时，就请安静下来学习吧*** 参考：Docker 集中化 web 界面管理平台 -Shipyard 部署记录]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一起学 Docker（三）-- 镜像的常用操作]]></title>
    <url>%2F2018%2F05%2F21%2F%E4%B8%80%E8%B5%B7%E5%AD%A6Docker%EF%BC%88%E4%B8%89%EF%BC%89--%E9%95%9C%E5%83%8F%E7%9A%84%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[1. 获取镜像：命令： docker pull &lt; 域名 &gt;/&lt;namespace&gt;/&lt;repo&gt;:&lt;tag&gt; 说明： 镜像是 Docker 运行容器的前提。 用户可以使用 docker pull 命令从网络上下载镜像。对于镜像来说，如果不显式地指定 tag, 则默认会选择 latest 标签，即下载仓库中最新版本的镜像。 默认是从 docker 官方下载的。只有 docker 官方的可以不需要增加命名空间直接进行下载。 2. 查看镜像列表 命令： docker images 说明： 使用 docker images 命令可以列出本地主机上已有的镜像。 信息含义：来自于哪个仓库、镜像的标签信息、镜像的 ID 号（唯一）、创建时间、镜像大小。 3. 查看镜像信息 命令： docker inspect &lt;image_id&gt; 说明： docker inspect 命令返回的是一个 JSON 的格式消息，如果我们只要其中的一项内容时，可以通过 -f 参数来指定。Image_id 通常可以使用该镜像 ID 的前若干个字符组成的可区分字符串来替代完成的 ID。 查看镜像的某一个详细信息 4. 查找镜像 命令： docker search &lt;image_name&gt; 说明： 使用 docker search 命令可以搜索远端仓库中共享的镜像，默认搜索 Docker hub 官方仓库中的镜像。 5. 删除镜像 命令： docker rmi &lt;image&gt;:&lt;tag&gt; 说明： 使用 docker rmi 命令可以删除镜像，其中 image 可以为标签或 ID。 注意： 当同一个镜像拥有多个标签，docker rmi 只是删除该镜像多个标签中的指定标签而已，而不影响镜像文件。 当有该镜像创建的容器存在时，镜像文件默认是无法被删除的。 如果一个镜像就有一个 tag 的话，删除 tag 就删除了镜像的本身。 一个镜像做一个 tag 执行删除 tag 操作 删除镜像操作 重新下载镜像，方便下一步关于镜像和容器的关系演示 如果镜像里面有容器正在运行，删除镜像的话，会提示 error，系统默认是不允许删除的，如果强制删除需要加入 -f 操作，但是 docker 是不建议这么操作的，因为你删除了镜像其实容器并未删除，直接导致容器找不到镜像，这样会比较混乱。 运行一个镜像里面的容器 查看运行中的容器 删除镜像，报错误 error，有一个容器正在这个镜像内运行 强制删除 已经找不到镜像，删除镜像未删除容器的后果 6. 创建镜像 命令： docker commit &lt;options&gt; &lt;container_id&gt;&lt;repository:tag&gt; 参数说明： -a , --author : 作者信息 -m , --meassage : 提交消息 -p , --pause=true : 提交时暂停容器运行 说明： 基于已有的镜像的容器的创建。再次下载 ubuntu，以 ubuntu 为例子创建 运行 ubuntu，-ti 把容器内标准绑定到终端并运行 bash，这样开跟传统的 linux 操作系统没什么两样，现在我们直接在容器内运行。这个内部系统都是极简的只保留我们的一些系统的运行参数，里面很多 vi 命令可能都是没有的。 退出容器 exit 容器创建成镜像的方法： 通过某个容器 d1d6706627f1 创建对应的镜像，有点类似 git 发现通过 docker images 里面多了一个镜像 liming/test 的仓库 7. 迁出镜像 命令： docker save -o &lt;image&gt;.tar&lt;image&gt;:&lt;tag&gt; 参数说明： -o: 设置存储压缩后的文件名称 说明： 可以使用 docker save 命令来迁出镜像，其中 image 可以为标签或 ID。 8. 载入镜像 命令： docker load --input &lt;image&gt;.tar 或 docker load &lt; &lt;image&gt;.tar 说明： 使用 docker load 命令可以载入镜像，其中 image 可以为标签或 ID。这将导入镜像及相关的元数据信息（包括标签等），可以使用 docker images 命令进行查看。我们先删除原有的 liming/test 镜像，执行查看镜像，然后在导入镜像 为了确定导入的镜像是否是原来删除的那个镜像，我们进入镜像，查看下 text.txt 是我们输入的 docker 这个内容 可能这个镜像的名字不符合 docker 的要求因为都是, 重新命名一下 9. 上传镜像 命令： docker push &lt; 域名 &gt;/&lt;namespace&gt;/&lt;repo&gt;:&lt;tag&gt; 说明： 可以使用 docker push 命令上传镜像到仓库，默认上传到 DockerHub 官方仓库（需要登录）。 开始提示我权限不足，因为我没有登录。 登录之后提示 登录后删除还是权限不足 因为我们 ubuntu/test 这个名字跟官网的 yhaing 名字不一致我修改下在试试 开始上传了 去官网看看是否上传成功 Docker 命令帮助docker helpdocker command $ sudo docker # docker 命令帮助 Commands: attach Attach to a running container # 当前 shell 下 attach 连接指定运行镜像 build Build an image from a Dockerfile # 通过 Dockerfile 定制镜像 commit Create a new image from a container&apos;s changes #提交当前容器为新的镜像 cp Copy files/folders from the containers filesystem to the host path # 从容器中拷贝指定文件或者目录到宿主机中 create Create a new container # 创建一个新的容器，同 run，但不启动容器 diff Inspect changes on a container&apos;s filesystem # 查看 docker 容器变化 events Get real time events from the server # 从 docker 服务获取容器实时事件 exec Run a command in an existing container # 在已存在的容器上运行命令 export Stream the contents of a container as a tar archive # 导出容器的内容流作为一个 tar 归档文件[对应 import] history Show the history of an image # 展示一个镜像形成历史 images List images # 列出系统当前镜像 import Create a new filesystem image from the contents of a tarball # 从 tar 包中的内容创建一个新的文件系统映像[对应 export] info Display system-wide information # 显示系统相关信息 inspect Return low-level information on a container # 查看容器详细信息 kill Kill a running container # kill 指定 docker 容器 load Load an image from a tar archive # 从一个 tar 包中加载一个镜像[对应 save] login Register or Login to the docker registry server # 注册或者登陆一个 docker 源服务器 logout Log out from a Docker registry server # 从当前 Docker registry 退出 logs Fetch the logs of a container # 输出当前容器日志信息 port Lookup the public-facing port which is NAT-ed to PRIVATE_PORT # 查看映射端口对应的容器内部源端口 pause Pause all processes within a container # 暂停容器 ps List containers # 列出容器列表 pull Pull an image or a repository from the docker registry server # 从 docker 镜像源服务器拉取指定镜像或者库镜像 push Push an image or a repository to the docker registry server # 推送指定镜像或者库镜像至 docker 源服务器 restart Restart a running container # 重启运行的容器 rm Remove one or more containers # 移除一个或者多个容器 rmi Remove one or more images # 移除一个或多个镜像[无容器使用该镜像才可删除，否则需删除相关容器才可继续或 -f 强制删除] run Run a command in a new container # 创建一个新的容器并运行一个命令 save Save an image to a tar archive # 保存一个镜像为一个 tar 包[对应 load] search Search for an image on the Docker Hub # 在 docker hub 中搜索镜像 start Start a stopped containers # 启动容器 stop Stop a running containers # 停止容器 tag Tag an image into a repository # 给源中镜像打标签 top Lookup the running processes of a container # 查看容器中运行的进程信息 unpause Unpause a paused container # 取消暂停容器 version Show the docker version information # 查看 docker 版本号 wait Block until a container stops, then print its exit code # 截取容器停止时的退出状态值 Run &apos;docker COMMAND --help&apos; for more information on a command. docker option Usage of docker: --api-enable-cors=false Enable CORS headers in the remote API # 远程 API 中开启 CORS 头 -b, --bridge=&quot;&quot; Attach containers to a pre-existing network bridge use &apos;none&apos; to disable container networking # 桥接网络 --bip=&quot;&quot; Use this CIDR notation address for the network bridge&apos;s IP, not compatible with -b # 和 -b 选项不兼容，具体没有测试过 -d, --daemon=false Enable daemon mode # daemon 模式 -D, --debug=false Enable debug mode # debug 模式 --dns=[] Force docker to use specific DNS servers # 强制 docker 使用指定 dns 服务器 --dns-search=[] Force Docker to use specific DNS search domains # 强制 docker 使用指定 dns 搜索域 -e, --exec-driver=&quot;native&quot; Force the docker runtime to use a specific exec driver # 强制 docker 运行时使用指定执行驱动器 --fixed-cidr=&quot;&quot; IPv4 subnet for fixed IPs (ex: 10.20.0.0/16) this subnet must be nested in the bridge subnet (which is defined by -b or --bip) -G, --group=&quot;docker&quot; Group to assign the unix socket specified by -H when running in daemon mode use &apos;&apos; (the empty string) to disable setting of a group #指定容器运行的用户组 -g, --graph=&quot;/var/lib/docker&quot; Path to use as the root of the docker runtime # 容器运行的根目录路径 -H, --host=[] The socket(s) to bind to in daemon mode specified using one or more tcp://host:port, unix:///path/to/socket, fd://* or fd://socketfd. # daemon 模式下 docker 指定绑定方式[tcp or 本地 socket] --icc=true Enable inter-container communication # 跨容器通信 --insecure-registry=[] Enable insecure communication with specified registries (no certificate verification for HTTPS and enable HTTP fallback) (e.g., localhost:5000 or 10.20.0.0/16) --ip=&quot;0.0.0.0&quot; Default IP address to use when binding container ports # 指定监听地址，默认所有 ip --ip-forward=true Enable net.ipv4.ip_forward # 开启转发 --ip-masq=true Enable IP masquerading for bridge&apos;s IP range --iptables=true Enable Docker&apos;s addition of iptables rules # 添加对应 iptables 规则 --mtu=0 Set the containers network MTU # 设置网络 mtu if no value is provided: default to the default route MTU or 1500 if no default route is available -p, --pidfile=&quot;/var/run/docker.pid&quot; Path to use for daemon PID file # 指定 pid 文件位置 --registry-mirror=[] Specify a preferred Docker registry mirror -s, --storage-driver=&quot;&quot; Force the docker runtime to use a specific storage driver # 强制 docker 运行时使用指定存储驱动 --selinux-enabled=false Enable selinux support # 开启 selinux 支持 --storage-opt=[] Set storage driver options # 设置存储驱动选项 --tls=false Use TLS; implied by tls-verify flags # 开启 tls --tlscacert=&quot;/root/.docker/ca.pem&quot; Trust only remotes providing a certificate signed by the CA given here --tlscert=&quot;/root/.docker/cert.pem&quot; Path to TLS certificate file # tls 证书文件位置 --tlskey=&quot;/root/.docker/key.pem&quot; Path to TLS key file # tls key 文件位置 --tlsverify=false Use TLS and verify the remote (daemon: verify client, client: verify daemon) # 使用 tls 并确认远程控制主机 -v, --version=false Print version information and quit # 输出 docker 版本信息 docker search$ sudo docker search --help Usage: docker search TERM Search the Docker Hub for images # 从 Docker Hub 搜索镜像 --automated=false Only show automated builds --no-trunc=false Don&apos;t truncate output -s, --stars=0 Only displays with at least xxx stars 示例： $ sudo docker search -s 100 ubuntu # 查找 star 数至少为 100 的镜像，找出只有官方镜像 start 数超过 100，默认不加 s 选项找出所有相关 ubuntu 镜像 NAME DESCRIPTION STARS OFFICIAL AUTOMATED ubuntuOfficial Ubuntu base image 425 [OK] docker info$ sudo docker info Containers: 1 # 容器个数 Images: 22 # 镜像个数 Storage Driver: devicemapper # 存储驱动 Pool Name: docker-8:17-3221225728-pool Pool Blocksize: 65.54 kB Data file: /data/docker/devicemapper/devicemapper/data Metadata file: /data/docker/devicemapper/devicemapper/metadata Data Space Used: 1.83 GB Data Space Total: 107.4 GB Metadata Space Used: 2.191 MB Metadata Space Total: 2.147 GB Library Version: 1.02.84-RHEL7 (2014-03-26) Execution Driver: native-0.2 # 存储驱动 Kernel Version: 3.10.0-123.el7.x86_64 Operating System: CentOS Linux 7 (Core) docker pull &amp;&amp; docker push$ sudo docker pull --help # pull 拉取镜像 Usage: docker pull [OPTIONS] NAME[:TAG] Pull an image or a repository from the registry -a, --all-tags=false Download all tagged images in the repository $ sudo docker push # push 推送指定镜像 Usage: docker push NAME[:TAG] Push an image or a repository to the registry 示例： $ sudo docker pull ubuntu # 下载官方 ubuntu docker 镜像，默认下载所有 ubuntu 官方库镜像 $ sudo docker pull ubuntu:14.04 # 下载指定版本 ubuntu 官方镜像 $ sudo docker push 192.168.0.100:5000/ubuntu # 推送镜像库到私有源[可注册 docker 官方账户，推送到官方自有账户] $ sudo docker push 192.168.0.100:5000/ubuntu:14.04 # 推送指定镜像到私有源 docker images列出当前系统镜像 $ sudo docker images --help Usage: docker images [OPTIONS] [NAME] List images -a, --all=false Show all images (by default filter out the intermediate image layers) # -a 显示当前系统的所有镜像，包括过渡层镜像，默认 docker images 显示最终镜像，不包括过渡层镜像 -f, --filter=[] Provide filter values (i.e. &apos;dangling=true&apos;) --no-trunc=false Don&apos;t truncate output -q, --quiet=false Only show numeric IDs 示例： $ sudo docker images # 显示当前系统镜像，不包括过渡层镜像 $ sudo docker images -a # 显示当前系统所有镜像，包括过渡层镜像 $ sudo docker images ubuntu # 显示当前系统 docker ubuntu 库中的所有镜像 REPOSITORY TAG IMAGE IDCREATED VIRTUAL SIZE ubuntu 12.04 ebe4be4dd427 4 weeks ago 210.6 MB ubuntu 14.04 e54ca5efa2e9 4 weeks ago 276.5 MB ubuntu 14.04-ssh 6334d3ac099a 7 weeks ago 383.2 MB docker rmi删除一个或者多个镜像 $ sudo docker rmi --help Usage: docker rmi IMAGE [IMAGE...] Remove one or more images -f, --force=false Force removal of the image # 强制移除镜像不管是否有容器使用该镜像 --no-prune=false Do not delete untagged parents # 不要删除未标记的父镜像 docker run$ sudo docker run --help 常用选项说明 -d, --detach=false， 指定容器运行于前台还是后台，默认为 false -i, --interactive=false， 打开 STDIN，用于控制台交互 -t, --tty=false， 分配 tty 设备，该可以支持终端登录，默认为 false -u, --user=&quot;&quot;， 指定容器的用户 -a, --attach=[]， 登录容器（必须是以 docker run -d 启动的容器） -w, --workdir=&quot;&quot;， 指定容器的工作目录 -c, --cpu-shares=0， 设置容器 CPU 权重，在 CPU 共享场景使用 -e, --env=[]， 指定环境变量，容器中可以使用该环境变量 -m, --memory=&quot;&quot;， 指定容器的内存上限 -P, --publish-all=false， 指定容器暴露的端口 -p, --publish=[]， 指定容器暴露的端口 -h, --hostname=&quot;&quot;， 指定容器的主机名 -v, --volume=[]， 给容器挂载存储卷，挂载到容器的某个目录 --volumes-from=[]， 给容器挂载其他容器上的卷，挂载到容器的某个目录 --cap-add=[]， 添加权限，权限清单详见：http://linux.die.net/man/7/capabilities --cap-drop=[]， 删除权限，权限清单详见：http://linux.die.net/man/7/capabilities --cidfile=&quot;&quot;， 运行容器后，在指定文件中写入容器 PID 值，一种典型的监控系统用法 --cpuset=&quot;&quot;， 设置容器可以使用哪些 CPU，此参数可以用来容器独占 CPU --device=[]， 添加主机设备给容器，相当于设备直通 --dns=[]， 指定容器的 dns 服务器 --dns-search=[]， 指定容器的 dns 搜索域名，写入到容器的 /etc/resolv.conf 文件 --entrypoint=&quot;&quot;， 覆盖 image 的入口点 --env-file=[]， 指定环境变量文件，文件格式为每行一个环境变量 --expose=[]， 指定容器暴露的端口，即修改镜像的暴露端口 --link=[]， 指定容器间的关联，使用其他容器的 IP、env 等信息 --lxc-conf=[]， 指定容器的配置文件，只有在指定 --exec-driver=lxc 时使用 --name=&quot;&quot;， 指定容器名字，后续可以通过名字进行容器管理，links 特性需要使用名字 --net=&quot;bridge&quot;， 容器网络设置: bridge 使用 docker daemon 指定的网桥 host // 容器使用主机的网络 container:NAME_or_ID &gt;// 使用其他容器的网路，共享 IP 和 PORT 等网络资源 none 容器使用自己的网络（类似 --net=bridge），但是不进行配置 --privileged=false， 指定容器是否为特权容器，特权容器拥有所有的 capabilities --restart=&quot;no&quot;， 指定容器停止后的重启策略: no：容器退出时不重启 on-failure：容器故障退出（返回值非零）时重启 always：容器退出时总是重启 --rm=false， 指定容器停止后自动删除容器(不支持以 docker run -d 启动的容器) --sig-proxy=true， 设置由代理接受并处理信号，但是 SIGCHLD、SIGSTOP 和 SIGKILL 不能被代理 docker start|stop|kill… …dockerstart|stop|kill|restart|pause|unpause|rm|commit|inspect|logs docker start CONTAINER [CONTAINER...] # 运行一个或多个停止的容器 docker stop CONTAINER [CONTAINER...] # 停掉一个或多个运行的容器 -t 选项可指定超时时间 docker kill [OPTIONS] CONTAINER [CONTAINER...] # 默认 kill 发送 SIGKILL 信号 -s 可以指定发送 kill 信号类型 docker restart [OPTIONS] CONTAINER [CONTAINER...] # 重启一个或多个运行的容器 -t 选项可指定超时时间 docker pause CONTAINER # 暂停一个容器，方便 commit docker unpause CONTAINER # 继续暂停的容器 docker rm [OPTIONS] CONTAINER [CONTAINER...] # 移除一个或多个容器 -f, --force=false Force removal of running container-l, --link=false Remove the specified link and not the underlying container-v, --volumes=false Remove the volumes associated with the container docker commit [OPTIONS] CONTAINER [REPOSITORY[:TAG]] # 提交指定容器为镜像 -a, --author=&quot;&quot; Author (e.g., &quot;John Hannibal Smith hannibal@a-team.com&quot;)-m, --message=&quot;&quot; Commit message-p, --pause=true Pause container during commit # 默认 commit 是暂停状态 docker inspect CONTAINER|IMAGE [CONTAINER|IMAGE...] # 查看容器或者镜像的详细信息 docker logs CONTAINER # 输出指定容器日志信息 -f, --follow=false Follow log output # 类似 tail -f -t, --timestamps=false Show timestamps--tail=&quot;all&quot; Output the specified number of lines at the end of logs (defaults to all logs) 参考文档：Docker Run Reference docker exec –helpUsage: docker exec [OPTIONS] CONTAINER COMMAND [ARG...] Run a command in an existing container -d, --detach=false Detached mode: run command in the background -i, --interactive=false Keep STDIN open even if not attached -t, --tty=false Allocate a pseudo-TTY 为了简化调试，可以使用 docker exec 命令通过 Docker API 和 CLI 在运行的容器上运行程序。 $ docker exec -it ubuntu_bash bash 上例将在容器 ubuntu_bash 中创建一个新的 Bash 会话。 Tune container lifecycles withdocker create 我们可以通过 docker run 命令创建一个容器并运行其中的程序，因为有很多用户要求创建容器的时候不启动容器，所以 docker create 应运而生了。 $ docker create -t -i fedora bash 6d8af538ec541dd581ebc2a24153a28329acb5268abe5ef868c1f1a261221752 上例创建了一个可写的容器层 (并且打印出容器 ID)，但是并不运行它，可以使用以下命令运行该容器： $ docker start -a -i 6d8af538ec5 bash-4.2# Security Options通过–security-opt 选项，运行容器时用户可自定义 SELinux 和 AppArmor 卷标和配置。 $ docker run --security-opt label:type:svirt_apache -i -t centos \ bash 上例只允许容器监听在 Apache 端口，这个选项的好处是用户不需要运行 docker 的时候指定–privileged 选项，降低安全风险。 参考文档：Docker 1.3: signed images, process injection, security options, Mac shared directories]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一起学 Docker（四）-- 容器的常用操作]]></title>
    <url>%2F2018%2F05%2F21%2F%E4%B8%80%E8%B5%B7%E5%AD%A6Docker%EF%BC%88%E5%9B%9B%EF%BC%89--%E5%AE%B9%E5%99%A8%E7%9A%84%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[1．创建容器 Docker 的容器十分轻量级，用户可以随时创建或删除容器。 新建容器：docker create Example：docker create –ti ubuntu 说明：使用 docker create 命令创建的容器处于停止状态，可以使用 docker start 命令启动它。 新增加了一个 name 等于 test_create 的，status 等于 create 新建并启动容器:docker run Example: docker run ubuntu/bin/echo “Hello World” 说明： 等价于先执行 docker create 命令，再执行 docker start 命令。 docker run 背后的故事： 1 检查本地是否存在制定的镜像，不存在就从公有仓库下载。 2 利用本地镜像创建并启动一个容器。 3 分配一个文件系统，并在只读的镜像层外面挂载一层可读写层。 4 从宿主机配置的网桥接口桥接一个虚拟接口到容器中去。 5 从地址池配置一个 IP 地址给容器。 6 执行用户的指定的用户程序。 7 执行完毕后容器被终止。 一条简单的命令： docker run -i –t ubuntu/bin/bash -t : 让 docker 分配一个伪终端并绑定到容器的标准输入上。 -i : 让容器的标准输入保持打开。 在交互模式下，用户可以通过所创建的终端来输入命令，exit 命令退出容器。 退出后，容器自动处于终止状态。 4 秒前容器被退出了。 守护台运行： 更多的时候，需要让 Docker 容器运行在后台以守护态（daemonized）形式运行。用户可以通过添加 -d 参数来实现。 Example: docker run –d ubuntu/bin/sh -c “while true;do echo hello world;sleep 1;done” 补充： 查看日志： docker logs&lt;container_id&gt; docker logs –f &lt;container_id&gt; 动态的查看日志，类似咱们查看 tomcat 的日志一样 2．终止容器 可以使用 docker stop 命令来终止一个运行中的容器。 docker stop&lt;container_id&gt; 注意： 当容器中的应用终结时，容器也会自动停止。 查看终止的容器：docker ps -a 查看运行的容器：docker ps 重新启动容器：docker start&lt;container_id&gt; 3．进入容器 在使用 -d 参数时，容器启动后会进入后台，用户无法看到容器中的信息。 docker exec &lt;options&gt;&lt;container_id&gt; &lt;command&gt; Exec 可以直接在容器内部运行命令。 进入容器： docker exec -i –t&lt;container_id&gt; bash run 运行一个容器后，进入容器的话，exit 容器直接退出 exec 进入容器后，exit 容器不退出，仍在后台运行 4．删除容器 可以使用 docker rm 命令删除终止状态的容器。 如果删除正在运行的容器，需要停止容器在进行删除 不管容器是否运行，可以使用 docker rm–f 命令进行删除。 5．导入和导出容器 导出容器是指导出一个已经创建的容器到一个文件，不管容器是否处于运行状态。可以使用 docker export 命令。 docker export&lt;container_id&gt; Example: Docker export test_id &gt;test.tar 导出的文件又可以使用 docker import 命令导入，成为镜像。 Example: cat export.tar | docker import - liming/testimport:latest 导入容器生成镜像，通过镜像生成容器，查看容器里面的内容 如何获得容器 ID？有几种方法可以做到这一点。手动方法是列出所有正在运行的容器，并在列表中找到您要查找的容器。只需从那里复制其 ID。 更自动化的方法是使用 shell 脚本和环境变量。例如，如果要获取引号容器的 ID，这是一个示例： $ export CONTAINER_ID = $(docker container ls | grep quotes | awk &apos;{print $1}&apos;) 这里我们使用 AWK 获取第一个字段，即容器 ID,注意这里取的是正在运行的容器的 ID。现在，您可以在表达式中使用 $CONTAINER_ID 变量，而不是使用容器名称： $ docker container stop $CONTAINER_ID 一旦停止容器后，其状态将更改为“已退出”。 你可以使用 docker container start 命令重新启动已停止的容器。]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一起学 Docker（二）-- 核心概念和安装]]></title>
    <url>%2F2018%2F05%2F20%2F%E4%B8%80%E8%B5%B7%E5%AD%A6Docker%EF%BC%88%E4%BA%8C%EF%BC%89--%E6%A0%B8%E5%BF%83%E6%A6%82%E5%BF%B5%E5%92%8C%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[环境介绍： 操作系统：64bit CentOS7 docker 版本：18.06.1-ce（最新版本） 版本新功能： hhttps://github.com/docker/docker-ce/releases/tag/v18.06.1-ce 安装步骤 系统：64 位 centos7 迅雷直接下载： http://mirrors.aliyun.com/centos/7.4.1708/isos/x86_64/CentOS-7-x86_64-DVD-1708.iso 虚拟机：virtualBox 或 VMware Workstation 最好是安装完整版本的 centos7，vm 安装 centos7 的过程我这里都不截图了，比较简单百度都可以看到。 要求: 内核版本最低为 3.10 查看当前内核版本： uname –r 要求: 更改网卡配置 更改网卡配置：vim /etc/sysconfig/network-scripts/ifcfg-ens33 更改完后重启服务：service network restart 注意：如果 ifconfig 命令不识别的话需要安装： yum installnet-tools 通过 yum 方式安装最新版 Docker第一步：更新 yum 源： sudo yum update 第二步：卸载旧版本(如果安装过旧版本的话) yum remove docker docker-common docker-selinux docker-engine 第三步：安装需要的软件包， yum-util 提供 yum-config-manager 功能，另外两个是 devicemapper 驱动依赖的 yum install -y yum-utils device-mapper-persistent-data lvm2 第四步：设置 yum 源 yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo 第五步：可以查看所有仓库中所有 docker 版本，并选择特定版本安装 yum list docker-ce --showduplicates | sort -r 第六步：安装 docker yum install docker-ce #由于 repo 中默认只开启 stable 仓库，故这里安装的是最新稳定版 18.06.1 或者指定 docker 版本安装： yum install &lt;FQPN&gt; # 例如：sudo yum install docker-ce-18.06.1.ce 第七步：启动并加入开机启动 systemctl start docker systemctl enable docker #centos7 已不用 chkconfig 了使用 systemctl list-unit-files 可以查看启动项 第八步：验证安装是否成功(有 client 和 service 两部分表示 docker 安装启动都成功了) docker version 第九步：添加国内镜像 添加国内 docker 镜像地址：vim /etc/docker/daemon.json {&quot;registry-mirrors&quot;: [&quot;https://registry.docker-cn.com&quot;,&quot;http://hub-mirror.c.163.com&quot;,&quot;https://docker.mirrors.ustc.edu.cn&quot;] } 重启 docker systemctl daemon-reload systemctl restart docker 第十步：启动 docker systemctl start docker 测试：sudo docker run hello-world docker 配置（按照正常的开发应用是 docker 控制有个专门的用户，为了学习方便我直接使用了 root 用户）： 创建 docker 用户组 sudo groupadd docker 增加当前用户到 docker 分组 sudo usermod -aG docker liming 验证在不使用 sudo 的情况下 docker 是否正常工作 docker run hello-world docker 卸载 查看安装包 yum list installed | grep docker 移除安装包： sudo yum -y remove docker-ce.x86_64 清除所有 docker 依赖文件 rm -rf /var/lib/docker 删除用户创建的配置文件 Docker 的核心概念Docker 核心概念之镜像 Docker 镜像镜像是创建 docker 容器的基础，docker 镜像类似于虚拟机镜像，可以将它理解为一个面向 docker 引擎的只读模块，包含文件系统。 例如：一个镜像可以包含一个完整的 centos 操作系统环境，里面仅安装了 Apache 或用户需要的其它应用程序。 镜像可以用来创建 Docker 容器。 创建 Docker 镜像有几种方式，多数是在一个现有镜像基础上创建新镜像，因为几乎你需要的任何东西都有了公共镜像，包括所有主流 Linux 发行版，你应该不会找不到你需要的镜像。不过，就算你想从头构建一个镜像，也有好几种方法。 创建镜像有三种方法： （1）基于已有镜像的容器创建。主要是利用 docker commit 命令。 （2）基于本地模板导入。推荐利用 OpenVZ 提供的模板来创建。 （3）基于 dockerfile 创建。首先按照 dockerfile 的格式，编写好 dockerfile 文件，之后通过 docker build 命令来创建镜像。docker build 会读取制定的 dockerfile，由 docker 服务器来创建镜像。 Docker 核心概念之容器 Docker 容器类似一个轻量级的沙箱，Docker 利用容器来运行和隔离应用。 容器是从镜像创建的运行实例。它可以被启动、开始、停止、删除。每个容器都是相互隔离的、保证安全的平台。 可以把容器看做是一个简易版的 Linux 环境（包括 root 用户权限、进程空间、用户空间和网络空间等）和运行在其中的应用程序。 注：镜像是本身是只读的，容器从镜像启动的时候，Docker 会在镜像的最上层创建一个可写层，镜像本身保持不变。 可以利用 docker create 命令创建一个容器，创建后的的容器处于停止状态，可以使用 docker start 命令来启动它。也可以直接利用 docker run 命令来直接从镜像启动运行一个容器。docker run = docker creat + docker start。 当利用 docker run 创建并启动一个容器时，docker 在后台的标准操作包括： （1）检查本地是否存在指定的镜像，不存在就从公有仓库下载。 （2）利用镜像创建并启动一个容器。 （3）分配一个文件系统，并在只读的镜像层外面挂载一层可读写层。 （4）从宿主机配置的网桥接口中桥接一个虚拟的接口到容器中。 （5）从地址池中配置一个 IP 地址给容器。 （6）执行用户指定的应用程序。 （7）执行完毕后容器终止。 Docker 核心概念之仓库 仓库是集中存放 Docker 镜像文件的地方。 有时候会把仓库和仓库注册服务器（Registry）混为一谈，并不严格区分。实际上，仓库注册服务器上往往存放着多个仓库，每个仓库中又包含了多个镜像，每个镜像有不同的标签（tag）。Centos 的 6.0 和 7.0 就是 tag。 仓库分为公有仓库和私有仓库，DockerHub 是目前最大的公有仓库。可以通过 docker push/pull 命令从仓库中上传和下载镜像，docker search 命令来搜索镜像。 Docker 的其他核心技术预览 Docker 核心是一个 操作系统级虚拟化 方法, 理解起来可能并不像 VM 那样直观。我们从虚拟化方法的四个方面：隔离性、可配额 / 可度量、便携性、安全性 来详细介绍 Docker 的技术细节。 LXC 介绍 1、Linux Container 容器是一种 内核虚拟化技术，可以提供轻量级的虚拟化，以便隔离进程和资源。 2、LXC 为 Linux Container的简写。可以提供轻量级的虚拟化，以便隔离进程和资源，而且不需要提供指令解释机制以及全虚拟化的其他复杂性。LXC 在资源管理方面依赖于 Linux 内核的 cgroups 子系统，LXC 在隔离控制方面依赖于 Linux 内核的 namespace 特性。容器有效地将由单个操作系统管理的资源划分到孤立的组中，以更好地在孤立的组之间平衡有冲突的资源使用需求。与传统虚拟化技术相比，它的优势在于： （1）与宿主机使用同一个内核，性能损耗小； （2）不需要指令级模拟； （3）不需要即时 (Just-in-time) 编译； （4）容器可以在 CPU 核心的本地运行指令，不需要任何专门的解释机制； （5）避免了准虚拟化和系统调用替换中的复杂性； （6）轻量级隔离，在隔离的同时还提供共享机制，以实现容器与宿主机的资源共享。 总结：Linux Container 是一种轻量级的虚拟化的手段。 3、Linux Container 提供了在单一可控主机节点上支持多个相互隔离的 server container 同时执行的机制。Linux Container 有点像 chroot，提供了一个拥有自己进程和网络空间的虚拟环境，但又有别于虚拟机，因为 lxc 是一种操作系统层次上的资源的虚拟化。 4、LXC 与 docker 的关系 （1）Docker 并不是 LXC 的替代品，Docker 的底层就是使用了 LXC 来实现的。LXC 将 Linux 进程沙盒化，使得进程之间相互隔离，并且能够控制各进程的资源分配。（2）在 LXC 的基础之上，Docker 提供了一系列更强的功能。 隔离性: Linux Namespace(ns)每个用户实例之间相互隔离, 互不影响。 一般的硬件虚拟化方法给出的方法是 VM，而 LXC 给出的方法是 container，更细一点讲就是 kernel namespace。其中 pid、net、ipc、mnt、uts、user 等 namespace 将 container 的进程、网络、消息、文件系统、UTS(“UNIX Time-sharing System”)和用户空间隔离开。 1) pid namespace 不同用户的进程就是通过 pid namespace 隔离开的，且不同 namespace 中可以有相同 pid。所有的 LXC 进程在 docker 中的父进程为 docker 进程，每个 lxc 进程具有不同的 namespace。同时由于允许嵌套，因此可以很方便的实现 Docker in Docker。 2) net namespace 有了 pid namespace, 每个 namespace 中的 pid 能够相互隔离，但是网络端口还是共享 host 的端口。网络隔离是通过 net namespace 实现的， 每个 net namespace 有独立的 network devices, IP addresses, IP routing tables, /proc/net 目录。这样每个 container 的网络就能隔离开来。docker 默认采用 veth 的方式将 container 中的虚拟网卡同 host 上的一个 docker bridge: docker0 连接在一起。 3) ipc namespace container 中进程交互还是采用 linux 常见的进程间交互方法(interprocess communication - IPC), 包括常见的信号量、消息队列和共享内存。然而同 VM 不同的是，container 的进程间交互实际上还是 host 上具有相同 pid namespace 中的进程间交互，因此需要在 IPC 资源申请时加入 namespace 信息 - 每个 IPC 资源有一个唯一的 32 位 ID。 4) mnt namespace 类似 chroot，将一个进程放到一个特定的目录执行。mnt namespace 允许不同 namespace 的进程看到的文件结构不同，这样每个 namespace 中的进程所看到的文件目录就被隔离开了。同 chroot 不同，每个 namespace 中的 container 在 /proc/mounts 的信息只包含所在 namespace 的 mount point。 5) uts namespace UTS(“UNIX Time-sharing System”) namespace 允许每个 container 拥有独立的 hostname 和 domain name, 使其在网络上可以被视作一个独立的节点而非 Host 上的一个进程。 6) user namespace 每个 container 可以有不同的 user 和 group id, 也就是说可以在 container 内部用 container 内部的用户执行程序而非 Host 上的用户。 可配额 / 可度量 - Control Groups (cgroups)cgroups 实现了对资源的配额和度量。 cgroups 的使用非常简单，提供类似文件的接口，在 /cgroup 目录下新建一个文件夹即可新建一个 group，在此文件夹中新建 task 文件，并将 pid 写入该文件，即可实现对该进程的资源控制。groups 可以限制 blkio、cpu、cpuacct、cpuset、devices、freezer、memory、net_cls、ns 九大子系统的资源，以下是每个子系统的详细说明： blkio 这个子系统设置限制每个块设备的输入输出控制。例如: 磁盘，光盘以及 usb 等等。 cpu 这个子系统使用调度程序为 cgroup 任务提供 cpu 的访问。 cpuacct 产生 cgroup 任务的 cpu 资源报告。 cpuset 如果是多核心的 cpu，这个子系统会为 cgroup 任务分配单独的 cpu 和内存。 devices 允许或拒绝 cgroup 任务对设备的访问。 freezer 暂停和恢复 cgroup 任务。 memory 设置每个 cgroup 的内存限制以及产生内存资源报告。 net_cls 标记每个网络包以供 cgroup 方便使用。 ns 名称空间子系统。 以上九个子系统之间也存在着一定的关系. 详情请参阅 官方文档。 便携性: AUFSAUFS (AnotherUnionFS) 是一种 Union FS, 简单来说就是支持将不同目录挂载到同一个虚拟文件系统下 (unite several directories into a single virtual filesystem) 的文件系统, 更进一步的理解, AUFS 支持为每一个成员目录 (类似 Git Branch) 设定 readonly、readwrite 和 whiteout-able 权限, 同时 AUFS 里有一个类似分层的概念, 对 readonly 权限的 branch 可以逻辑上进行修改(增量地, 不影响 readonly 部分的)。通常 Union FS 有两个用途, 一方面可以实现不借助 LVM、RAID 将多个 disk 挂到同一个目录下, 另一个更常用的就是将一个 readonly 的 branch 和一个 writeable 的 branch 联合在一起，Live CD 正是基于此方法可以允许在 OS image 不变的基础上允许用户在其上进行一些写操作。Docker 在 AUFS 上构建的 container image 也正是如此，接下来我们从启动 container 中的 linux 为例来介绍 docker 对 AUFS 特性的运用。 典型的启动 Linux 运行需要两个 FS: bootfs + rootfs: bootfs (boot file system) 主要包含 bootloader 和 kernel, bootloader 主要是引导加载 kernel, 当 boot 成功后 kernel 被加载到内存中后 bootfs 就被 umount 了. rootfs (root file system) 包含的就是典型 Linux 系统中的 /dev, /proc,/bin, /etc 等标准目录和文件。 对于不同的 linux 发行版, bootfs 基本是一致的, 但 rootfs 会有差别, 因此不同的发行版可以公用 bootfs 如下图: 典型的 Linux 在启动后，首先将 rootfs 设置为 readonly, 进行一系列检查, 然后将其切换为 “readwrite” 供用户使用。在 Docker 中，初始化时也是将 rootfs 以 readonly 方式加载并检查，然而接下来利用 union mount 的方式将一个 readwrite 文件系统挂载在 readonly 的 rootfs 之上，并且允许再次将下层的 FS(file system) 设定为 readonly 并且向上叠加, 这样一组 readonly 和一个 writeable 的结构构成一个 container 的运行时态, 每一个 FS 被称作一个 FS 层。如下图: 得益于 AUFS 的特性, 每一个对 readonly 层文件 / 目录的修改都只会存在于上层的 writeable 层中。这样由于不存在竞争, 多个 container 可以共享 readonly 的 FS 层。 所以 Docker 将 readonly 的 FS 层称作 “image” - 对于 container 而言整个 rootfs 都是 read-write 的，但事实上所有的修改都写入最上层的 writeable 层中, image 不保存用户状态，只用于模板、新建和复制使用。 上层的 image 依赖下层的 image，因此 Docker 中把下层的 image 称作父 image，没有父 image 的 image 称作 base image。因此想要从一个 image 启动一个 container，Docker 会先加载这个 image 和依赖的父 images 以及 base image，用户的进程运行在 writeable 的 layer 中。所有 parent image 中的数据信息以及 ID、网络和 lxc 管理的资源限制等具体 container 的配置，构成一个 Docker 概念上的 container。如下图: 安全性: AppArmor, SELinux, GRSEC安全永远是相对的，这里有三个方面可以考虑 Docker 的安全特性: 由 kernel namespaces 和 cgroups 实现的 Linux 系统固有的安全标准;Docker Deamon 的安全接口;Linux 本身的安全加固解决方案, 类如 AppArmor, SELinux;由于安全属于非常具体的技术，这里不在赘述，请直接参阅Docker 官方文档。]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一起学 Docker（一）-- 初识]]></title>
    <url>%2F2018%2F05%2F19%2F%E4%B8%80%E8%B5%B7%E5%AD%A6Docker%EF%BC%88%E4%B8%80%EF%BC%89--%E5%88%9D%E8%AF%86%2F</url>
    <content type="text"><![CDATA[什么是 dockerDocker 是一个开源项目，诞生于 2013 年初，最初是 dotCloud 公司内部的一个业余项目。它基于 Google 公司推出的 Go 语言实现。 项目后来加入了 Linux 基金会，遵从了 Apache 2.0 协议，项目代码在 GitHub 上进行维护。 Docker 自开源后受到广泛的关注和讨论，以至于 dotCloud 公司后来都改名为 Docker Inc。Redhat 已经在其 RHEL6.5 中集中支持 Docker；Google 也在其 PaaS 产品中广泛应用。 Docker 是一个程序运行、测试、交付的开放平台，Docker 被设计为能够使你快速地交付应用。 在 Docker 中，你可以将你的程序分为不同的基础部分，对于每一个基础部分都可以当做一个应用程序来管理。 Docker 能够帮助你快速地测试、快速地编码、快速地交付，并且缩短你从编码到运行应用的周期。 Docker 使用轻量级的容器虚拟化平台，并且结合工作流和工具，来帮助你管理、部署你的应用程序。 Docker 在其核心，Docker 实现了让几乎任何程序都可以在一个安全、隔离的容器中运行。安全和隔离可以使你可以同时在机器上运行多个容器。Docker 容器轻量级的特性，意味着你可以得到更多的硬件性能。 围绕着 Docker 容器的虚拟化工具和平台，可以在以下几个方面为你提供帮助： 1）帮助你把应用程序 (包括其余的支持组件) 放入到 Docker 容器中。 2）分发和转移你的容器至你的团队其它成员来进行进一步的开发和测试。 3）部署这些应用程序至你的生产环境，不论是本地的数据中心还是云平台。 Docker 的用途1）快速交付你的应用程序 Docker 可以为你的开发过程提供完美的帮助。Docker 允许开发者在本地包含了应用程序和服务的容器进行开发，之后可以集成到连续的一体化和部署工作流中。 举个例子，开发者们在本地编写代码并且使用 Docker 和同事分享其开发栈。当开发者们准备好了之后，他们可以将代码和开发栈推送到测试环境中，在该环境进行一切所需要的测试。从测试环境中，你可以将 Docker 镜像推送到服务器上进行部署。 2）开发和拓展更加简单 Docker 的以容器为基础的平台允许高度可移植的工作。Docker 容器可以在开发者机器上运行，也可以在实体或者虚拟机上运行，也可以在云平台上运行。 Docker 的可移植、轻量特性同样让动态地管理负载更加简单。你可以用 Docker 快速地增加应用规模或者关闭应用程序和服务。Docker 的快速意味着变动几乎是实时的。 3）达到高密度和更多负载 Docker 轻巧快速，它提供了一个可行的、符合成本效益的替代基于虚拟机管理程序的虚拟机。这在高密度的环境下尤其有用。例如，构建你自己的云平台或者 PaaS，在中小的部署环境下同样可以获取到更多的资源性能。 Docker 的主要组成Docker 有两个主要的部件： Docker: 开源的容器虚拟化平台。 Docker Hub: 用于分享、管理 Docker 容器的 Docker SaaS 平台。 Docker 的架构 Docker 使用客户端 - 服务器(client-server) 架构模式。 Docker 客户端会与 Docker 守护进程进行通信。Docker 守护进程会处理复杂繁重的任务，例如建立、运行、发布你的 Docker 容器。 Docker 客户端和守护进程可以运行在同一个系统上，当然也可以使用 Docker 客户端去连接一个远程的 Docker 守护进程。 Docker 客户端和守护进程之间通过 socket 或者 RESTful API 进行通信。 1）Docker 守护进程 如上图所示，Docker 守护进程运行在一台主机上。用户并不直接和守护进程进行交互，而是通过 Docker 客户端间接和其通信。 2）Docker 客户端 Docker 客户端，实际上是 docker 的二进制程序，是主要的用户与 Docker 交互方式。它接收用户指令并且与背后的 Docker 守护进程通信，如此来回往复。 3）Docker 内部 要理解 Docker 内部构建，需要理解以下三种部件： Docker 镜像 - Docker images Docker 仓库 - Docker registeries Docker 容器 - Docker containers Docker 镜像 Docker 镜像是 Docker 容器运行时的只读模板，每一个镜像由一系列的层 (layers) 组成。Docker 使用 UnionFS 来将这些层联合到单独的镜像中。UnionFS 允许独立文件系统中的文件和文件夹 (称之为分支) 被透明覆盖，形成一个单独连贯的文件系统。正因为有了这些层的存在，Docker 是如此的轻量。当你改变了一个 Docker 镜像，比如升级到某个程序到新的版本，一个新的层会被创建。因此，不用替换整个原先的镜像或者重新建立(在使用虚拟机的时候你可能会这么做)，只是一个新 的层被添加或升级了。现在你不用重新发布整个镜像，只需要升级，层使得分发 Docker 镜像变得简单和快速。 Docker 仓库 Docker 仓库用来保存镜像，可以理解为代码控制中的代码仓库。同样的，Docker 仓库也有公有和私有的概念。公有的 Docker 仓库名字是 Docker Hub。Docker Hub 提供了庞大的镜像集合供使用。这些镜像可以是自己创建，或者在别人的镜像基础上创建。Docker 仓库是 Docker 的分发部分。 Docker 容器 Docker 容器和文件夹很类似，一个 Docker 容器包含了所有的某个应用运行所需要的环境。每一个 Docker 容器都是从 Docker 镜像创建的。Docker 容器可以运行、开始、停止、移动和删除。每一个 Docker 容器都是独立和安全的应用平台，Docker 容器是 Docker 的运行部分。 4）libcontainerDocker 从 0.9 版本开始使用 libcontainer 替代 lxc，libcontainer 和 Linux 系统的交互图如下： 5）命名空间「Namespaces」 1）pid namespace 不同用户的进程就是通过 pid namespace 隔离开的，且不同 namespace 中可以有相同 PID。 具有以下特征: 每个 namespace 中的 pid 是有自己的 pid=1 的进程(类似 /sbin/init 进程) 每个 namespace 中的进程只能影响自己的同一个 namespace 或子 namespace 中的进程 因为 /proc 包含正在运行的进程，因此在 container 中的 pseudo-filesystem 的 /proc 目录只能看到自己 namespace 中的进程 因为 namespace 允许嵌套，父 namespace 可以影响子 namespace 的进程，所以子 namespace 的进程可以在父 namespace 中看到，但是具有不同的 pid 2）mnt namespace 类似 chroot，将一个进程放到一个特定的目录执行。mnt namespace 允许不同 namespace 的进程看到的文件结构不同，这样每个 namespace 中的进程所看到的文件目录就被隔离开了。同 chroot 不同，每个 namespace 中的 container 在 /proc/mounts 的信息只包含所在 namespace 的 mount point。 3）net namespace 网络隔离是通过 net namespace 实现的， 每个 net namespace 有独立的 network devices, IP addresses, IP routing tables, /proc/net 目录。这样每个 container 的网络就能隔离开来。 docker 默认采用 veth 的方式将 container 中的虚拟网卡同 host 上的一个 docker bridge 连接在一起。 4）uts namespace UTS (&quot;UNIX Time-sharing System&quot;) namespace 允许每个 container 拥有独立的 hostname 和 domain name, 使其在网络上可以被视作一个独立的节点而非 Host 上的一个进程。 5）ipc namespace container 中进程交互还是采用 Linux 常见的进程间交互方法 (interprocess communication - IPC), 包括常见的信号量、消息队列和共享内存。然而同 VM 不同，container 的进程间交互实际上还是 host 上具有相同 pid namespace 中的进程间交互，因此需要在 IPC 资源申请时加入 namespace 信息 - 每个 IPC 资源有一个唯一的 32bit ID。 6）user namespace 每个 container 可以有不同的 user 和 group id, 也就是说可以以 container 内部的用户在 container 内部执行程序而非 Host 上的用户。 有了以上 6 种 namespace 从进程、网络、IPC、文件系统、UTS 和用户角度的隔离，一个 container 就可以对外展现出一个独立计算机的能力，并且不同 container 从 OS 层面实现了隔离。然而不同 namespace 之间资源还是相互竞争的，仍然需要类似 ulimit 来管理每个 container 所能使用的资源。 6）资源配额「cgroups」 cgroups 实现了对资源的配额和度量。 cgroups 的使用非常简单，提供类似文件的接口，在 /cgroup 目录下新建一个文件夹即可新建一个 group，在此文件夹中新建 task 文件，并将 pid 写入该文件，即可实现对该进程的资源控制。具体的资源配置选项可以在该文件夹中新建子 subsystem ，{子系统前缀}.{资源项} 是典型的配置方法， 如 memory.usageinbytes 就定义了该 group 在 subsystem memory 中的一个内存限制选项。 另外，cgroups 中的 subsystem 可以随意组合，一个 subsystem 可以在不同的 group 中，也可以一个 group 包含多个 subsystem - 也就是说一个 subsystem。 memory 内存相关的限制 cpu 在 cgroup 中，并不能像硬件虚拟化方案一样能够定义 CPU 能力，但是能够定义 CPU 轮转的优先级，因此具有较高 CPU 优先级的进程会更可能得到 CPU 运算。 通过将参数写入 cpu.shares , 即可定义改 cgroup 的 CPU 优先级 - 这里是一个相对权重，而非绝对值 blkio block IO 相关的统计和限制，byte/operation 统计和限制 (IOPS 等)，读写速度限制等，但是这里主要统计的都是同步 IO devices 设备权限限制 Docker 的工作原理1）可以建立一个容纳应用程序的容器。2）可以从 Docker 镜像创建 Docker 容器来运行应用程序。3）可以通过 Docker Hub 或者自己的 Docker 仓库分享 Docker 镜像。 Docker 镜像是如何工作的？Docker 镜像是 Docker 容器运行时的只读模板，每一个镜像由一系列的层 (layers) 组成； Docker 使用 UnionFS(联合文件系统)来将这些层联合到一二镜像中，UnionFS 文件系统允许独立文件系统中的文件和文件夹 (称之为分支) 被透明覆盖，形成一个单独连贯的文件系统。 正因为有了这些层 (layers) 的存在，Docker 才会如此的轻量。当你改变了一个 Docker 镜像，比如升级到某个程序到新的版本，一个新的层会被创建。因此，不用替换整个原先的镜像或者重新建立(在使用虚拟机的时候你可能会这么做)，只是一个新的层被添加或升级了。所以你不用重新发布整个镜像，只需要升级。层使得分发 Docker 镜像变得简单和快速。 每个镜像都是从一个基础的镜像开始的，比如 ubuntu，一个基础的 Ubuntu 镜像，或者是 Centos，一个基础的 Centos 镜像。你可以使用你自己的镜像作为新镜像的基础，例如你有一个基础的安装了 Nginx 的镜像，你可以使用该镜像来建立你的 Web 应用程序镜像。（Docker 通常从 Docker Hub 获取基础镜像） Docker 镜像从这些基础的镜像创建，通过一种简单、具有描述性的步骤，我们称之为 指令(instructions)。 每一个指令会在镜像中创建一个新的层，指令可以包含这些动作： 1）运行一个命令。 2）增加文件或者文件夹。 3）创建一个环境变量。 5）当运行容器的时候哪些程序会运行。 这些指令存储在 Dockerfile 文件中。当你需要建立镜像的时候，Docker 可以从 Dockerfile 中读取这些指令并且运行，然后返回一个最终的镜像。 Docker 仓库是如何工作的？Docker 仓库是 Docker 镜像的存储仓库。可以推送镜像到 Docker 仓库中, 然后在 Docker 客户端，可以从 Docker 仓库中搜索镜像。 Docker 容器是如何工作的？一个 Docker 容器包含了一个操作系统、用户添加的文件和元数据(meta-data)。每个容器都是从镜像建立的，镜像告诉 Docker 容器内包含了什么，当容器启动时运行什么程序，还有许多配置数据。 Docker 镜像是只读的，当 Docker 运行一个从镜像建立的容器，它会在镜像顶部添加一个可读写的层，应用程序可以在这里运行。 ### 当运行 docker 容器时发生了什么？ 使用 docker 命令时，Docker 客户端都告诉 Docker 守护进程运行一个容器。 $ sudo docker run -i -t ubuntu /bin/bash 可以来分析这个命令，Docker 客户端使用 docker 命令来运行，run 参数表明客户端要运行一个新的容器。 Docker 客户端要运行一个容器需要告诉 Docker 守护进程的最小参数信息是： 1）这个容器从哪个镜像创建，这里是 ubuntu，基础的 Ubuntu 镜像。 2）在容器中要运行的命令，这里是 /bin/bash，在容器中运行 Bash shell。 那么运行这个命令之后在底层发生了什么呢？ 按照顺序，Docker 做了这些事情： 1）拉取 ubuntu 镜像: Docker 检查 ubuntu 镜像是否存在，如果在本地没有该镜像，Docker 会从 Docker Hub 下载。如果镜像已经存在，Docker 会使用它来创建新的容器。 2）创建新的容器: 当 Docker 有了这个镜像之后，Docker 会用它来创建一个新的容器。 3）分配文件系统并且挂载一个可读写的层: 容器会在这个文件系统中创建，并且一个可读写的层被添加到镜像中。 4）分配网络 / 桥接接口: 创建一个允许容器与本地主机通信的网络接口。 5）设置一个 IP 地址: 从池中寻找一个可用的 IP 地址并且服加到容器上。 6）运行你指定的程序: 运行指定的程序。 7）捕获并且提供应用输出: 连接并且记录标准输出、输入和错误让你可以看到你的程序是如何运行的。 由此你就可以拥有一个运行着的 Docker 容器了！从这里开始你可以管理你的容器，与应用交互，应用完成之后，可以停止或者删除你的容器。 Docker 的底层技术Docker 使用 Go 语言编写，并且使用了一系列 Linux 内核提供的性能来实现我们已经看到的这些功能。 命名空间(Namespaces)（这个上面也详细说明了） Docker 充分利用了一项称为 namespaces 的技术来提供隔离的工作空间，我们称之为 container(容器)。当你运行一个容器的时候，Docker 为该容器创建了一个命名空间集合。 这样提供了一个隔离层，每一个应用在它们自己的命名空间中运行而且不会访问到命名空间之外。 一些 Docker 使用到的命名空间有： pid 命名空间: 使用在进程隔离(PID: Process ID)。 net 命名空间: 使用在管理网络接口(NET: Networking)。 ipc 命名空间: 使用在管理进程间通信资源 (IPC: InterProcess Communication)。 mnt 命名空间: 使用在管理挂载点 (MNT: Mount)。 uts 命名空间: 使用在隔离内核和版本标识 (UTS: Unix Timesharing System)。 群组控制 Docker 还使用到了 cgroups 技术来管理群组。使应用隔离运行的关键是让它们只使用你想要的资源。这样可以确保在机器上运行的容器都是良民(good multi-tenant citizens)。群组控制允许 Docker 分享或者限制容器使用硬件资源。例如，限制指定的容器的内容使用。 联合文件系统 联合文件系统 (UnionFS) 是用来操作创建层的，使它们轻巧快速。Docker 使用 UnionFS 提供容器的构造块。Docker 可以使用很多种类的 UnionFS 包括 AUFS, btrfs, vfs, and DeviceMapper。 容器格式 Docker 连接这些组建到一个包装中，称为一个 container format(容器格式)。默认的容器格式是 libcontainer。Docker 同样支持传统的 Linux 容器使用 LXC。在未来，Docker 也许会支持其它的容器格式，例如与 BSD Jails 或 Solaris Zone 集成。 Docker 的网络配置 Dokcer 通过使用 Linux 桥接提供容器之间的通信，docker0 桥接接口的目的就是方便 Docker 管理。当 Docker daemon 启动时需要做以下操作： 1）如果 docker0 不存在则创建 2）搜索一个与当前路由不冲突的 ip 段 3）在确定的范围中选择 ip 4）绑定 ip 到 docker0 Docker 四种网络模式docker run 创建 Docker 容器时，可以用–net 选项指定容器的网络模式，Docker 有以下 4 种网络模式： 1）host 模式，使用 –net=host 指定。 2）container 模式，使用 –net=container:NAMEorID 指定。 3）none 模式，使用 –net=none 指定。 4）bridge 模式，使用 –net=bridge 指定，默认设置。 host 模式 如果启动容器的时候使用 host 模式，那么这个容器将不会获得一个独立的 Network Namespace，而是和宿主机共用一个 Network Namespace。容器将不会虚拟出自己的网卡，配置自己的 IP 等，而是使用宿主机的 IP 和端口。 比如：我们在 10.10.101.105/24 的机器上用 host 模式启动一个含有 web 应用的 Docker 容器，监听 tcp 80 端口。当我们在容器中执行任何类似 ifconfig 命令查看网络环境时，看到的都是宿主机上的信息。而外界访问容器中的应用，则直接使用 10.10.101.105:80 即可，不用任何 NAT 转换，就如直接跑在宿主机中一样。但是，容器的其他方面，如文件系统、进程列表等还是和宿主机隔离的。 container 模式 这个模式指定新创建的容器和已经存在的一个容器共享一个 Network Namespace，而不是和宿主机共享。新创建的容器不会创建自己的网卡，配置自己的 IP，而是和一个指定的容器共享 IP、端口范围等。同样，两个容器除了网络方面，其他的如文件系统、进程列表等还是隔离的。两个容器的进程可以通过 lo 网卡设备通信。 none 模式 这个模式和前两个不同。在这种模式下，Docker 容器拥有自己的 Network Namespace，但是，并不为 Docker 容器进行任何网络配置。也就是说，这个 Docker 容器没有网卡、IP、路由等信息。需要我们自己为 Docker 容器添加网卡、配置 IP 等。 bridge 模式 bridge 模式是 Docker 默认的网络设置，此模式会为每一个容器分配 Network Namespace、设置 IP 等，并将一个主机上的 Docker 容器连接到一个虚拟网桥上。当 Docker server 启动时，会在主机上创建一个名为 docker0 的虚拟网桥，此主机上启动的 Docker 容器会连接到这个虚拟网桥上。虚拟网桥的工作方式和物理交换机类似，这样主机上的所有容器就通过交换机连在了一个二层网络中。接下来就要为容器分配 IP 了，Docker 会从 RFC1918 所定义的私有 IP 网段中，选择一个和宿主机不同的 IP 地址和子网分配给 docker0，连接到 docker0 的容器就从这个子网中选择一个未占用的 IP 使用。如一般 Docker 会使用 172.17.0.0/16 这个网段，并将 172.17.42.1/16 分配给 docker0 网桥（在主机上使用 ifconfig 命令是可以看到 docker0 的，可以认为它是网桥的管理接口，在宿主机上作为一块虚拟网卡使用） 列出当前主机网桥[root@localhost ~]# brctl show//brctl 工具依赖 bridge-utils 软件包 bridge name bridge id STP enabled interfaces docker0 8000.024223421c41 no veth4b4f1b1 查看当前 docker0 ip[root@localhost ~]# ifconfig docker0 docker0: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt; mtu 1500 inet 172.17.0.1 netmask 255.255.0.0 broadcast 0.0.0.0 inet6 fe80::42:23ff:fe42:1c41 prefixlen 64 scopeid 0x20&lt;link&gt; ether 02:42:23:42:1c:41 txqueuelen 0 (Ethernet) RX packets 181704 bytes 9952837 (9.4 MiB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 286903 bytes 598331176 (570.6 MiB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 在容器运行时，每个容器都会分配一个特定的虚拟机口并桥接到 docker0。每个容器都会配置同 docker0 ip 相同网段的专用 ip 地址，docker0 的 IP 地址被用于所有容器的默认网关。 运行一个容器[root@localhost ~]# docker images REPOSITORY TAG IMAGE IDCREATED SIZE docker.io/ubuntulatest 0ef2e08ed3fa2 weeks ago 130 MB [root@localhost ~]# docker run -t -i -d docker.io/ubuntu /bin/bash bad5133ecd6a4c740d57621c8fdb536dd9512718a3cae371789b8063ec730944 // 这个字符串的前 12 位字符就是容器的 ID [root@localhost ~]# docker ps CONTAINER IDIMAGE COMMAND CREATED STATUS PORTS NAMES bad5133ecd6adocker.io/ubuntu&quot;/bin/bash&quot; 33 seconds ago Up 32 seconds suspicious_rosalind [root@localhost ~]# brctl show bridge namebridge idSTP enabled interfaces docker08000.024223421c41noveth4b4f1b1 vetheaa4add 以上, docker0 扮演着 bad5133ecd6a 这个容器的虚拟接口 vetheaa4add interface 桥接的角色。 使用特定范围的 IP Docker 会尝试寻找没有被主机使用的 ip 段，尽管它适用于大多数情况下，但是它不是万能的，有时候我们还是需要对 ip 进一步规划。Docker 允许你管理 docker0 桥接或者通过 -b 选项自定义桥接网卡，需要安装 bridge-utils 软件包。 基本步骤如下： 1）确保 docker 的进程是停止的 2）创建自定义网桥 3）给网桥分配特定的 ip 4）以 -b 的方式指定网桥 具体操作如下： $ sudo service docker stop $ sudo ip link set dev docker0 down $ sudo brctl delbr docker0 $ sudo brctl addbr bridge0 $ sudo ip addr add 192.168.5.1/24 dev bridge0 $ sudo ip link set dev bridge0 up $ ip addr show bridge0 $ echo &apos;DOCKER_OPTS=&quot;-b=bridge0&quot;&apos; &gt;&gt; /etc/default/docker $ sudo service docker start 不同主机间容器通信 不同容器之间的通信可以借助于 pipework 这个工具： $ git clone https://github.com/jpetazzo/pipework.git $ sudo cp -rp pipework/pipework /usr/local/bin/ 安装相应依赖软件 $ sudo apt-get install iputils-arping bridge-utils -y 桥接网络 # brctl show bridge name bridge id STP enabled interfaces br0 8000.000c291412cd no eth0 docker0 8000.56847afe9799 no vetheb48029 可以删除 docker0，直接把 docker 的桥接指定为 br0。也可以保留使用默认的配置，这样单主机容器之间的通信可以通过 docker0，而跨主机不同容器之间通过 pipework 新建 docker 容器的网卡桥接到 br0，这样跨主机容器之间就可以通信了。 具体操作如下： 1）ubuntu 操作： $ sudo service docker stop $ sudo ip link set dev docker0 down $ sudo brctl delbr docker0 $ echo &apos;DOCKER_OPTS=&quot;-b=br0&quot;&apos; &gt;&gt; /etc/default/docker $ sudo service docker start 2）CentOS 7 操作 $ sudo systemctl stop docker $ sudo ip link set dev docker0 down $ sudo brctl delbr docker0 $ cat /etc/sysconfig/docker | grep &apos;OPTIONS=&apos; OPTIONS=--selinux-enabled -b=br0 -H fd:// $ sudo systemctl start docker pipework 不同容器之间的通信可以借助于 pipework 这个工具给 docker 容器新建虚拟网卡并绑定 IP 桥接到 br0 $ git clone https://github.com/jpetazzo/pipework.git $ sudo cp -rp pipework/pipework /usr/local/bin/ $ pipework Syntax: pipework &lt;hostinterface&gt; [-i containerinterface] &lt;guest&gt; &lt;ipaddr&gt;/&lt;subnet&gt;[@default_gateway] [macaddr][@vlan] pipework &lt;hostinterface&gt; [-i containerinterface] &lt;guest&gt; dhcp [macaddr][@vlan] pipework --wait [-i containerinterface] 如果删除了默认的 docker0 桥接，把 docker 默认桥接指定到了 br0，则最好在创建容器的时候加上–net=none，防止自动分配的 IP 在局域网中有冲突。 $ sudo docker run --rm -ti --net=none ubuntu:14.04 /bin/bash root@a46657528059:/# $ # Ctrl-P + Ctrl-Q 回到宿主机 shell，容器 detach 状态 $ sudo docker ps CONTAINER IDIMAGE COMMAND CREATED STATUS PORTS NAMES a46657528059ubuntu:14.04 &quot;/bin/bash&quot; 4 minutes ago Up 4 minutes hungry_lalande $ sudo pipework br0 -i eth0 a46657528059 192.168.115.10/24@192.168.115.2 # 默认不指定网卡设备名，则默认添加为 eth1 # 另外 pipework 不能添加静态路由，如果有需求则可以在 run 的时候加上 --privileged=true 权限在容器中手动添加， # 但这种安全性有缺陷，可以通过 ip netns 操作 $ sudo docker attach a46657528059 root@a46657528059:/# ifconfig eth0 eth0 Link encap:Ethernet HWaddr 86:b6:6b:e8:2e:4d inet addr:192.168.115.10 Bcast:0.0.0.0 Mask:255.255.255.0 inet6 addr: fe80::84b6:6bff:fee8:2e4d/64 Scope:Link UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 RX packets:8 errors:0 dropped:0 overruns:0 frame:0 TX packets:9 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1000 RX bytes:648 (648.0 B) TX bytes:690 (690.0 B) root@a46657528059:/# route -n Kernel IP routing table Destination Gateway Genmask Flags Metric RefUse Iface 0.0.0.0 192.168.115.2 0.0.0.0 UG0 00 eth0 192.168.115.0 0.0.0.0 255.255.255.0 U 0 00 eth0 使用 ip netns 添加静态路由，避免创建容器使用–privileged=true 选项造成一些不必要的安全问题： $ docker inspect --format=&quot;{{.State.Pid}}&quot; a46657528059 # 获取指定容器 pid 6350 $ sudo ln -s /proc/6350/ns/net /var/run/netns/6350 $ sudo ip netns exec 6350 ip route add 192.168.0.0/16 dev eth0 via 192.168.115.2 $ sudo ip netns exec 6350 ip route# 添加成功 192.168.0.0/16 via 192.168.115.2 dev eth0 ... ... 在其它宿主机进行相应的配置，新建容器并使用 pipework 添加虚拟网卡桥接到 br0，测试通信情况即可。 另外，pipework 可以创建容器的 vlan 网络，这里不作过多的介绍了，官方文档已经写的很清楚了，可以查看以下两篇文章： Pipework 官方文档 Docker 网络详解及 pipework 源码解读与实践 为什么使用 dockerDocker 容器虚拟化的优点 环境隔离； 通过 cgroups 和 namesapce 进行实现资源隔离，实现一台机器运行多个容器互不影响。 更快速的交付部署； 使用 docker，开发人员可以利用镜像快速构建一套标准的研发环境；开发完成后，测试和运维人员可以直接通过使用相同的环境来部署代码。Docker 可以快速创建和删除容器，实现快速迭代，大量节约开发、测试、部署的时间。并且，各个步骤都有明确的配置和操作，整个过程全程课件，使团队里更容易理解应用创建和工作的过程。 更高效的资源利用； docker 容器的运行不需要额外的虚拟化管理程序的支持，它是内核级的虚拟化，可以实现更高的性能，同时对资源的额外需求很低。 更易迁移扩展； docker 容器几乎可以在任意的平台上运行，包括乌力吉、虚拟机、公有云、私有云、个人电脑、服务器等，这种兼容性让用户可以在不同平台之间轻松的迁移应用。 更简单的更新管理。 使用 Dockerfile，只需要小小的配置修改，就可以替代以往的大量的更新工作。并且所有修改都是以增量的方式进行分发和更新，从而实现自动化和高效的容器管理。 虚拟化与 docker虚拟化定义：虚拟化是一种资源管理技术，是将计算机的各种实体资源，如服务器、网络、内存及存储等，予以抽象、转换后呈现出来，打破实体结构间的不可切割的障碍，使用户可以比原本的配置更好的方式来应用这些资源。这些资源的新虚拟部分是不受现有资源的架设方式，地域或物理配置所限制。一般所指的虚拟化资源包括计算能力和数据存储。 系统虚拟化，Hypervisor Virtualization，全虚拟化。在 Host 中通过 Hypervisor 层实现安装多个 GuestOS，每个 GuestOS 都有自己的内核，和主机的内核不同，GuestOS 之间完全隔离。 容器虚拟化，Operating System Virtualization ，使用 Linux 内核中的 namespaces 和 cgroups 实现进程组之间的隔离。是用内核技术实现的隔离，所以它是一个共享内核的虚拟化技术。 容器虚拟化没有 GuestOS，使用 Docker 时下载的镜像，只是为运行 App 提供的一个依赖的环境，是一个删减版本的系统镜像。一般情况下系统虚拟化没有容器虚拟化的运行效率高，但是系统安全性高很多。 优越性： 你在一台机器可以开 10 个虚拟机，如果用 docker 可以开 100 个容器，就是这么霸气 docker 官网注册 注册一个 docker 账号：https://hub.docker.com/ 完成注册，等待一会就能收到验证邮件。 收到后激活邮箱，进行登录 登录完毕 Docker 资源Docker 官方英文资源： docker 官网 Docker windows 入门 Docker Linux 入门 Docker mac 入门 Docker 用户指引 Docker 官方博客 Docker Hub Docker 开源 Docker 中文资源： Docker 中文网站 Docker 入门教程 Docker 安装手册 一小时 Docker 教程 docker 从入门到实践 Docker 纸质书 Docker PPT 转自：跟我一起学 docker(一)–认识]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[jenkins 实例似乎已离线问题]]></title>
    <url>%2F2018%2F05%2F14%2Fjenkins%E5%AE%9E%E4%BE%8B%E4%BC%BC%E4%B9%8E%E5%B7%B2%E7%A6%BB%E7%BA%BF%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[很久没有安装 jenkins 了，因为之前用的的服务器一直正常使用，令人郁闷的是，之前用 jenkins 一直没出过这个问题。 令人更郁闷的是，我尝试了好多个历史版本和最新版本，甚至从之前的服务器把 jenkins 在跑的程序打包 copy 这个服务器。终究还是不行。 启动时候，提示：该 jenkins 实例似乎已离线 可以说是非常坑！！！！！！！！！！！！！！！！！！！！！！！！！！ 虽然可以离线安装，但是对于博主来说不解决怎么行呢？经过一番踩坑与资料查找终于解决了，这里与大家分享一下： 问题如图下所示： 解决上述问题方法： 1) 修改 /var/lib/jenkins/updates/default.json jenkins 在下载插件之前会先检查网络连接，其会读取这个文件中的网址。默认是 访问谷歌，这就很坑了，服务器网络又不能 FQ，肯定监测失败呀，不得不说 jenkins 的开发者脑子锈了，所以将图下的 google 改为www.baidu.com 即可，更改完重启服务。 2) 修改 /var/lib/jenkins/hudson.model.UpdateCenter.xml 该文件为 jenkins 下载插件的源地址，改地址默认 jenkins 默认为：https://updates.jenkins.io/update-center.json，就是因为 https 的问题，此处我们将其改为 http 即可，之后重启 jenkins 服务即可。 其他国内备用地址（也可以选择使用）： https://mirrors.tuna.tsinghua.edu.cn/jenkins/updates/update-center.json http://mirror.esuni.jp/jenkins/updates/update-center.json 在修复完之后，我们发现离线问题已经解决，如图下所示： 这样我们就可以愉快地安装插件了。]]></content>
      <categories>
        <category>Jenkins</category>
      </categories>
      <tags>
        <tag>Jenkins</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo 使用 jenkins 自动部署到阿里云]]></title>
    <url>%2F2018%2F05%2F14%2FHexo%E4%BD%BF%E7%94%A8jenkins%E8%87%AA%E5%8A%A8%E9%83%A8%E7%BD%B2%E5%88%B0%E9%98%BF%E9%87%8C%E4%BA%91%2F</url>
    <content type="text"><![CDATA[## 摘要 本文基于本地已搭建好了 hexo+github pages 的环境下使用 Jenkins 构建持续集成到自己的阿里云服务器上，实现本地发布博客后同时部署到 GitHub 和个人的阿里云服务器上。 虽说利用 github pages 服务能够对外发布博客，但是作为一个码农还是希望有自己的域名博客，但是我比较懒，不想手动发布博客。我想自动化地既发布到 github 也能同时发布到个人网站。所以决定采用 CICD 的方法，CICD 工具使用开源的 jenkins，jenkins 也搭建在阿里云个人服务器上。 其他的 CI 方法： 使用 Git Hook 自动部署 Hexo 到个人 VPS 配合 TRAVIS CI，将 HEXO 博客自动部署到你的服务器上 注：个人觉得使用 Travis 不太方便，还是 Jenkins 的 WEB 页面配置来得直观，纯属个人观点 下载并运行 jenkins注意端口使用的是默认 8080 wget -O /etc/yum.repos.d/jenkins.repo http://pkg.jenkins-ci.org/redhat-stable/jenkins.repo rpm --import https://jenkins-ci.org/redhat/jenkins-ci.org.key yum install jenkins 安装完成之后运行如下命令启动 systemctl start jenkins git 环境搭建 git 安装 参考资料： 廖雪峰老师的 git 教程 git 官网下载 生成 ssh 认证，执行如下命令 参考资料： Git 提交时报错 warning: LF will be replaced by CRLF in git config --global user.name &quot;yourname&quot; git config --global user.email youremail@example.com ssh-keygen -t rsa -C &quot;youremail@example.com&quot; git config --global core.autocrlf false // 禁用自动转换，这个不设置后面上传时会出现警告，如下 最后获取到的 ssh 认证在 C:\Users\yourname\.ssh 中 服务器端配置 搭建远程 Git 私库 1. 登录到远程服务器，建议使用Xshell 5 2. 安装 git，执行如下命令 git --version // 如无，则安装 yum install curl-devel expat-devel gettext-devel openssl-devel zlib-devel perl-devel yum install -y git 3. 创建用户并配置其仓库，执行如下命令 参考资料： 使用 Git Hook 自动部署 Hexo 到个人 VPS useradd git passwd git // 设置密码 su git // 这步很重要，不切换用户后面会很麻烦 cd /home/git/ mkdir -p projects/blog // 项目存在的真实目录 mkdir repos &amp;&amp; cd repos git init --bare blog.git // 创建一个裸露的仓库 cd blog.git/hooks vi post-receive // 创建 hook 钩子函数，输入了内容如下（原理可以参考上面的链接） #!/bin/sh git --work-tree=/home/git/projects/blog --git-dir=/home/git/repos/blog.git checkout -f 添加完毕后修改权限，执行如下命令 chmod +x post-receive exit // 退出到 root 登录 chown -R git:git /home/git/repos/blog.git // 添加权限 4. 测试 git 仓库是否可用，在 Windows 端另找空白文件夹，执行如下命令 git clone git@server_ip:/home/git/repos/blog.git 如果能把空仓库拉下来，就说明 git 仓库搭建成功了 5. 建立 ssh 信任关系，在本地电脑，执行如下命令 参考资料： ssh-copy-id 帮你建立信任 ssh-copy-id -i C:/Users/yourname/.ssh/id_rsa.pub git@server_ip ssh git@server_ip // 测试能否登录 注：此时的 ssh 登录 git 用户不需要密码！否则就有错，请仔细重复步骤 3-4 6. 如果第 5 步能成功，为了安全起见禁用 git 用户 的 shell 登录权限，从而只能用 git clone,git push 等登录，执行如下命令 参考资料： Git Server - 限制 Git 用户使用 SSH 登陆操作 cat /etc/shells // 查看 `git-shell` 是否在登录方式里面，有则跳过 which git-shell // 查看是否安装 vi /etc/shells 添加上 2 步显示出来的路劲，通常在 /usr/bin/git-shell 修改 /etc/passwd 中的权限，将原来的 git:x:1000:1000::/home/git:/bin/bash 修改为 git:x:1000:1000:,,,:/home/git:/usr/bin/git-shell 搭建 nginx 服务器1. 下载并安装 nginx，执行如下命令 参考资料： Nginx 源码安装和简单的配置 Nginx 配置 HTTPS 服务器 cd /usr/local/src wget http://nginx.org/download/nginx-1.15.2.tar.gz tar xzvf nginx-1.15.2.tar.gz cd nginx-1.15.2 ./configure // 如果后面还想要配置 SSL 协议，就执行后面一句！ ./configure --prefix=/usr/local/nginx --with-http_stub_status_module --with-http_ssl_module --with-file-aio --with-http_realip_module make &amp;&amp; make install 设置开机自启动 vim /usr/lib/systemd/system/nginx.service [Unit] Description=The nginx HTTP and reverse proxy server After=syslog.target network.target remote-fs.target nss-lookup.target [Service] Type=forking PIDFile=/usr/local/nginx/logs/nginx.pid ExecStartPre=/usr/local/nginx/sbin/nginx -t ExecStart=/usr/local/nginx/sbin/nginx ExecReload=/bin/kill -s HUP /usr/local/nginx/logs/nginx.pid ExecStop=/bin/kill -s QUIT /usr/local/nginx/logs/nginx.pid PrivateTmp=true [Install] WantedBy=multi-user.target 启动 nginx systemctl enable nginx.service systemctl restart nginx.service systemctl status nginx.service ss -lntup 2. 配置 nginx 文件 参考资料： Linux 中 nginx 基本操作命令 先启动是否安装成功，执行如下命令 nginx // 直接来！浏览器查看 server_ip，默认是 80 端口 配置文件，执行如下命令 nginx -s stop // 先停止 nginx cd /usr/local/nginx/conf vi nginx.conf 修改 root 解析路径，如下图 同时将 user 改为 root 如下图，不然 nginx 无法访问 /home/git/project/blog nginx -s reload 参考： 带你跳过各种坑，一次性把 Hexo 博客部署到自己的服务器 配置 jenkins在 jenkins 页面提示目录中找到默认密码，输入 jenkis 域名，登陆 jenkins。 安装 jenkins 社区推荐的插件 注意：到这一步会有一个坑，简直是神坑，因为 Jenkins 自身的原因，导致默认安装一到这一步就会报错：Jenkins 是离线状态，这是因为伟大的墙导致的。具体解决方法可以看我的另一篇博客： jenkins 实例似乎已离线问题 配置 github获取 sercret text登陆 github 网站，进入 github-&gt;Settings-&gt;Developer settings-&gt; Generate new token，点击生成完毕一定记录下下面的secret text。secret text 一定要记住，忘记的话只能重新生成。 GitHub webhooks 设置 进入 GitHub 上指定的项目（hexo 仓库） –&gt; setting --&gt; WebHooks&amp;Services --&gt; add webhook –&gt; 输入刚刚部署 jenkins 的服务器的 IP 图片中标红区域是变化的，后缀都是一样的为github-webhook。 jenkins 中的 github 配置 配置 GitHub Plugin系统管理 –&gt; 系统设置 –&gt; GitHub –&gt; Add GitHub Sever API URL 输入 https://api.github.com，Credentials点击 Add 添加，Kind选择Secret Text, 具体如下图所示。 设置完成后，点击Test Connection, 提示Credentials verified for user UUserName, rate limit: xxx, 则表明有效。 创建一个 freestyle 任务 General 设置 填写 GitHub project URL, 也就是你的项目主页 eg.https://github.com/your_name/your_repo_name 项目名称自定义。 配置源码管理 这里有几点要注意：（1）要加入一个有写权限的账号，点击 Add 后选择创建用户 创建完需要去 系统管理 --- 全局安全配置 ---- 项目矩阵授权策略 里分配用户需要的写权限。 具体可以参考： jenkins 配置记录（1）–添加用户权限 （2）要监听的 Branch 根据自己需要填写 构建触发器，构建环境 构建 构建脚本 将上图的构建脚本替换如下： cd /var/www/blog（hexo 目录） git pull hexo clean hexo g -d 构建后操作 构建前 clone hexo 将 hexo 初始代码拉取到 /var/www/blog 目录中，以后 jenkins 会监控 github 的 push 操作，一旦发现 push 会自动更新。 cd /var/www git clone https://github.com/dumingcode/dumingcode.github.io.git blog nginx 反向代理 hexohexo 为静态网站，所以直接用 nginx 反向代理即可,nginx 脚本如下：注意 root 指向的是 hexo 部署目录。 server { listen 80; server_name blog.buyasset.club; index index.html index.htm index.php default.html default.htm default.php; root /var/www/blog; #error_page 404 /404.html; location ~ .*\.(ico|gif|jpg|jpeg|png|bmp|swf)$ { access_log off; expires 1d; } location ~ .*\.(js|css|txt|xml)?$ { access_log off; expires 12h; } location / {try_files $uri $uri/ =404;} } 测试 CICD 效果 进入本地 hexo 目录，修改发布的博客，然后执行 hexo g -d，登陆 jenkins 发现 jenkins 已经获取到了 push 操作，并且执行了自动构建任务。以下为 jenkins 的变更记录 Site updated: 2018-04-21 13:35:51 (commit: 76f3c53) (details) Commit 76f3c530d077782fd66a8ca375afaa17cd188286 by duming Site updated: 2018-04-21 13:35:51 (commit: 76f3c53) 当配置完成测试构建时如果碰到 Jenkins 控制台报错：error: cannot open .git/FETCH_HEAD: Permission denied, 可先登录服务器查看 .git/FETCH_HEAD 文件的权限是否正确， 我最后的解决方法是： [root@blog var]# cd /var/www/ [root@blog www]# ll total 4 drwxr-xr-x 17 root root 4096 Sep 30 15:04 blog [root@blog www]# chown -R jenkins.jenkins blog 因为最开始创建这个目录使用的 root 创建，Jenkins 对这个目录没有权限，导致最后纠结了好久。 参考链接 手把手教你搭建 Jenkins+Github 持续集成环境 Jenkins+Github 持续集成 Jenkins 最佳实践 hexo 自动部署 基于 Hexo 的全自动博客构建部署系统]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[蓝绿发布、滚动发布、灰度发布等部署方案对比与总结]]></title>
    <url>%2F2018%2F04%2F15%2F%E8%93%9D%E7%BB%BF%E5%8F%91%E5%B8%83%E3%80%81%E6%BB%9A%E5%8A%A8%E5%8F%91%E5%B8%83%E3%80%81%E7%81%B0%E5%BA%A6%E5%8F%91%E5%B8%83%E7%AD%89%E9%83%A8%E7%BD%B2%E6%96%B9%E6%A1%88%E5%AF%B9%E6%AF%94%E4%B8%8E%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[在项目迭代的过程中，不可避免需要进行项目上线。上线对应着部署或者重新部署，部署对应着修改，修改则意味着风险。目前有很多用于部署的技术，有的简单，有的复杂，有的得停机，有的不需要停机即可完成部署。作为技术人员，大家可能听说过“滚动发布”和“蓝绿发布”等术语，但是很多人并不清楚这些术语背后的原理。本文试图总结当前主流的发布策略，每个的优劣，适用性，让开发人员特别是架构师对现代发布技术有一个更为清晰全面的认识，让大家能够根据自己的企业上下文，对发布策略做出正确的选型和实践。 单服务器组发布 先解释下单服务器组的概念，早先我们机器资源比较紧张，不像现在云计算和虚拟化（包括容器技术）这么发达，所以应用机器基本是预先静态分配好的（一般由运维负责分配），原来应用 A 住在这 n 台机器上，那么下次升级发布的应用 A 也住在这 n 台机器上，所以称为单服务器组发布方式。 蛮力发布 如下图所示，这种发布方式比较简单粗暴，有点像我们传统的软件升级方式，主要靠手工完成，先将老版本 V1 全部下掉，再将新版本发到机器上去。这种方式会引入服务中断（停机），在开发测试环境是可行的，但对于生产环境发布，其会直接影响用户的使用体验，这种方式一般是不建议的。 优势和适用场合 优势： •简单成本低 不足： •服务中断用户受影响，出了问题回退也慢 适用场合： •开发测试环境 •非关键应用（用户影响面小） •初创公司什么都缺，找夜深人静用户访问量小的时间干 蛮力发布会引入服务中断时间 金丝雀发布（单服务器组）在蛮力发布基础上的一种简单改进发布方式，目前仍然是不少成长型技术组织的主流发布方式。单服务器组下的金丝雀发布的简化步骤如下图所示： 部署过程1. 金丝雀发布一般先发 1 台，或者一个小比例，例如 2% 的服务器，主要做流量验证用，也称为金丝雀 (Canary) 测试（国内常称灰度测试）。以前旷工开矿下矿洞前，先会放一只金丝雀进去探是否有有毒气体，看金丝雀能否活下来，金丝雀发布由此得名。简单的金丝雀测试一般通过手工测试验证，复杂的金丝雀测试需要比较完善的监控基础设施配合，通过监控指标反馈，观察金丝雀的健康状况，作为后续发布或回退的依据。 2. 如果金丝测试通过，则把剩余的 V1 版本全部升级为 V2 版本。如果金丝雀测试失败，则直接回退金丝雀，发布失败。 灰度发布／金丝雀发布由以下几个步骤组成： • 准备好部署各个阶段的工件，包括：构建工件，测试脚本，配置文件和部署清单文件。 • 从负载均衡列表中移除掉「金丝雀」服务器。 • 升级「金丝雀」应用（排掉原有流量并进行部署）。 • 对应用进行自动化测试。 • 将「金丝雀」服务器重新添加到负载均衡列表中（连通性和健康检查）。 • 如果「金丝雀」在线使用测试成功，升级剩余的其他服务器（否则就回滚）。 除此之外灰度发布还可以设置路由权重，动态调整不同的权重来进行新老版本的验证。 优势和适用场合 优势： •用户体验影响小，金丝雀发布过程出现问题只影响少量用户 不足： •发布自动化程度不够，发布期间可引发服务中断 适用场合： •对新版本功能或性能缺乏足够信心 •用户体验要求较高的网站业务场景 •缺乏足够的自动化发布工具研发能力 少量金丝雀先接受流量，再全量发布 滚动式发布（单服务器组） 定义 滚动发布：一般是取出一个或者多个服务器停止服务，执行更新，并重新将其投入使用。周而复始，直到集群中所有的实例都更新成新版本。 特点 这种部署方式相对于蓝绿部署，更加节约资源——它不需要运行两个集群、两倍的实例数。我们可以部分部署，例如每次只取出集群的 20% 进行升级。 滚动发布是在金丝雀发布基础上的进一步优化改进，是一种自动化程度较高的发布方式，用户体验比较平滑，是目前成熟型技术组织所采用的主流发布方式。单服务器组下的滚动发布的简化步骤如下图所示： 部署过程1. 滚动式发布一般先发 1 台，或者一个小比例，如 2% 服务器，主要做流量验证用，类似金丝雀 (Canary) 测试。 2. 滚动式发布需要比较复杂的发布工具和智能 LB，支持平滑的版本替换和流量拉入拉出。 3. 每次发布时，先将老版本 V1 流量从 LB 上摘除，然后清除老版本，发新版本 V2，再将 LB 流量接入新版本。这样可以尽量保证用户体验不受影响。 4. 一次滚动式发布一般由若干个发布批次组成，每批的数量一般是可以配置的（可以通过发布模板定义）。例如第一批 1 台（金丝雀），第二批 10%，第三批 50%，第四批 100%。每个批次之间留观察间隔，通过手工验证或监控反馈确保没有问题再发下一批次，所以总体上滚动式发布过程是比较缓慢的 (其中金丝雀的时间一般会比后续批次更长，比如金丝雀 10 分钟，后续间隔 2 分钟)。 5. 回退是发布的逆过程，将新版本流量从 LB 上摘除，清除新版本，发老版本，再将 LB 流量接入老版本。和发布过程一样，回退过程一般也比较慢的。 6. 滚动式发布国外术语通常叫 Rolling Update Deployment。 优势和适用场合 优势： •用户体验影响小，体验较平滑 不足： •发布和回退时间比较缓慢 •发布工具比较复杂，LB 需要平滑的流量摘除和拉入能力 适用场合： •用户体验不能中断的网站业务场景 •有一定的复杂发布工具研发能力； 滚动式发布，流量平滑过渡，图片来自附录 6.1 双服务器组发布 随着云计算和虚拟化技术的成熟，特别是容器等轻量级虚拟化技术的引入，计算资源受限和申请缓慢问题已经逐步解决，可以做到弹性按需分配。为一次发布分配两组服务器，一组运行现有的 V1 老版本，一组运行待上线的 V2 新版本，再通过 LB 切换流量方式完成发布，这就是所谓的双服务器组发布方式。 蓝绿发布（双服务器组）蓝绿发布仅适用于双服务器组发布，可以认为是对蛮力发布的一种简单优化发布方式。简化过程如下图所示： 部署过程1.V1 版本称为蓝组，V2 版本称为绿组，发布时通过 LB 一次性将流量从蓝组直接切换到绿组，不经过金丝雀和滚动发布，蓝绿发布由此得名； 2. 出现问题回退也很直接，通过 LB 直接将流量切回蓝组。 3. 发布初步成功后，蓝组机器一般不直接回收，而是留一个待观察期，视具体情况观察期的时间可长可短，观察期过后确认发布无问题，则可以回收蓝组机器。 从过程不难发现，在部署的过程中，我们的应用始终在线。并且新版本上线的过程中，并没有修改老版本的任何内容，在部署期间，老版本的状态不受影响，这样风险很小。并且只要老版本的资源不被删除，理论上，我们可以在任何时间回滚到老版本。 蓝绿发布的注意事项 当你切换到蓝色环境时，需要妥当处理未完成的业务和新的业务。如果你的数据库后端无法处理，会是一个比较麻烦的问题。• 可能会出现需要同时处理微服务架构应用和传统架构应用的情况，如果在蓝绿部署中协调不好这两者，还是有可能会导致服务停止。• 需要提前考虑数据库与应用部署同步迁移 / 回滚的问题。• 蓝绿部署需要有基础设施支持。• 在非隔离基础架构（ VM 、 Docker 等）上执行蓝绿部署，蓝色环境和绿色环境有被摧毁的风险。 优势和适用场合 优势： •升级切换和回退速度非常快 不足： •切换是全量的，如果 V2 版本有问题，则对用户体验有直接影响； •需要两倍机器资源； 适用场合： •对用户体验有一定容忍度的场景 •机器资源有富余或者可以按需分配（AWS 云，或自建容器云） •暂不具备复杂滚动发布工具研发能力； 蓝绿发布一次完成流程切换 金丝雀发布（双服务器组）对蓝绿部署的一种简单优化，发布时先从绿组拉入 1 台金丝雀，待金丝雀验证通过再发全量。对比蓝绿发布，该发布方式的优势是有一个生产流量的金丝雀验证过程，可以减轻 V2 可能有问题的风险和影响面。简化发布过程如下图所示： 滚动式发布（双服务器组）滚动式发布是对上面的蓝绿和金丝雀发布的进一步优化，按批次增量滚动发布，提供更平滑的用户体验。 部署过程1. 发布前先申请一批新服务器，数量一般和 V1 版本相同，将 V2 版本应用发布到新服务器上。例如如果在 AWS 云上，则可以直接调用 API 申请一批新 VM，如果用容器云 Kubernetes，则可以直接启动一批新容器（使用 V2 版本容器镜像）。 2. 一般会先通过 LB 拉入 1 台 V2 版本的机器，这台机器也相当于金丝雀，用于流量验证。 3. 逐步按批次完成发布，每批只需要通过 LB 拉入 V2 版本，再拉出对应数量的 V1 版本。批次之间留有观察间隔，通过手工或监控反馈确保没有问题再继续发布。 4. 发布有问题回退很快，直接通过 LB 将流量切回 V1 即可。 5. 完成发布后，一般 V1 版本要保留观察以备万一，比如留 1 天，1 天后没有问题则回收 V1 机器资源。 优势和适用场合 优势： •用户体验影响小； •升级切换和回退（rollback）速度比单服务器组滚动发布要快，LB 切流量即可； 不足： •需要两倍机器资源； •发布工具比较复杂，LB 需要流量切换能力 适用场合： •用户体验不能中断的网站业务场景 •机器资源有富余或者可以按需分配（AWS 云，或自建容器云） •有一定的发布工具研发能力； 滚动式发布，流量平滑过渡 其它发布方式 上述都是偏传统的发布方式，能覆盖大部分应用发布场景。针对一些关键新功能的上线发布，或者一些特定的场景，还有一些特殊的发布方式。 功能开关发布 利用代码中的功能开关（Feature Flag/Toggle/Switch）来控制发布逻辑，一般不需要复杂的发布工具和智能 LB 配合，是一种相对比较低成本和简单的发布方式。这种方式也是支持现代 DevOps 理念，研发人员可以灵活定制和自助完成的发布方式。功能开关的原理如下图所示： 部署过程 • 功能开关发布需要一个配置中心或者开关中心这样的服务支持，例如携程的 Apollo 配置中心或者开源的 FF4J，这些都支持开关发布。业界还有专门的功能开关 SaaS 服务，例如 LaunchDarkly。通过配置中心，运维或研发人员可以在运行期动态配置功能开关的值。当然，功能开关发布只是配置中心的一种使用场景，配置中心还能支持其它很多动态配置场景。 • 功能开关服务一般提供客户端 SDK，方便开发人员集成。在运行期，客户端 SDK 会同步最新的开关值，技术实现有推方式 (push)，也有拉方式 (pull)，或者推拉结合方式。 • 新功能（V2 new feature）和老功能（V1 old feature）住在同一套代码中，新功能隐藏在开关后面，如果开关没有打开，则走老代码逻辑，如果开关打开，则走新代码逻辑。技术实现上可以理解为一个简单的 if/else 逻辑。 • 应用上线后，开关先不打开，然后运维或研发人员通过开关中心打开新功能，经过流量验证新功能没有问题，则发布完成；如果有问题，则随时可以通过开关中心切回老功能逻辑。 优势和不足 • 优势 升级切换和回退速度非常快。相对于复杂的发布工具，实施比较简单，成本相对低廉。研发能够灵活定制发布逻辑，支持 DevOps 自助发布。 • 不足 切换是全量的，如果 V2 版本有问题，则对用户体验有直接影响。对代码有侵入，代码逻辑会变复杂，需要定期清理老版本逻辑，维护成本变高。 适用场合： •对用户体验有一定容忍度的场景 •已有配置中心或开关中心服务 •暂不具备研发复杂发布工具能力； 影子测试 对于一些涉及核心业务的遗留系统的升级改造，为了确保万无一失，有一种称为影子测试的大招，采用比较复杂的流量复制、回放和比对技术实现。下面是影子测试的一个样例架构图： 部署过程 影子测试一般适用于遗留系统的等价重构迁移，例如 .net 转 Java，或者 SQLServer 数据库升级为 MySQL 数据库，且外部依赖不能太多，否则需要开发很多 mock，测试部署成本会很高，且比对测试更加复杂和不稳定。 • 目标实现老的 legacy 服务迁移升级到新的 experimental 服务。 • 测试开始前，需要在测试环境部署一份 legacy 服务和 experimental 服务，同时将生产数据库复制两份到测试环境。同时需要将生产请求日志收集起来，一般可以通过 kafka 队列收集，然后通过类似 goreplay 这样的工具，消费 kafka 里头的请求日志，复制回放，将请求分发到 legacy 服务和 experimental 服务，收到响应后进行比对，如果所有响应比对成功，则可以认为 legacy 服务和 experimental 服务在功能逻辑上是等价的；如果有响应比对失败，则认为两者在功能逻辑上不等价，需要修复 experimental 并重新进行影子测试，直到全部比对成功。根据系统复杂度和关键性不同，比对测试时间短的可能需要几周，长的可达半年之久。 • 影子测试因为旁路在独立测试环境中进行，可以对生产流量完全无影响。 优势和不足 • 优势 对生产用户体验完全无影响。可以使用生产真实流量进行测试（复制比对）。 • 不足 搭建复杂度很高，技术门槛高，数据库的导出复制是难点。外部依赖不能太多，否则测试部署成本很高，且比对测试更加复杂和不稳定。 适用场合： •核心关键业务，比如涉及资金的 •具备一定影子测试平台研发能力，包括流量复制、数据库导出复制和分发比对系统。 A/B 测试 灰度发布是指在黑与白之间，能够平滑过渡的一种发布方式。AB Test 就是一种灰度发布方式，让一部分用户继续用 A，一部分用户开始用 B，如果用户对 B 没有什么反对意见，那么逐步扩大范围，把所有用户都迁移到 B 上面来。灰度发布可以保证整体系统的稳定，在初始灰度的时候就可以发现、调整问题，以保证其影响度 A/B 测试原来主要用于产品功能的比对测试的方法，收集用户反馈和对比数据做产品功能设计的决策，例如可用性、受欢迎程度、可见性等等。 A/B 测试通常用在应用的前端上，不过当然需要后端来支持。实际上，A/B 测试也可以作为一种新功能发布技术。下图展示基于 LB 实现的一种 A/B 测试发布。 实践要点 1. 上图中，原来 PC 端和手机端都访问老版本 V1 服务（也称 A 组或控制组），当 V2 新版本（也称 B 组或实验组）发布以后，为了验证 V2 的功能正确性，同时也为了避免 V2 有问题时影响所有用户，先通过 LB 将手机端的流量切换到 V2 版本，经过一段时间的 A/B 比对测试和观察（主要通过用户和监控反馈），确保 V2 正常，则通过 LB 将全部流量切换到 V2。 2. 基于 LB 方式实现 A/B 测试，LB 需要能够通过某种条件做流量路由，例如通过 client ip，设备类型，浏览器类型，甚至是定制的 HTTP Header 或查询字符串。 3. 高级的 A/B 测试需要专门的平台支撑，wasabi 附录 6.6 就是 intuit 开源的一个支持高级 A/B 测试的平台，这类平台可以细粒度到针对某类用户做 A/B 测试，例如针对某个地区的用户，某个年龄段的用户，公司内部用户等等。举了例子，假设一个关键业务的新功能上线，为了降低风险采用 A/B 测试，可以做到先只让公司内部员工能访问到新功能，待新功能验证过，再全量放开给外部用户使用。 4. 功能开关和 A/B 测试有点相似，但功能开关一般是无状态和全量的，无法做到针对某类特定用户进行测试，而 A/B 测试一般是有状态的，能够跟踪事务和用户级别的状态，可以实现针对某类特定用户进行测试。 A/B 测试与蓝绿部署的区别在于， A/B 测试目的在于通过科学的实验设计、采样样本代表性、流量分割与小流量测试等方式来获得具有代表性的实验结论，并确信该结论在推广到全部流量可信；蓝绿部署的目的是安全稳定地发布新版本应用，并在必要时回滚。A/B 测试原来主要用于产品功能的比对测试，收集用户反馈和对比数据做产品功能设计的决策。实际上，A/B 测试也可以作为一种新功能发布技术。下图展示基于 LB 实现的一种 A/B 测试发布。 优势和适用场合 优势： •用户体验影响小； •可以使用生产流量测试； •可以做到针对某类特定目标用户进行测试； 不足： •搭建复杂度相对高，有一定技术门槛 适用场合： •核心关键业务，比如涉及资金的 •具备一定的 A/B 测试平台研发能力 针对某类目标用户进行 A/B 测试 比较 下表对各种发布策略，从各个维度进行综合比较，供参考： 结论和建议 下面是对发布策略的一些选型建议，供不同阶段公司参考： 1. 蛮力发布一般是不建议采用的，除非是开发测试环境，用户体验不敏感的非关键应用，或者是创业期什么都缺时候的无奈之举。 2. 如果暂时还不具备研发较复杂的滚动发布工具和配套智能 LB，则功能开关是一种不错的轻量级发布技术，投入相对较小的成本，可以让研发人员灵活定制发布逻辑。 3. 金丝雀发布通过少量新版本服务器接收生产流量的方式去验证新版本，可以显著降低风险。金丝雀发布适用于大部分场景，一般成长型公司就可以采用。 4. 对于达到一定业务体量的公司，考虑到用户体验对业务的关键性，则需要投入研发资源开发支持滚动式发布的工具和配套的智能 LB，实现自动化和零停机的发布。滚动式发布一般和金丝雀发布配合，先发一台金丝雀去验证流量，再按批次增量发布。 5. 随着轻量级虚拟化（例如容器）的普及，双服务器组发布方式具有更快的发布和回退速度，是值得投入的高级发布技术。蓝绿部署仅适用于双服务器组，滚动式发布既可以在单服务器组上实现，也可以在双服务器组上实现。 6. 对于涉及关键核心业务的新功能上线，采用 A/B 测试，可以显著降低发布风险，A/B 测试是唯一一种支持针对特定用户组进行生产测试的高级发布技术。当然 A/B 测试的投入不低，建议有一定研发能力的组织采用。 7. 对于关键核心业务的迁移重构，为确保万无一失，最后的一个大招是影子测试，影子测试对生产流量和用户完全无影响。当然这个大招的投入成本和门槛都高，建议有足够业务体量和研发能力的组织投入。 8. 上述的各种发布策略并不是非此即彼的，一个公司常常会综合采用多种发布技术作为互补，实现灵活的发布能力。例如主流的发布手段是金丝雀 + 滚动式发布，某些业务线可能根据业务场景需要采用功能开关发布，还有一些业务线则可能采用高级的 A/B 测试发布手段。 参考文章https://www.google.com 微服务部署：蓝绿部署、滚动部署、灰度发布、金丝雀发布 金丝雀发布、滚动发布、蓝绿发布到底有什么差别？ https://github.com/ContainerSolutions/k8s-deployment-strategies https://opensource.com/article/18/2/feature-flags-ring-deployment-model https://github.com/ctripcorp/apollo http://www.ff4j.org/ https://launchdarkly.com/ https://github.com/intuit/wasabi https://blog.zenika.com/2017/04/19/migration-dun-legacy-avec-goreplay/ https://github.com/buger/goreplay http://blog.shurenyun.com/untitled-9/ https://en.wikipedia.org/wiki/A/B_testing]]></content>
      <categories>
        <category>Deployment</category>
      </categories>
      <tags>
        <tag>Deployment</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java-JVM 参数详解]]></title>
    <url>%2F2018%2F04%2F10%2FJava-JVM%2F</url>
    <content type="text"><![CDATA[JVM 基本参数 -Xmx: 运行最大内存（memory maximum） 是指设定程序运行期间最大可占用的内存大小。如果程序运行需要占用更多的内存，超出了这个设置值，就会抛出 OutOfMemory 异常。堆的最大内存数，等同于 -XX:MaxHeapSize -Xms：启动内存(memory startup) 是指设定程序启动时占用内存大小。一般来讲，大点，程序会启动的快一点，但是也可能会导致机器暂时间变慢。堆的初始化初始化大小 -Xmn：(memory nursery/new) 堆中新生代初始及最大大小，如果需要进一步细化，初始化大小用 -XX:NewSize，最大大小用 -XX:MaxNewSize -Xss：(stack size) 线程栈大小，等同于 -XX:ThreadStackSize jvm 设置的值查看 执行 ps -ef | grep tomcat 或ps -ef | grep java输出如下 root 1882 1 0 8 月 02 ? 01:39:42 /root/SoftwareInstall/jdk/bin/java -Djava.util.logging.config.file=/usr/local/tomcat-geoserver/conf/logging.properties -Djava.util.logging.manager=org.apache.juli.ClassLoaderLogManager -server -Xms3072M -Xmx3072M -Xmn512M -Xss512k -XX:+AggressiveOpts - ..... org.apache.catalina.startup.Bootstrap start 如果没有设置，默认是不会有 -Xms3072M -Xmx3072M -Xmn512M -Xss512k 值打印 docker-compose 设置 jvmenvironment: - JAVA_OPTS= &apos;-Xmx3072m&apos; JVM 问题总结 geoserver 添加图层预览时提示java.lang.OutOfMemoryError: GC overhead limit exceeded 该错误 解决把 -Xmx 设置更大]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ZABBIX 3.2 监控服务器 TCP 连接状态]]></title>
    <url>%2F2017%2F07%2F29%2FZABBIX%203.2%20%E7%9B%91%E6%8E%A7%E6%9C%8D%E5%8A%A1%E5%99%A8TCP%E8%BF%9E%E6%8E%A5%E7%8A%B6%E6%80%81%2F</url>
    <content type="text"><![CDATA[ZABBIX 3.2 监控服务器 TCP 连接状态TCP 11 种状态图 (我也记不住所有的) TCP 连接可以使用命令获取 [root@abcdocker ~]# netstat -an|awk &apos;/^tcp/{++S[$NF]}END{for(a in S) print a,S[a]}&apos; TIME_WAIT 99 CLOSE_WAIT 44 FIN_WAIT1 1 FIN_WAIT2 5 ESTABLISHED 275 LAST_ACK 1 LISTEN 25 可以使用 man netstat 查看 TCP 的各种状态信息描述 LISTEN - 侦听来自远方 TCP 端口的连接请求； SYN-SENT - 在发送连接请求后等待匹配的连接请求； SYN-RECEIVED - 在收到和发送一个连接请求后等待对连接请求的确认； ESTABLISHED- 代表一个打开的连接，数据可以传送给用户； FIN-WAIT-1 - 等待远程 TCP 的连接中断请求，或先前的连接中断请求的确认； FIN-WAIT-2 - 从远程 TCP 等待连接中断请求； CLOSE-WAIT - 等待从本地用户发来的连接中断请求； CLOSING - 等待远程 TCP 对连接中断的确认； LAST-ACK - 等待原来发向远程 TCP 的连接中断请求的确认； TIME-WAIT - 等待足够的时间以确保远程 TCP 接收到连接中断请求的确认； CLOSED - 没有任何连接状态； 一、编写配置文件[root@abcdocker zabbix]# grep &quot;Include&quot; zabbix_agentd.conf Include=/etc/zabbix/zabbix_agentd.d/*.conf 我们查看我们设置的 Include 目录，这下面的 *.conf 文件都是可以读取的 编写配置文件 [root@abcdocker zabbix_agentd.d]# cat status.conf UserParameter=tcp.status[*],/etc/zabbix/zabbix_agentd.d/tcp_status.sh &quot;$1&quot; 咳咳，注意听讲(敲黑板) UserParameter= 后面是 key 的名称 /etc/zabbix/zabbix_agentd.d 存放脚本的路径 以前的文章有写过，大家可以看我的 zabbix 板块 复制脚本 [root@abcdocker zabbix_agentd.d]# cat tcp_status.sh #!/bin/bash #this script is used to get tcp and udp connetion status #tcp status metric=$1 tmp_file=/tmp/tcp_status.txt /bin/netstat -an|awk &apos;/^tcp/{++S[$NF]}END{for(a in S) print a,S[a]}&apos; &gt; $tmp_file case $metric in closed) output=$(awk &apos;/CLOSED/{print $2}&apos; $tmp_file) if [&quot;$output&quot; == &quot;&quot;];then echo 0 else echo $output fi ;; listen) output=$(awk &apos;/LISTEN/{print $2}&apos; $tmp_file) if [&quot;$output&quot; == &quot;&quot;];then echo 0 else echo $output fi ;; synrecv) output=$(awk &apos;/SYN_RECV/{print $2}&apos; $tmp_file) if [&quot;$output&quot; == &quot;&quot;];then echo 0 else echo $output fi ;; synsent) output=$(awk &apos;/SYN_SENT/{print $2}&apos; $tmp_file) if [&quot;$output&quot; == &quot;&quot;];then echo 0 else echo $output fi ;; established) output=$(awk &apos;/ESTABLISHED/{print $2}&apos; $tmp_file) if [&quot;$output&quot; == &quot;&quot;];then echo 0 else echo $output fi ;; timewait) output=$(awk &apos;/TIME_WAIT/{print $2}&apos; $tmp_file) if [&quot;$output&quot; == &quot;&quot;];then echo 0 else echo $output fi ;; closing) output=$(awk &apos;/CLOSING/{print $2}&apos; $tmp_file) if [&quot;$output&quot; == &quot;&quot;];then echo 0 else echo $output fi ;; closewait) output=$(awk &apos;/CLOSE_WAIT/{print $2}&apos; $tmp_file) if [&quot;$output&quot; == &quot;&quot;];then echo 0 else echo $output fi ;; lastack) output=$(awk &apos;/LAST_ACK/{print $2}&apos; $tmp_file) if [&quot;$output&quot; == &quot;&quot;];then echo 0 else echo $output fi ;; finwait1) output=$(awk &apos;/FIN_WAIT1/{print $2}&apos; $tmp_file) if [&quot;$output&quot; == &quot;&quot;];then echo 0 else echo $output fi ;; finwait2) output=$(awk &apos;/FIN_WAIT2/{print $2}&apos; $tmp_file) if [&quot;$output&quot; == &quot;&quot;];then echo 0 else echo $output fi ;; *) echo -e &quot;\e[033mUsage: sh $0 [closed|closing|closewait|synrecv|synsent|finwait1|finwait2|listen|established|lastack|timewait]\e[0m&quot; esac 提示： 脚本来源于网络 因为脚本是把 tcp 的一些信息存放在 /tmp/ 下，为了 zabbix 可以读取到我们设置 zabbix 可以读的权限 [root@abcdocker zabbix_agentd.d]# chmod +x tcp_connection_status.sh [root@abcdocker zabbix_agentd.d]# chown zabbix.zabbix /tmp/tcp_status.txt 重点： 这里要添加执行权限和 tcp_status 的 属主 和属组 执行脚本测试 既然脚本写完了，我们就需要执行一下 [root@abcdocker zabbix_agentd.d]# zabbix_get -s 127.0.0.1 -k tcp.status[established] 8 [root@abcdocker zabbix_agentd.d]# zabbix_get -s 127.0.0.1 -k tcp.status[lastack] 0 [root@abcdocker zabbix_agentd.d]# zabbix_get -s 127.0.0.1 -k tcp.status[finwait1] 0 [root@abcdocker zabbix_agentd.d]# zabbix_get -s 127.0.0.1 -k tcp.status[timewait] 101 如果没有 zabbix_get 需要 yum 安装 二、WEB 界面配置1. 创建模板 2. 设置模板 3. 添加监控项 添加完基本上就是下面这样 为了方便大家添加，我已经将 name 和 key 整理如下. 手动添加即可 Name Key CLOSED tcp.status[closed] CLOSE_WAIT tcp.status[closewait] CLOSING tcp.status[closing] ESTABLISHED tcp.status[established] FIN WAIT1 tcp.status[finwait1] FIN WAIT2 tcp.status[finwait2] LAST ACKtcp.status[lastack] LISTEN tcp.status[listen] SYN RECVtcp.status[synrecv] SYN SENTtcp.status[synsent] TIME WAIT tcp.status[timewait] 我这里提供模板：链接：http://pan.baidu.com/s/1sle6oNj 密码：oqgs可以直接使用模板导入即可 4. 添加图表 我们所有的操作都在 TCP 模板下面添加和设置的，大家不要设置错了 添加完之后我们点击update 三、添加主机 进行查看 四、出图结果 小结： 因为 tcp 连接数不太好设置触发器，因为业务不同，具体设置多少还是要根据需求来。因为我这是个人博客监控所以连接数是多少都可以！ 关于 tcp 最大连接可以参考下面的文章http://www.cnblogs.com/fjping0606/p/4729389.html 关于 ZABBIX 更多相关文章请前往 ZABBIX 板块 参考 http://john88wang.blog.51cto.com/2165294/1586234/ 转自：ZABBIX 3.2 监控服务器 TCP 连接状态]]></content>
      <categories>
        <category>zabbix</category>
      </categories>
      <tags>
        <tag>zabbix</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ZABBIX 忘记登录密码]]></title>
    <url>%2F2017%2F07%2F29%2FZABBIX%20%E5%BF%98%E8%AE%B0%E7%99%BB%E5%BD%95%E5%AF%86%E7%A0%81%2F</url>
    <content type="text"><![CDATA[摘要 有些童鞋会忘记 zabbix 的登陆密码，今天给大家写一篇找回登陆密码~ 做运维由于账号比较多，脑子容易瓦特. 结果忘记自己的 zabbix 登录密码下面是找回登录密码的例子 未修改之前 (忘记登录密码) [root@abcdocker ~]# mysql -uroot -p -e &quot;select * from zabbix.users\G&quot; Enter password: *************************** 1. row *************************** userid: 1 alias: Admin name: Zabbix surname: Administrator passwd: ab66b6e18854fa4d45499d0a04a47b64 url: autologin: 1 autologout: 0 lang: en_GB refresh: 30 type: 3 theme: default attempt_failed: 0 attempt_ip: 14.130.112.2 attempt_clock: 1501141026 rows_per_page: 50 登录 MySQL 修改密码 [root@abcdocker ~]# mysql -uroot -p 由于密码是 md5 加密的，我们可以查看默认的 zabbix 密码的 md5 mysql&gt; use zabbix; mysql&gt; update users set passwd=&apos;5fce1b3e34b520afeffb37ce08c7cd66&apos; where userid=&apos;1&apos;; Query OK, 1 row affected (0.01 sec) Rows matched: 1 Changed: 1 Warnings: 0 解释：5fce1b3e34b520afeffb37ce08c7cd66 = zabbix 因为 zabbix 默认密码就是 zabbix 登录 Web 用户名：Admin 密码：zabbix 提示 ：登陆上请马上修改密码 完！ 转自：ZABBIX 忘记登录密码]]></content>
      <categories>
        <category>zabbix</category>
      </categories>
      <tags>
        <tag>zabbix</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Jenkins 自动化部署上线]]></title>
    <url>%2F2017%2F03%2F23%2FJenkins%20%E8%87%AA%E5%8A%A8%E5%8C%96%E9%83%A8%E7%BD%B2%E4%B8%8A%E7%BA%BF%2F</url>
    <content type="text"><![CDATA[摘要 jenkins 自动化部署项目, 通过 jenkins 部署来节省运维时间, 不需要手动 cp 上线及版本发布 一、Jenkins 是什么 Jenkins 是一款自包含的开源自动化服务, 可用于自动执行与构建, 测试和交付或部署软件有关的各种任务。 Jenkins 目前可以通过本地系统软件包 Docker 进行安装, 甚至可以通过任何安装了 Java 运行环境的计算机独立运行 二、上线流程图 既然我们说到自动化上线, 我们就不得不说说一个项目上线的流程. 只有规范起来才可以做到不出事故！ 上线流程图如下图所示 三、Jenkins 安装配置 Jenkins 依赖 Java 环境, 我们需要安装 Java 环境以及相关的环境准备 ### 关闭防火墙 $ iptables -F $ iptables -X $ systemctl stop firewalld $ systemctl disable firewalld ### 安装 yum 源 $ wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo $ wget -O /etc/yum.repos.d/epel.repo http://mirrors.aliyun.com/repo/epel-7.repo $ yum clean all &amp;&amp; yum makecache 1. 下载 Jdk 包 下载地址：http://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html 上传 jdk 包到服务器 ### 解压拷贝 jdk $ tar xf jdk-8u171-linux-x64.tar.gz -C /usr/local/ $ ln -s /usr/local/jdk1.8.0_171/ /usr/local/jdk $ ln -s /usr/local/jdk/bin/java /usr/bin/java ### 设置环境变量 $ vim /etc/profile export JAVA_HOME=/usr/local/jdk export CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar export PATH=$JAVA_HOME/bin:$PATH $ source /etc/profile 2. 安装 Jenkins$ wget -O /etc/yum.repos.d/jenkins.repo https://pkg.jenkins.io/redhat/jenkins.repo $ rpm --import https://pkg.jenkins.io/redhat/jenkins.io.key $ yum install jenkins -y $ systemctl start jenkins ## 如果我们启动 Jenkins 出现错误可以直接使用 systemctl status jenkins 查看错误 jenkins 相关目录释义： (1)/usr/lib/jenkins/：jenkins 安装目录，war 包会放在这里。 (2)/etc/sysconfig/jenkins：jenkins 配置文件，“端口”，“JENKINS_HOME”等都可以在这里配置。 (3)/var/lib/jenkins/：默认的 JENKINS_HOME。 (4)/var/log/jenkins/jenkins.log：jenkins 日志文件。 检查端口是否存在 3. 配置 Jenkins Jenkins 有安全策略, 我们按照提示拷贝验证码即可 将验证码复制到 Web 框里 我们这里使用推荐就可以了，因为后期我们都可以在安装 安装插件中，有的插件会因为网络问题无法安装成功 我们这里可以创建一个管理员，或者直接使用 admin 我们最好不要直接使用admin 安装完成访问地址：iP:8080 到这里我们 Jenkins 已经安装成功, 剩下的就是配置插件和配置环境 因为我们目前什么都没有需要安装插件, 点击下步安装插件 为了模拟环境我们需要安装 Jenkins 一些相关插件 下面 2 个 maven 插件都需要勾选 插件名称:maven lntergration 我们勾选安装重启 安装完成后如下图所示 默认是没有下面的 maven 项目的 4.Jenkins 配置项目 配置 SVN 地址 因为我是新建的 Jenkins 目录, 没有权限, 所以需要创建一个用于认证. 填写 SVN 地址，因为我这里的 svn 已经链接到 ldap, 所以不需要输入 svn 的密码, 默认这里是 svn 的用户和密码 具体文章可以参考 VisualSVN 迁移至 Linux SVN+Apache+ssl 集成 LDAP 认证成功之后 了解 maven 配置 首先我们的 svn 分支下面需要有 pom.xml 继续往下 ↓↓↓↓ 因为我们只安装 maven 的插件，并没有安装 maven 服务，所以这里需要我们配置 我们就在这里添加一个名字，maven 就自动安装了 Maven 安装完成了，需要依赖吧都是从 maven.apache.org 下载会比较慢，所以我们指定私服的地址, 因为在实际生产中也都是使用私服的。 在 maven 的配置文件里面也需要配置 配置文件 conf/settings.xml 因为我们所使用的是 Jenkins 的自动安装，而不是指定路径所以我们要查到这个配置文件 maven 自动安装的配置路径 配置 Maven 仓库地址 这里配置的都是私服地址 相关文章 Jenkins+Maven+SVN+Nexus 搭建持续集成环境 配置 Maven 镜像地址 配置 Maven 编译参数 [研发都会] 相关文章：maven 编译命令 这个 pom.xml 里面配置是私服的地址 因为代码里面有很多东西是需要拉去依赖包，这些依赖包就存放在本地的私有仓库里(Nexus) 代码中 pom.xml 配置如下 私有仓库的地址 5. 构建测试 控制台输出说明 6.Jenkins 工程目录 可以通过修改 Jenkins 主目录 Jenkins 打包好后的目录，这个 war 包就是我们需要拷贝的 tomcat 下面的 四、Jenkins 自动化部署项目案例 因为目前环境原因，我这里只是截图 Jenkins 发布的流程(本次演示只是针对测试环境日常发布版本) (1) Java 环境演示 [Jenkins 和 Tomcat 在一台服务器上]1.Jenkins 配置 SVN 部分配置 maven 及脚本设置 2. 不发脚本配置如下： 相关参考：Jenkins 可用环境变量列表 脚本的存放路径可以在 系统管理 -&gt; 全局配置 -&gt;Jenkins 路径 Last login: Thu Jun 28 18:01:59 2018 from 172.16.29.39 [root@tomcat ~]# cat /jenkins/deploy.sh #!/bin/bash # # Jenkins 工程构建通用 TOMCAT 部署脚本 # @author abcdocker # @create_time 2017-08-19 # # 在 Jenkins 内配置部署单元参数 # 参数格式：MAVEN_MODULE_NAME:TOMCAT_ABSOLUTE_PATH MAVEN 模块名称: 需要部署的目标 TOMCAT 绝对路径 # 只有单个部署单元且没有 Maven 子模块时，模块名称参数可以没有，参数格式为：TOMCAT_ABSOLUTE_PATH # # 注意： # 在本部署脚本内会执行 TOMCAT 启动脚本，为避免 Jenkins 在构建成功以后杀掉所有衍生的后台进程，需要在 Jenkins 内配置全局环境变量 BUILD_ID 值为 allow_to_run_as_daemon # # DEPLOY_TARGET_TOMCAT=$TOMCAT #校验部署参数，不能为空 if [-z &quot;$DEPLOY_TARGET_TOMCAT&quot;] then echo echo 部署参数为空，部署失败！ echo &quot;#####################################################################&quot; echo echo 单个部署单元参数格式： echo MAVEN_MODULE_NAME:TOMCAT_ABSOLUTE_PATH MAVEN 模块名称: 需要部署的目标 TOMCAT 绝对路径 echo echo 多个部署单元参数格式：（多个部署单元使用空格分割） echo MAVEN_MODULE_NAME:TOMCAT_ABSOLUTE_PATH MAVEN_MODULE_NAME:TOMCAT_ABSOLUTE_PATH echo echo &quot;#####################################################################&quot; exit 1 fi echo echo 部署参数：${DEPLOY_TARGET_TOMCAT} TOMCAT_ARR=${DEPLOY_TARGET_TOMCAT//;/} ARR=($TOMCAT_ARR) ARR_LEN=${#ARR[*]} echo 共 ${ARR_LEN} 个部署单元 i=1 #获取 Jenkins 传入的目标 TOMCAT 组 for T in $TOMCAT_ARR do echo echo 开始 处理第 ${i} 个部署单元 echo 第一个部署单元：$T #获取目标 TOMCAT 的 WAR 路径和 TOMCATA 的绝对路径 TOMCAT_PARAM=(${T//:/}) MODULE_NAME=${TOMCAT_PARAM[0]} TARGET_TOMCAT_PATH=${TOMCAT_PARAM[1]} WAR_PATH=&quot;$WORKSPACE/$MODULE_NAME/target/*.war&quot; echo 部署单元模块名称：&quot;${MODULE_NAME}&quot; echo 部署 WAR 包路径：&quot;${WAR_PATH}&quot; echo 部署 TOMCAT 路径：&quot;${TARGET_TOMCAT_PATH}&quot; #需要考虑 MAVEN 单模块下的部署问题 #if [&quot;${#ARR[*]}&quot; -eq 1 -a -z &quot;$TARGET_TOMCAT_PATH&quot; ] if [&quot;$ARR_LEN&quot; -eq 1 -a -z &quot;$TARGET_TOMCAT_PATH&quot;] then #MAVEN 过程没有子模块，单个部署单元 TARGET_TOMCAT_PATH=$MODULE_NAME MODULE_NAME=&quot;NULL&quot; fi #校验参数，WORKSPACE 变量来自于 Jenkins 环境变量 if [-z &quot;$MODULE_NAME&quot; -o ! -f $WAR_PATH] then echo 错误：MAVEN 部署模块名称 参数为空 或 找不到 WAR 包！ echo 部署失败！ exit 1 fi if [-z &quot;$TARGET_TOMCAT_PATH&quot; -o ! -d &quot;$TARGET_TOMCAT_PATH&quot;] then echo 错误：目标 TOMCAT 绝对路径 参数为空 或 该 TOMCAT 目录不存在！ echo 部署失败！ exit 1 fi echo 开始清理目标 TOMCAT 启动进程... TOMCAT_PID=`ps -ef |grep &quot;$TARGET_TOMCAT_PATH&quot; |grep &quot;start&quot; |awk &apos;{print $2}&apos;` if [-n &quot;$TOMCAT_PID&quot;] then echo TOMCAT_${i}，PID${TOMCAT_PID}，正在结束该进程... kill -9 $TOMCAT_PID &amp;&amp; echo PID${TOMCAT_PID} 已被干掉！ else echo TOMCAT_${i} 进程未启动！ fi echo 开始清理目标 TOMCAT 缓存... rm -rf $TARGET_TOMCAT_PATH/webapps/* rm -rf $TARGET_TOMCAT_PATH/logs/* rm -rf $TARGET_TOMCAT_PATH/work/* echo 开始部署 WAR 包... cp -a $WAR_PATH $TARGET_TOMCAT_PATH/webapps/ROOT.war &amp;&amp; echo WAR 包部署完毕。 echo 开始启动目标 TOMCAT 服务... sleep 10 /bin/bash $TARGET_TOMCAT_PATH/bin/startup.sh echo 开始配置 web 目录的 FTP 权限... #启动过程会自动解压 WAR 包，所以在这里需要等待 WAR 包解压完成再调整目录权限 sleep 30 chown -R vftpuser.vftpuser ${TARGET_TOMCAT_PATH}/webapps/ &amp;&amp; echo 目录权限配置完毕。 echo 部署成功 echo 完成 第 ${i} 个部署单元处理。 echo ((i++)) done 3. 构建效果如下图所示： (2) Java 环境演示 [Jenkins 和 Tomcat 不在一台服务器上]上面的脚本是针对 Jenkins 和 Tomcat 都在相同的目录, 有的时候我们测试环境会存在不在一台服务器的情况, 脚本如下 只是脚本简单的修改 [root@tomcat ~]# cat /jenkins/ysc.sh #!/bin/bash # # Jenkins 工程构建通用 TOMCAT 部署脚本 # @author 刘曙 # @create_time 2017-08-19 # # 在 Jenkins 内配置部署单元参数 # 参数格式：MAVEN_MODULE_NAME:TOMCAT_ABSOLUTE_PATH MAVEN 模块名称: 需要部署的目标 TOMCAT 绝对路径 # 只有单个部署单元且没有 Maven 子模块时，模块名称参数可以没有，参数格式为：TOMCAT_ABSOLUTE_PATH # # 注意： # 在本部署脚本内会执行 TOMCAT 启动脚本，为避免 Jenkins 在构建成功以后杀掉所有衍生的后台进程，需要在 Jenkins 内配置全局环境变量 BUILD_ID 值为 allow_to_run_as_daemon # # DEPLOY_TARGET_TOMCAT=$YSC HOST=root@172.16.1.35 #校验部署参数，不能为空 if [-z &quot;$DEPLOY_TARGET_TOMCAT&quot;] then echo echo 部署参数为空，部署失败！ echo &quot;#####################################################################&quot; exit 1 fi echo echo 部署参数：${DEPLOY_TARGET_TOMCAT} TOMCAT_ARR=${DEPLOY_TARGET_TOMCAT//;/} ARR=($TOMCAT_ARR) ARR_LEN=${#ARR[*]} echo 共 ${ARR_LEN} 个部署单元 i=1 #获取 Jenkins 传入的目标 TOMCAT 组 for T in $TOMCAT_ARR do echo echo 开始 处理第 ${i} 个部署单元 echo 第一个部署单元：$T #获取目标 TOMCAT 的 WAR 路径和 TOMCATA 的绝对路径 TOMCAT_PARAM=(${T//:/}) MODULE_NAME=${TOMCAT_PARAM[0]} TARGET_TOMCAT_PATH=${TOMCAT_PARAM[1]} #WAR_PATH=&quot;/jenkins/workspace/ysc-all/${MODULE_NAME}/target/*.war&quot; WAR_PATH=&quot;${WORKSPACE}/${MODULE_NAME}/target/*.war&quot; echo 部署单元模块名称：&quot;${MODULE_NAME}&quot; echo 部署 WAR 包路径：&quot;${WAR_PATH}&quot; echo 部署 TOMCAT 路径：&quot;${TARGET_TOMCAT_PATH}&quot; #判断 IP 是否有相关目录 ssh 172.16.1.35 &quot;[-d $TARGET_TOMCAT_PATH]&quot; &gt;/dev/null 2&gt;&amp;1 if [$? != 0];then echo 错误 else echo 正确 fi #校验参数，WORKSPACE 变量来自于 Jenkins 环境变量 if [-z &quot;$MODULE_NAME&quot; -o ! -f $WAR_PATH] then echo 错误：MAVEN 部署模块名称 参数为空 或 找不到 WAR 包！ echo 部署失败！ exit 1 fi #scp 软件包 ssh $HOST /etc/init.d/${MODULE_NAME} stop ssh 172.16.1.35 &quot;[-d $TARGET_TOMCAT_PATH/webapps/ROOT/]&quot; &gt;/dev/null 2&gt;&amp;1 if [$? = 0];then ssh 172.16.1.35 rm -rf $TARGET_TOMCAT_PATH/webapps/ROOT if [$? = 0];then scp $WAR_PATH root@172.16.1.35:$TARGET_TOMCAT_PATH/webapps/ROOT.war &amp;&amp; echo WAR 包部署完毕。 echo $TARGET_TOMCAT_PATH is OK else echo 删除 $TARGET_TOMCAT_PATH is error fi else echo &quot;not found $TARGET_TOMCAT_PATH/webapps/ROOT&quot; scp $WAR_PATH root@172.16.1.35:$TARGET_TOMCAT_PATH/webapps/ROOT.war &amp;&amp; echo WAR 包部署完毕。 ssh $HOST /etc/init.d/${MODULE_NAME} restart fi #################### 启动文件 done #scp /home/config.properties/ysc/${MODULE_NAME}.js root@172.16.1.35:$TARGET_TOMCAT_PATH/webapps/ROOT/web/js/basePath.js ssh $HOST /etc/init.d/${MODULE_NAME} restart Jenkins 配置如下修改 修改完成后我们构建演示 提示：这种环境下配置文件都是通过 maven build 进行控制，也就是通过研发控制配置文件 + (3) Java 环境演示 [上线脚本]线上环境演示 我们的上线流程如下： Jenkins 配置如下： 1. 首先测试环境脚本： [root@tomcat ~]# cat /server/scripts/.upgrade-smscenter.sh #!/bin/bash WAR=”/jenkins/workspace/portal-smscenter/bxg-sms-center-web/target/*.war” Path=”/data/hub/bxg-smscenter/date +%Y%m%d/“ scp_war(){ if [! -d $Path];then ssh root@file-server mkdir -p $Path scp $WAR root@file-server:$Path else scp $WAR root@file-server:$Path fi } ssh_file(){ ssh root@file-server &quot;/bin/bash /server/script/bxg/bxg-smscenter.sh&quot; } scp_war ssh_file 2. 跳板机脚本修改 [root@File-server1 ~]# cat /server/script/bxg/bxg-smscenter.sh #!/bin/bash HOST=online-server2 WAR=&quot;/data/hub/bxg-smscenter/`date +%Y%m%d`&quot; DIR=&quot;/application/smscenter/webapps/ROOT/&quot; function scp_file {if `ssh root@$HOST &quot;[ ! -d $WAR]&quot;`;then ssh root@$HOST &quot;mkdir -p $WAR&quot; fi scp $WAR/*.war root@$HOST:$WAR/ echo &quot;scp $WAR/*.war root@$HOST:$WAR/&quot; } ssh_deploy(){ssh root@$HOST &quot;/bin/bash /server/scripts/deploy_smscenter.sh&quot;} scp_file ssh_deploy 3.web 服务器脚本 [root@online-server2 ~]# cat /server/scripts/deploy_smscenter.sh #!/bin/bash WAR=&quot;/data/hub/bxg-smscenter/`date +%Y%m%d`&quot; OBJECT=&quot;/application/smscenter/webapps/ROOT/&quot; Backup=&quot;/data/tomcat/bxg-smscenter-`date +%Y%m%d`&quot; SCR_D=&quot;/application/smscenter/webapps/ROOT&quot; #config=&quot;/data/bak&quot; backup_tar(){ tar zcvf $Backup.tar.gz $SCR_D/ echo &quot; 为了防止意外 cp 整个项目目录存放 &quot; cp -a $SCR_D/ $Backup rm -rf $OBJECT/* } cp_war(){unzip $WAR/*.war -d $SCR_D/} cp_config(){ cat $Backup/WEB-INF/classes/application-prod.properties &gt;$SCR_D/WEB-INF/classes/application-prod.properties /etc/init.d/smscenter restart } backup_tar cp_war cp_config [root@online-server2 ~]# 相关文章 企业必会技能 tomcat + (4) NodeJs 环境演示 [上线脚本]node 环境上线流程Jenkins 配置如下 [node 项目不适用 maven, 所以可以不用创建 maven 项目, 直接在 Jenkins 创建普通项目就可以] 1. 测试环境脚本 [root@tomcat ~]# cat /server/scripts/mobile/mobile.sh #!/bin/bash source /etc/profile HOST=file-server BASE_DIR=/server/scripts/mobile/m url=$1 server=$2 DATE=`date +%Y%m%d` tar(){ rm -rf $BASE_DIR [-d $BASE_DIR] || mkdir $BASE_DIR cd $BASE_DIR echo &quot;##########################################################&quot; echo &quot; 代码拉取中！！！&quot; svn co -q $url/ . echo &quot;##########################################################&quot; } cp(){cd ${BASE_DIR} /bin/tar -zcvf m_${DATE}.tar.gz ./* echo &quot;##########################################################&quot; echo &quot; 文件已经打包完成！ 正在拷贝中！！！&quot; echo &quot;##########################################################&quot; sleep 5 scp m_${DATE}.tar.gz root@$HOST:/data/hub/bxg-mobile/ echo &quot;##########################################################&quot; echo &quot; 文件已经拷贝完成！ 正在上传服务器中！！！&quot; echo &quot;##########################################################&quot; ssh root@file-server &quot;/bin/bash /server/script/bxg/bxg-mobile.sh $server&quot; } tar cp [root@tomcat ~]# 2. 跳板机脚本 [root@File-server1 ~]# cat /server/script/bxg/bxg-mobile.sh #!/bin/bash HOST=$1 Mobile_tar=&quot;/data/hub/bxg-mobile&quot; DIR=&quot;/application/node&quot; DATE=`date +%Y%m%d` scp_file(){if `ssh root@$HOST &quot;[ ! -d $Mobile_tar]&quot;`;then ssh root@$HOST &quot;mkdir -p $Mobile_tar&quot; fi scp $Mobile_tar/m_${DATE}.tar.gz root@$HOST:$DIR/ sleep 3 echo &quot; &quot; echo &quot;##########################################################&quot; echo &quot;File-server 正在拷贝 ${HOST}！！！&quot; sleep 3 echo &quot;##########################################################&quot; } ssh_deploy(){ echo &quot;Hi&quot; ssh root@$HOST &quot;/bin/bash /server/scripts/bxg/bxg-mobile.sh&quot; } scp_file ssh_deploy 3.web 发布脚本 [root@iZbp11tefvghtcfn5mudgdZ ~]# cat /server/scripts/bxg/bxg-mobile.sh #!/bin/bash source /etc/profile DIR=&quot;/application/node&quot; DATE=`date +%Y%m%d` Backup=&quot;/application/node/m/&quot; BAK=&quot;/data/hub/bxg-mobile&quot; backup_tar(){ echo &quot; 为了防止意外 cp 整个项目目录存放 &quot; cp -ar $Backup ${BAK}/mobile_$DATE echo &quot;online-server 原目录拷贝备份完成！&quot; } cp_war(){ /etc/init.d/mobile stop #mv $Backup /tmp/m_${DATE} &amp;&amp; rm -rf /tmp/m_${DATE} rm -rf $Backup mkdir $Backup &amp;&amp; cd /application/node/ tar xf m_${DATE}.tar.gz -C $Backup } npm_config(){ cd $Backup cnpm install npm run build-prod npm run start-prod &amp;&gt;/var/log/mobile_${DATE}.log &amp; } C(){ echo &quot;++++++++++++++++++++++++++++++++++++++++++++++++++++&quot; curl -I 127.0.0.1:3000 echo &quot;###################################################&quot; echo &quot; 若是 200 服务启动正常！ 可以启动另一台！&quot; echo &quot;###################################################&quot; } backup_tar cp_war npm_config C 相关文章 Node.js 环境搭建 总结：Jenkins 自动化不是运维一个人就可以完成的, 需要研发的参与, 本文只是给大家展示一下我公司的自动化, 我眼里所谓的自动化. 希望大家不喜勿喷, 对文章有意见或建议请在评论留言哦~ 转自：Jenkins 自动化部署上线]]></content>
      <categories>
        <category>Jenkins</category>
      </categories>
      <tags>
        <tag>Jenkins</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Jenkins+Maven+SVN+Nexus 搭建持续集成环境]]></title>
    <url>%2F2017%2F03%2F22%2FJenkins%2BMaven%2BSVN%2BNexus%20%E6%90%AD%E5%BB%BA%E6%8C%81%E7%BB%AD%E9%9B%86%E6%88%90%E7%8E%AF%E5%A2%83%2F</url>
    <content type="text"><![CDATA[摘要 Jenkins 只是一个平台，真正运作的都是插件。这就是 jenkins 流行的原因，因为 jenkins 什么插件都有。Hudson 是 Jenkins 的前身，是基于 Java 开发的一种持续集成工具，用于监控程序重复的工作，Hudson 后来被收购，成为商业版。后来创始人又写了一个 jenkins，jenkins 在功能上远远超过 hudson 一、DevOps DevOps 是“开发”和“运维”的缩写。 DevOps 是一组最佳实践强调 (IT 研发、运维、测试) 在应用和服务生命周期中的协作和沟通，强调整个组织的合作以及交付和基础设施变更的自动化，从而实现持续集成、持续部署和持续交付 DevOps 平台四大模块1. 项目管理 (创建项目—&gt;&gt; 项目需求)2. 运维平台 (监控–日志收集—等)3. 持续交付 (提交完代码—&gt; 自动打包—&gt; 构建)4. 代码托管 (gitlab—-&gt; 代码提交)————————————————————&gt;&gt;DevOps 平台 针对 DevOps 开源项目1. 项目管理—(JIRA 非开源但是用的人比较多)、(Redmine 使用 ruby 写的)2. 代码托管—(SVN–usvn 有 web 管理界面)、(GitLab)3. 持续交付—(主流 Jenkins)、(GitLab gitlab-ci 也可以做交付)4. 运维平台—(国内的开源运维平台可能就是腾讯蓝鲸) 二、服务介绍 很多事情不是光运维就可以决定的，还需要跟研发交流，我这里只是演示一个大概的持续交付的流程~ 2.1 Jenkins 介绍 Jenkins 只是一个平台，真正运作的都是插件。这就是 jenkins 流行的原因，因为 jenkins 什么插件都有 Hudson 是 Jenkins 的前身，是基于 Java 开发的一种持续集成工具，用于监控程序重复的工作，Hudson 后来被收购，成为商业版。后来创始人又写了一个 jenkins，jenkins 在功能上远远超过 hudson 2.2 Maven 介绍 maven 的用途maven 是一个项目构建和管理的工具，提供了帮助 管理 构建、文档、报告、依赖、scms、发布、分发 的方法。可以方便的编译代码、进行依赖管理、管理二进制库等等。maven 的好处在于可以将项目过程规范化、自动化、高效化以及强大的可扩展性 利用 maven 自身及其插件还可以获得代码检查报告、单元测试覆盖率、实现持续集成等等。 maven 的核心概念介绍 Pom pom 是指 project object Model。pom 是一个 xml，在 maven2 里为 pom.xml。是 maven 工作的基础，在执行 task 或者 goal 时，maven 会去项目根目录下读取 pom.xml 获得需要的配置信息 pom 文件中包含了项目的信息和 maven build 项目所需的配置 Artifact 这个有点不好解释，大致说就是一个项目将要产生的文件，可以是 jar 文件，源文件，二进制文件，war 文件，甚至是 pom 文件。每个 artifact 都由 groupId:artifactId:version 组成的标识符唯一识别。需要被使用 (依赖) 的 artifact 都要放在仓库 (见 Repository) 中 Repositories Repositories 是用来存储 Artifact 的。如果说我们的项目产生的 Artifact 是一个个小工具，那么 Repositories 就是一个仓库，里面有我们自己创建的工具，也可以储存别人造的工具，我们在项目中需要使用某种工具时，在 pom 中声明 dependency，编译代码时就会根据 dependency 去下载工具（Artifact），供自己使用。 Build Lifecycle 是指一个项目 build 的过程。maven 的 BuildLifecycle 分为三种，分别为 default（处理项目的部署）、clean（处理项目的清理）、site（处理项目的文档生成）。他们都包含不同的 lifecycle。Build Lifecycle 是由 phases 构成的 …. 参考：关于 Maven 常用参数及说明 2.3 SVN 介绍SVN 是近年来崛起的非常优秀的版本管理工具，与 CVS 管理工具一样，SVN 是一个固态的跨平台的开源的版本控制系统。SVN 版本管理工具管理者随时间改变的各种数据。这些数据放置在一个中央资料档案库 repository 中，这个档案库很像一个普通的文件服务器或者 FTP 服务器，但是，与其他服务器不同的是，SVN 会备份并记录每个文件每一次的修改更新变动。这样我们就可以把任意一个时间点的档案恢复到想要的某一个旧的版本，当然也可以直接浏览指定的更新历史记录。 本站相关文章 SVN 服务实战应用指南 VisualSVN 迁移至 Linux SVN+Apache+ssl 集成 LDAP 2.4 Nexus 介绍maven 的仓库只有两大类：1. 本地仓库 2. 远程仓库，在远程仓库中又分成了 3 种： 1 中央仓库 2 私服 3 其它公共库。 私服是一种特殊的远程仓库，它是架设在局域网内的仓库服务，私服代理广域网上的远程仓库，供局域网内的 Maven 用户使用。当 Maven 需要下载构件的时候，它从私服请求，如果私服上不存在该构件，则从外部的远程仓库下载，缓存在私服上之后，再为 Maven 的下载请求提供服务。我们还可以把一些无法从外部仓库下载到的构件上传到私服上。 Maven 私服的 个特性： 1. 节省自己的外网带宽：减少重复请求造成的外网带宽消耗 2. 加速 Maven 构件：如果项目配置了很多外部远程仓库的时候，构建速度就会大大降低 3. 部署第三方构件：有些构件无法从外部仓库获得的时候，我们可以把这些构件部署到内部仓库 (私服) 中，供内部 maven 项目使用 4. 提高稳定性，增强控制：Internet 不稳定的时候，maven 构建也会变的不稳定，一些私服软件还提供了其他的功能 5. 降低中央仓库的负荷：maven 中央仓库被请求的数量是巨大的，配置私服也可以大大降低中央仓库的压力 因此我们在实际的项目中通常使用私服来间接访问中央仓库，项目通常不直接访问中央仓库 三、环境搭建 首先最新版本 2.97 只支持 java1.8，我们需要将 jdk 版本设置为 1.8 tomcat 的版本最好也是 8.0.x 版本，如果使用 8.5 可能会有问题 系统我们使用 Centos7 3.1 配置 jdk 环境$ wget http://download.oracle.com/otn-pub/java/jdk/8u144-b01/090f390dda5b47b9b721c7dfaa008135/jdk-8u144-linux-x64.tar.gz $ tar zxf jdk-8u91-linux-x64.tar.gz -C /usr/local/ $ ln –s /usr/local/jdk1.8.0_91 /usr/local/jdk $ vim /etc/profile export JAVA_HOME=/usr/local/jdk export CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar export PATH=$JAVA_HOME/bin:$PATH 检查 $ java -version java version &quot;1.8.0_151&quot; Java(TM) SE Runtime Environment (build 1.8.0_151-b12) Java HotSpot(TM) 64-Bit Server VM (build 25.151-b12, mixed mode) 3.2 安装 Jenkins提示：首先 Jenkins 安装方式有 2 中，一种是 yum 安装，另一种是使用 war 的方式进行安装（war 就需要安装 tomcat） 官方文档：https://pkg.jenkins.io/redhat/ 如果我们想使用 war 包的方式可以直接下载 war 包 这里我们可以参考本站以前文章 持续集成之 Jenkins+Gitlab 简介 [一] 下载 tomcat （tomcat 和 jdk 版本最好相同） $ wget http://mirrors.hust.edu.cn/apache/tomcat/tomcat-8/v8.0.48/bin/apache-tomcat-8.0.48.tar.gz $ tar xf apache-tomcat-8.0.48.tar.gz –C /application/ $ mv /application/apache-tomcat-8.0.48 /application/jenkins $ rm –rf /application/jenkins/webapps/* &amp;&amp; mkdir –p /application/jenkins/webapps/ROOT 下载 war 包 $ wget http://mirrors.jenkins.io/war/latest/jenkins.war $ cp jenkins.war /application/jenkins/webapps/ROOT/ $unzip /application/jenkins/webapps/ROOT/jenkins.war 我们直接执行 bin/startup.sh 启动就可以 启动我们可以查看 tomcat 日志 Jenkins 访问地址：localhost:8080 关于 tomcat 安装参数及配置修改可以参考本站 企业必会技能 tomcat 新版本的 jenkins 为了保证安全，在安装之后有一个锁，需要设置密码之后才可以解锁 我们选择推荐安装即可 安装插件中 设置管理员账号密码 登陆 jenkins 3.2 安装 maven 环境$ wget http://mirrors.tuna.tsinghua.edu.cn/apache/maven/maven-3/3.5.2/binaries/apache-maven-3.5.2-bin.tar.gz $ tar xf apache-maven-3.5.2-src.tar.gz $ mv apache-maven-3.5.2 /usr/local/ $ ln -s /usr/local/apache-maven-3.5.2/ /usr/local/maven $ vim /etc/profile export M2_HOME=/usr/local/maven export PATH=${M2_HOME}/bin:$PATH $source /etc/profile 验证 $ mvn -v Apache Maven 3.5.2 (138edd61fd100ec658bfa2d307c43b76940a5d7d; 2017-10-18T03:58:13-04:00) Maven home: /usr/local/maven Java version: 1.8.0_151, vendor: Oracle Corporation Java home: /usr/local/jdk1.8.0_151/jre Default locale: en_US, platform encoding: UTF-8 OS name: &quot;linux&quot;, version: &quot;3.10.0-327.el7.x86_64&quot;, arch: &quot;amd64&quot;, family: &quot;unix&quot; 这里我们需要修改 maven 的 settings.xml 设置一些相关配置。这里我们直接访问 https://www.abcdocker.com/abcdocker/2893 3.3 安装私服(Nexus) 下载地址：http://www.sonatype.org/nexus/go/ $ wget https://sonatype-download.global.ssl.fastly.net/nexus/3/nexus-3.7.0-04-unix.tar.gz $ tar xf nexus-3.7.0-04-unix.tar.gz -C /usr/local/ $ ln -s /usr/local/nexus-3.7.0-04/ /usr/local/nexus 设置环境变量 $ vim /etc/profile export JAVA_HOME=/usr/local/jdk export CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar export PATH=$JAVA_HOME/bin:$PATH export JENKINS_HOME=/jenkins export M2_HOME=/usr/local/maven export PATH=${M2_HOME}/bin:$PATH:/usr/local/nexus/bin $ source /etc/profile 启动脚本 $ nexus WARNING: ************************************************************ WARNING: Detected execution as &quot;root&quot; user. This is NOT recommended! WARNING: ************************************************************ Usage: /usr/local/nexus/bin/nexus {start|stop|run|run-redirect|status|restart|force-reload} $ nexus start WARNING: ************************************************************ WARNING: Detected execution as &quot;root&quot; user. This is NOT recommended! WARNING: ************************************************************ Starting nexus 如果我们想把警告去除，需要在修改用户和环境变量。 访问地址：localhost:8081 端口可以在 /etc/nexus-default.properties 中修改） nexus 登陆界面 3.4 Jenkins 配置 因为我们需要构建 Java 项目，所以需要安装一个 Maven 插件 插件名称 Maven Integration plugin系统管理–&gt; 管理插件 此时我们可以在已安装的插件中找到 配置 Jenkins 全局工具配置 系统管理 --&gt; 全局工具配置 配置我们的 JDK、Maven 地址保存就可以 四、Jenkins 构建项目 4.1 创建 maven 项目 创建 maven 项目，起名称 4.2 设置构建参数 这里是说我们构建的记录保留的天数与个数 SVN 地址以及账户的配置 没有问题就不会报错 这是 maven 的编译参数，如果有问题需要与研发的童鞋商议 添加 Shell 脚本，添加的 shell 脚本可以是命令，也可以是执行一个脚本。 构建演示： 这里是正在下载依赖包，因为我们项目一般在测试环境使用，是很多研发一起使用，所以这里的地址就是我们私服(Nexus 地址) 执行完毕 当我们执行完成之后上面的 shell 脚本可以是将 war 包复制到 tomcat 项目目录里 /jenkins/workspace/maven/bxg-ask-center-web/target–jenkins 主目录—项目目录—- 代码分支—– 以下是我们以前 Jenkins shell 中的配置，比较 low 仅供参考 提示：很多相关的参数不是运维能决定的，需要研发参与 更改 Jenkins 的主目录 https://www.cnblogs.com/zz0412/p/jenkins_jj_07.html 如何用 Maven 创建 web 项目（具体步骤） https://www.cnblogs.com/apache-x/p/5673663.html Maven 私服 Nexus 详解 http://blog.csdn.net/u013516966/article/details/43753143 maven 核心，pom.xml 详解(转) https://www.cnblogs.com/qq78292959/p/3711501.html 转自：Jenkins+Maven+SVN+Nexus 搭建持续集成环境]]></content>
      <categories>
        <category>Jenkins</category>
      </categories>
      <tags>
        <tag>Jenkins</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[持续集成 + 自动化部署 [代码流水线管理及 Jenkins 和 gitlab 集成]]]></title>
    <url>%2F2017%2F03%2F22%2F%E6%8C%81%E7%BB%AD%E9%9B%86%E6%88%90%2B%E8%87%AA%E5%8A%A8%E5%8C%96%E9%83%A8%E7%BD%B2%5B%E4%BB%A3%E7%A0%81%E6%B5%81%E6%B0%B4%E7%BA%BF%E7%AE%A1%E7%90%86%E5%8F%8AJenkins%E5%92%8Cgitlab%E9%9B%86%E6%88%90%5D%2F</url>
    <content type="text"><![CDATA[一、代码流水线管理 Pipeline 名词顾名思义就是流水线的意思，因为公司可能会有很多项目。如果使用 jenkins 构建完成后，开发构建项目需要一项一项点击，比较麻烦。所以出现 pipeline 名词。 代码质量检查完毕之后，我们需要将代码部署到测试环境上去，进行自动化测试 新建部署代码项目 点击新建 这里只需要写一下描述 执行 Shell 脚本 温馨提示：执行命令主要涉及的是权限问题，我们要搞明白，jenkins 是以什么权限来执行命令的。那么问题来了，我们现在 192.168.56.11 上，如果在想 192.168.56.12 上执行命令。需要怎么做呢？ 我们做无秘钥有 2 种分案： 1、使用 jenkins 用户将秘钥分发给 192.168.56.12 上 2、使用 root 用户将秘钥分发给 192.168.56.12 上，如果使用 root 用户还要进行 visudo 授权。因为 Web 上默认执行命令的用户是 jenkins 1. 我们使用 root 做密码验证 这里我们的 key 已经做好，如果没做可以直接 ssh-keygen -t ras 来生成秘钥 我们将 192.168.56.11 上的公钥复制到 192.168.56.12 上 [root@linux-node1 ~]# cat .ssh/id_rsa.pub ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQChVQufrGwqP5dkzIU4ZwXCjRuSvMVGN5lJdvL/QFckmlVphWMsQw06VsPhgcI1NDjGbKOh5pbjrylyJUCig5YIJ1xuMOZ2YAK32SceMxnVhEb/G4wNb9VMsGQ/Vs4CxrU1HdATktH9zDAV4Qz81x2POYJW5B5LAvwZ4owqnIpZ7o3ya6xBxEvCIMSVtD17oKrNqAphsg+e68KvRexiNCEbCbRGGq3bKevgiDsWpSGnCYsJC0+cSrUxuzEO3G6AqGI/qR3nOeg91rOsoAP3FpFjBKgb/sXggkwwjmGIqFXJrUG+XmczeF4kG/rUrNbdy84e5RyHoIS3XKnJuRjTxHyD root@linux-node1 [root@linux-node2 ~]# vim .ssh/authorized_keys ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQChVQufrGwqP5dkzIU4ZwXCjRuSvMVGN5lJdvL/QFckmlVphWMsQw06VsPhgcI1NDjGbKOh5pbjrylyJUCig5YIJ1xuMOZ2YAK32SceMxnVhEb/G4wNb9VMsGQ/Vs4CxrU1HdATktH9zDAV4Qz81x2POYJW5B5LAvwZ4owqnIpZ7o3ya6xBxEvCIMSVtD17oKrNqAphsg+e68KvRexiNCEbCbRGGq3bKevgiDsWpSGnCYsJC0+cSrUxuzEO3G6AqGI/qR3nOeg91rOsoAP3FpFjBKgb/sXggkwwjmGIqFXJrUG+XmczeF4kG/rUrNbdy84e5RyHoIS3XKnJuRjTxHyD root@linux-node1 [root@linux-node1 ~]# ssh 192.168.56.12 The authenticity of host &apos;192.168.56.12 (192.168.56.12)&apos; can&apos;t be established. ECDSA key fingerprint is b5:74:8f:f1:03:2d:cb:7d:01:28:30:12:34:9c:35:8c. Are you sure you want to continue connecting (yes/no)? yes Warning: Permanently added &apos;192.168.56.12&apos; (ECDSA) to the list of known hosts. Last login: Sat Dec 17 02:14:31 2016 from 192.168.56.1 [root@linux-node2 ~]# ll total 4 -rw-------. 1 root root 1021 Dec 13 05:56 anaconda-ks.cfg #现在 SSH 连接就不需要密码了 授权 jenkins 用户，使用 visudo 或者编辑配置文件 /etc/sudoers [root@linux-node1 ~]# vim /etc/sudoers 92 jenkins ALL=(ALL) NOPASSWD:/usr/bin/ssh #jenkins 授权所有主机，不需要密码执行 ssh。切记不要授权 ALL 我们在 192.168.56.12 上写一个简单 shell 脚本，检测是否可以执行成功。正式环境可以写一个自动化部署的脚本 [root@linux-node2 ~]# echo &quot;echo &quot;hello word&quot;&quot; &gt;demo.sh [root@linux-node2 ~]# chmod +x demo.sh [root@linux-node2 ~]# ll demo.sh -rwxr-xr-x 1 root root 16 Dec 17 02:24 demo.sh jenkins 编写执行脚本 然后我们点击立即构建 成功！ 现在我们要将代码质量管理和测试部署连接起来。 这时候就用到了 git 钩子 我们需要安装 jenkins 插件parameterized 我们选择demo-deploy 再次点击项目设置的时候就会出现Trigger parameterized build on other projects 提示 ：Projects to build 是为构建设置一个项目。例如我们想构建完代码项目后执行测试的，这里就填写测试的就可以。 最后点击保存，点击构建。我们查看效果 这样我们每次点击 demo-deploy 它就会在构建完成之后在对auto-deploy 进行构建 下载 pipeline。这样只需要构建一个项目，就会帮我们完成所有相关项目 搜索插件 pipeline 等待安装完成 我们点击首页 + 号，新建一个试图 点击 OK pipeline 配置 然后我们点击保存 pipeline 视图如下： 点击 Run 这样就先代码质量进行管理，然后就开始部署了 构建成功后： 这样我们下次想看 pipeline 视图的时候，点击上面的 demo-pipeline 即可 二、Jenkins + gitlab 集成 Jenkins + gitlab 集成后，实现的功能是 开发写好代码提交至 gitlab 上，当时开始 push 到 gitlab 上之后，jenkins 自动帮我们立即构建 这个项目我们需要安装一个 gitlab 钩子 的脚本 提示： jenkins 不论想实现什么功能，都需要安装插件！！ 安装完插件之后我们就开始配置钩子脚本 这里需要我们在服务器里面写一个令牌，在 jenkins 上也写一个令牌。这两个可以连接到一起就可以。 # 因为用到了令牌我们还需要在安装一个插件，否则将无法使用。因为令牌是需要登录之后才会有，所以需要有一个管理的插件 插件搜索：Build Aut 为了令牌的安全性，我们使用 openssl 生成一个 [root@linux-node1 ~]# openssl rand -hex 10 0a37c6d7ba1fe3472e26 然后我们点击保存即可 因为 jenkins 上也提示我们需要在 gitlab 上添加钩子脚本 点击我们创建的项目 选中 Webhooks Build Authorization Token Root Plugin 插件使用说明https://wiki.jenkins-ci.org/display/JENKINS/Build+Token+Root+Plugin 使用 Build 插件后，url 如下： http://192.168.56.11:8080/buildByToken/build?job=auto-deploy&amp;token=0a37c6d7ba1fe3472e26 auto-deploy= 项目名称(构建时的项目名称) 0a37c6d7ba1fe3472e26=jenkins 填写的令牌 然后点击Add Webhook 下方就会出现我们这个选项，我们点击 Test 进行测试 测试结果 向 git 服务器提交代码，验证是否可以自动部署： [root@linux-node1 ~]# echo &quot;Build Token Root Plugin&quot; &gt; index.html [root@linux-node1 ~]# git add index.html [root@saltmaster ~/weather]# git commit -m &quot;text&quot; [root@linux-node1 ~]# git push origin master jenkins 服务器的日志记录： [root@linux-node1 ~]# tail -f /var/log/jenkins/jenkins.log jenkins 项目构建： 访问 web 界面验证代码是否最新的：jenkins 控制台输出信息： 转自：持续集成 + 自动化部署[代码流水线管理及 Jenkins 和 gitlab 集成]]]></content>
      <categories>
        <category>Jenkins</category>
      </categories>
      <tags>
        <tag>Jenkins</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[持续集成之代码质量管理 -Sonar [三]]]></title>
    <url>%2F2017%2F03%2F21%2F%E6%8C%81%E7%BB%AD%E9%9B%86%E6%88%90%E4%B9%8B%E4%BB%A3%E7%A0%81%E8%B4%A8%E9%87%8F%E7%AE%A1%E7%90%86-Sonar%20%5B%E4%B8%89%5D%2F</url>
    <content type="text"><![CDATA[摘要 Sonar 是一个用于代码质量管理的开放平台。通过插件机制，Sonar 可以集成不同的测试工具，代码分析工具，以及持续集成工具。与持续集成工具（例如 Hudson/Jenkins 等）不同，Sonar 并不是简单地把不同的代码检查工具结果（例如 FindBugs，PMD 等）直接显示在 Web 页面上，而是通过不同的插件对这些结果进行再加工处理，通过量化的方式度量代码质量的变化，从而可以方便地对不同规模和种类的工程进行代码质量管理。 Sonar 介绍 Sonar 是一个用于代码质量管理的开放平台。通过插件机制，Sonar 可以集成不同的测试工具，代码分析工具，以及持续集成工具。与持续集成工具（例如 Hudson/Jenkins 等）不同，Sonar 并不是简单地把不同的代码检查工具结果（例如 FindBugs，PMD 等）直接显示在 Web 页面上，而是通过不同的插件对这些结果进行再加工处理，通过量化的方式度量代码质量的变化，从而可以方便地对不同规模和种类的工程进行代码质量管理。 在对其他工具的支持方面，Sonar 不仅提供了对 IDE 的支持，可以在 Eclipse 和 IntelliJ IDEA 这些工具里联机查看结果；同时 Sonar 还对大量的持续集成工具提供了接口支持，可以很方便地在持续集成中使用 Sonar。 此外，Sonar 的插件还可以对 Java 以外的其他编程语言提供支持，对国际化以及报告文档化也有良好的支持。 Sonar 部署 Sonar 的相关下载和文档可以在下面的链接中找到：http://www.sonarqube.org/downloads/。需要注意最新版的 Sonar 需要至少 JDK 1.8 及以上版本。 上篇文章我们已经可以成功的使用 git 进行拉去，Sonar 的功能就是来检查代码是否有 BUG。除了检查代码是否有 bug 还有其他的功能，比如说：你的代码注释率是多少，代码有一些建议，编写语法的建议。所以我们叫质量管理 Sonar 还可以给代码打分，并且引用了技术宅的功能（告诉你有很多地方没改） Sonar 部署[root@linux-node1 ~]# yum install -y java-1.8.0 [root@linux-node1 ~]# cd /usr/local/src 软件包我们通过 wget 或者下载，rz 上传到服务器 #软件包下载：https://sonarsource.bintray.com/Distribution/sonarqube/sonarqube-5.6.zip [root@linux-node1 src]# unzip sonarqube-5.6.zip [root@linux-node1 src]# mv sonarqube-5.6 /usr/local/ [root@linux-node1 src]# ln -s /usr/local/sonarqube-5.6/ /usr/local/sonarqube 准备 Sonar 数据库 如果没有数据库请执行yum install -y mariadb mariadb-server [root@linux-node1 ~]# systemctl start mariadb [root@linux-node1 ~]# systemctl enable mariadb Created symlink from /etc/systemd/system/multi-user.target.wants/mariadb.service to /usr/lib/systemd/system/mariadb.service. [root@linux-node1 ~]# mysql_secure_installation [root@linux-node1 ~]# mysql -uroot -p123456 特别提示： sonar 好像不支持mysql 5.5，所以如果看日志出现以上 error 请安装mysql 5.6 或者更高版本http://blog.csdn.net/onothing12345/article/details/49910087 执行 sql 语句 mysql&gt; CREATE DATABASE sonar CHARACTER SET utf8 COLLATE utf8_general_ci; mysql&gt; GRANT ALL ON sonar.* TO &apos;sonar&apos;@&apos;localhost&apos; IDENTIFIED BY &apos;sonar@pw&apos;; mysql&gt; GRANT ALL ON sonar.* TO &apos;sonar&apos;@&apos;%&apos; IDENTIFIED BY &apos;sonar@pw&apos;; mysql&gt; FLUSH PRIVILEGES; 配置 Sonar[root@linux-node1 ~]# cd /usr/local/sonarqube/conf/ [root@linux-node1 conf]# ls sonar.properties wrapper.conf 编写配置文件，修改数据库配置 [root@linux-node1 conf]# vim sonar.properties #我们只需要去配置文件里面修改数据库的认证即可 14 sonar.jdbc.username=sonar# 数据库用户 15 sonar.jdbc.password=sonar@pw #数据库密码 23 sonar.jdbc.url=jdbc:mysql://localhost:3306/sonar?useUnicode=true&amp;characterEncoding=utf8&amp;rewriteBatchedStatements=true&amp;useConfigs=maxPerformance 配置 Java 访问数据库驱动 (可选) 默认情况 Sonar 有自带的嵌入的数据库，那么你如果使用类是 Oracle 数据库，必须手动复制驱动类到${SONAR_HOME}/extensions/jdbc-driver/oracle/ 目录下，其它支持的数据库默认提供了驱动。其它数据库的配置可以参考官方文档：http://docs.sonarqube.org/display/HOME/SonarQube+Platform 启动 Sonar 你可以在 Sonar 的配置文件来配置 Sonar Web 监听的 IP 地址和端口，默认是 9000 端口。 [root@linux-node1 conf]# vim sonar.properties 99 #sonar.web.host=0.0.0.0 106 #sonar.web.port=9000 启动命令如下： [root@linux-node1 ~]# /usr/local/sonarqube/bin/linux-x86-64/sonar.sh start Starting SonarQube... Started SonarQube. 如果有什么问题可以看一下日志[/usr/local/sonarqube/logs/sonar.log] 检查是否有相应的端口 [root@linux-node1 ~]# netstat -lntup Active Internet connections (only servers) Proto Recv-Q Send-Q Local Address Foreign Address State PID/Program name tcp0 0 127.0.0.1:8080 0.0.0.0:* LISTEN 2239/unicorn master tcp0 0 0.0.0.0:80 0.0.0.0:* LISTEN 505/nginx: master p tcp0 0 0.0.0.0:22 0.0.0.0:* LISTEN 569/sshd tcp0 0 127.0.0.1:250.0.0.0:* LISTEN 971/master tcp0 0 127.0.0.1:43163 0.0.0.0:* LISTEN 5205/java tcp0 0 0.0.0.0:80600.0.0.0:* LISTEN 505/nginx: master p tcp0 0 127.0.0.1:32000 0.0.0.0:* LISTEN 4925/java tcp0 0 0.0.0.0:43044 0.0.0.0:* LISTEN 4952/java tcp0 0 0.0.0.0:33350 0.0.0.0:* LISTEN 5205/java tcp0 0 0.0.0.0:90000.0.0.0:* LISTEN 5011/java tcp0 0 0.0.0.0:33385 0.0.0.0:* LISTEN 5011/java tcp0 0 127.0.0.1:9001 0.0.0.0:* LISTEN 4952/java tcp6 0 0 :::3306 :::*LISTEN 4658/mysqld tcp6 0 0 :::34993:::*LISTEN 2348/java tcp6 0 0 :::8081 :::*LISTEN 2348/java tcp6 0 0 :::22 :::*LISTEN 569/sshd tcp6 0 0 ::1:25 :::*LISTEN 971/master udp6 0 0 :::33848:::*2348/java udp6 0 0 :::5353 :::*2348/java #端口是 9000 哦！ Web 登陆：IP:9000 提示： sonar 跟 jenkins 类似，也是以插件为主 sonar 安装插件有 2 种方式：第一种将插件下载完存放在 sonar 的插件目录，第二种使用 web 界面来使用安装 存放插件路径[/usr/local/sonarqube/extensions/plugins/] 安装中文插件 登陆：用户名：admin 密码：admin 需要重启才会生效 生效后如下图： 我们在安装一个 php 语言 温馨提示：如果下载不下来我们直接去 github 进行下载，因为我们这个插件都是使用 wget 进行下载的 我们现在只能使用 java 的 jar 包和 php，因为我们只安装了 java 和 php 的语言插件。如果想使用 Python 的程序，就需要安装 Python 的语言插件 Sonar 插件 ---&gt; 语言插件 （分析什么语言，你就需要安装什么语言的插件） Sonar 通过 SonarQube Scanner（扫描器）来对代码进行分析 官方文档：http://docs.sonarqube.org/display/SCAN/Analyzing+with+SonarQube+Scanner 下载扫描器插件 [root@linux-node1 ~]# wget https://sonarsource.bintray.com/Distribution/sonar-scanner-cli/sonar-scanner-2.8.zip [root@linux-node1 ~]# unzip sonar-scanner-2.8.zip [root@linux-node1 ~]# mv sonar-scanner-2.8 /usr/local/ [root@linux-node1 ~]# ln -s /usr/local/sonar-scanner-2.8/ /usr/local/sonar-scanner 我们要将扫描器和 sonar 关联起来 [root@linux-node1 ~]# cd /usr/local/sonar-scanner [root@linux-node1 sonar-scanner]# ls bin conf lib [root@linux-node1 sonar-scanner]# cd conf/ [root@linux-node1 conf]# ls sonar-scanner.properties [root@linux-node1 conf]# vim sonar-scanner.properties sonar.host.url=http://localhost:9000#sonar 地址 sonar.sourceEncoding=UTF-8 #字符集 sonar.jdbc.username=sonar# 数据库账号 sonar.jdbc.password=sonar@pw #数据库密码 sonar.jdbc.url=jdbc:mysql://localhost:3306/sonar?useUnicode=true&amp;amp;characterEncoding=utf8# 数据库连接地址 #打开注释即可 我们现在需要找一个代码进行分析。 sonar 插件提供了一个代码的库 github:https://github.com/SonarSource/sonar-examples 我们下载软件包：https://codeload.github.com/SonarSource/sonar-scanning-examples/zip/master 解压 [root@linux-node1 src]# unzip sonar-examples-master.zip [root@linux-node1 php]# cd sonar-examples-master/projects/languages/php [root@linux-node1 php]# cd php-sonar-runner-unit-tests/ [root@linux-node1 php-sonar-runner-unit-tests]# ll total 8 -rw-r--r-- 1 root root 647 Dec 14 09:57 README.md drwxr-xr-x 2 root root 51 Dec 14 09:57 reports -rw-r--r-- 1 root root 346 Dec 14 09:57 sonar-project.properties drwxr-xr-x 3 root root 31 Dec 14 09:57 src drwxr-xr-x 2 root root 25 Dec 14 09:57 tests #这里就是 PHP 的目录 配置文件解释： 如果你想让我扫描，就需要在代码路径下放一个配置文件 [root@linux-node1 php-sonar-runner-unit-tests]# cat sonar-project.properties sonar.projectKey=org.sonarqube:php-ut-sq-scanner #Key sonar.projectName=PHP :: PHPUnit :: SonarQube Scanner #这里的名称会显示在一会的 web 界面上 sonar.projectVersion=1.0# 版本，这里的版本一会也会显示在 web 界面上 sonar.sources=src #软件包存放路径 sonar.tests=tests sonar.language=php #语言 sonar.sourceEncoding=UTF-8 #字体 # Reusing PHPUnit reports sonar.php.coverage.reportPath=reports/phpunit.coverage.xml sonar.php.tests.reportPath=reports/phpunit.xml 也就是说在项目里面必须有这个配置文件才可以进行扫描 扫描 提示：需要在项目文件里面进行执行 [root@linux-node1 php-sonar-runner-unit-tests]# /usr/local/sonar-scanner/bin/sonar-scanner INFO: Scanner configuration file: /usr/local/sonar-scanner/conf/sonar-scanner.properties INFO: Project root configuration file: /usr/local/src/sonar-examples-master/projects/languages/php/php-sonar-runner-unit-tests/sonar-project.properties INFO: SonarQube Scanner 2.8 INFO: Java 1.8.0_111 Oracle Corporation (64-bit) INFO: Linux 3.10.0-514.2.2.el7.x86_64 amd64 INFO: User cache: /root/.sonar/cache INFO: Load global repositories INFO: Load global repositories (done) | time=211ms WARN: Property &apos;sonar.jdbc.url&apos; is not supported any more. It will be ignored. There is no longer any DB connection to the SQ database. WARN: Property &apos;sonar.jdbc.username&apos; is not supported any more. It will be ignored. There is no longer any DB connection to the SQ database. WARN: Property &apos;sonar.jdbc.password&apos; is not supported any more. It will be ignored. There is no longer any DB connection to the SQ database. INFO: User cache: /root/.sonar/cache INFO: Load plugins index INFO: Load plugins index (done) | time=3ms INFO: Download sonar-csharp-plugin-5.0.jar INFO: Download sonar-java-plugin-3.13.1.jar INFO: Download sonar-l10n-zh-plugin-1.11.jar INFO: Plugin [l10nzh] defines &apos;l10nen&apos; as base plugin. This metadata can be removed from manifest of l10n plugins since version 5.2. INFO: Download sonar-scm-git-plugin-1.2.jar INFO: Download sonar-php-plugin-2.9.1.1705.jar INFO: Download sonar-scm-svn-plugin-1.3.jar INFO: Download sonar-javascript-plugin-2.11.jar INFO: SonarQube server 5.6 INFO: Default locale: &quot;en_US&quot;, source code encoding: &quot;UTF-8&quot; INFO: Process project properties INFO: Load project repositories ................................................. ................................................. 提示：我们什么都不指定就会在当面目录下扫描 sonar-project.properties 文件，根据配置文件进行扫描工作。扫描之后我们在 web 界面上就可以看到代码的扫描结果 这里的名字，版本 都是在 sonar-project.properties 文件中定义的 质量阈帮我们设定好一个阈值，超过相应的阈值就算有bug 为了让 jenkins 可以在构建项目的时候执行 sonar，所以我们需要在 jenkins 上安装插件 现在就可以进行配置，让 jenkins 和sonar结合在一起。这样我们构建项目的时候就会进行代码检测 点击保存 配置 编辑我们的项目，选择最下放。找到构建 对 PHP 文件进行复制 [root@linux-node1 php-sonar-runner-unit-tests]# cat /usr/local/src/sonar-examples-master/projects/languages/php/php-sonar-runner-unit-tests/sonar-project.properties sonar.projectKey=org.sonarqube:php-ut-sq-scanner sonar.projectName=PHP :: PHPUnit :: SonarQube Scanner sonar.projectVersion=1.0 sonar.sources=src sonar.tests=tests sonar.language=php sonar.sourceEncoding=UTF-8 # Reusing PHPUnit reports sonar.php.coverage.reportPath=reports/phpunit.coverage.xml sonar.php.tests.reportPath=reports/phpunit.xml Analysis properties 分析的参数 填写完毕后，我们点击保存 我们选择立即构建 提示：此时的 SonarQube 是无法点击的 点击 Console Output 可以查看构建输出的内容 # 提示：只要没有 error 就可以 构建完成后，我们发现这里的 SonarQube 可以点击，我们点击 SonarQube 就会链接到 192.168.56.11:9000 就是代码查看器的地址 现在我们已经做到了可以在 git 上进行拉取代码。并进行检测 我们还可以配置一个构建失败发送邮箱： 在我们项目里面设置构建后操作，选择E-mail Notification 温馨提示：使用 163 邮箱发送的通知被 163 服务器退回了，因此我将设置在 jenkins 的邮箱改成了 QQ 邮箱 QQ：邮箱需要设置如下： 1、需要开启 POPE3/SMTP 服务 2、在 jenkins 上配置的密码我们需要点击生成授权码进行使用 QQ 邮件默认会收到如下提示： 当再次构件成功时，邮件内容如下： 转自：持续集成之代码质量管理 -Sonar [三]]]></content>
      <categories>
        <category>Jenkins</category>
      </categories>
      <tags>
        <tag>Jenkins</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[持续集成之 Jenkins+Gitlab 简介 [二]]]></title>
    <url>%2F2017%2F03%2F20%2F%E6%8C%81%E7%BB%AD%E9%9B%86%E6%88%90%E4%B9%8BJenkins%2BGitlab%E5%AE%9E%E7%8E%B0%E6%8C%81%E7%BB%AD%E9%9B%86%E6%88%90%20%5B%E4%BA%8C%5D%2F</url>
    <content type="text"><![CDATA[持续集成之 Jenkins+Gitlab 实现持续集成 [二]项目：使用 git+jenkins 实现持续集成 开始构建 General 源码管理 我们安装的是 git 插件，还可以安装 svn 插件 我们将 git 路径存在这里还需要权限认证，否则会出现error 我们添加一个认证 选择一下认证方式（我们可以在 系统管理 --&gt;Configure Credentials）里面进行设置 提示：gitlab 有一个 key，是我们用来做仓库的 key。拥有的权限是read-only 公钥我们需要在服务器上查看。 [root@linux-node1 ~]# ssh-keygen -t rsa Generating public/private rsa key pair. Enter file in which to save the key (/root/.ssh/id_rsa): Created directory &apos;/root/.ssh&apos;. Enter passphrase (empty for no passphrase): Enter same passphrase again: Your identification has been saved in /root/.ssh/id_rsa. Your public key has been saved in /root/.ssh/id_rsa.pub. The key fingerprint is: 5c:55:51:4e:a0:ad:1f:87:e0:96:9b:24:a3:09:68:62 root@linux-node2 The key&apos;s randomart image is: +--[RSA 2048]----+ |..++o| | . o o | | . o . .| | . . . . + . | | E o . S o * o .| | . o . o = + o | |o o . | | | | | +-----------------+ 在 192.168.56.11 部署的节点上，生成 key [root@linux-node1 ~]# cat .ssh/id_rsa.pub ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDWEDIIatngRx5NaqU6t+f6FvY2RqYp3V3u5CNJS6xAamGokQ3MnbsTv/V8yKy2TpvNcXsaXmqwQtOVSAO4BzltidMPxBJUQCqKdMRbPqpzo7ZqGCuLcCfHC8M6tSbr1AaHkLbow29YbCMyzCCkjDfRcOez8yHuLj5BSFpKYCjx2wpJxoZ/Z6J8Fslsyu7MaRMvUhBMAF6mqQaC1qZ6K4BMt0IpAuJvoL4dNu9P6KcnG3Wy2zrzoKzkFUi0xpKCmpYo2bq4zRXgAFAndp44j5iMKEavWPeRH0RHTGsfE5vU5/0CI9LCRjtp/3vTaYlBryq5vNXb2abCrJXWws0jwp6L root@linux-node2 我们设置完成后测试 git 是否可以拉去 [root@linux-node2 ~]# yum install git -y #如果没有 git 命令就安装一个 git 客户端 [root@linux-node1 ~]# git clone git@www.abcdocker.com:web/web-demo.git Cloning into &apos;web-demo&apos;... The authenticity of host &apos;www.abcdocker.com (192.168.56.11)&apos; can&apos;t be established. ECDSA key fingerprint is b5:74:8f:f1:03:2d:cb:7d:01:28:30:12:34:9c:35:8c. Are you sure you want to continue connecting (yes/no)? yes Warning: Permanently added &apos;www.abcdocker.com,192.168.56.11&apos; (ECDSA) to the list of known hosts. remote: Counting objects: 10, done. remote: Compressing objects: 100% (8/8), done. remote: Total 10 (delta 0), reused 0 (delta 0) Receiving objects: 100% (10/10), 70.00 KiB | 0 bytes/s, done. 私钥： [root@linux-node1 ~]# cat .ssh/ id_rsa id_rsa.pub known_hosts [root@linux-node1 ~]# cat .ssh/id_rsa -----BEGIN RSA PRIVATE KEY----- MIIEpQIBAAKCAQEAoVULn6xsKj+XZMyFOGcFwo0bkrzFRjeZSXby/0BXJJpVaYVj LEMNOlbD4YHCNTQ4xmyjoeaW468pciVAooOWCCdcbjDmdmACt9knHjMZ1YRG/xuM DW/VTLBkP1bOAsa1NR3QE5LR/cwwFeEM/NcdjzmCVuQeSwL8GeKMKpyKWe6N8mus QcRLwiDElbQ9e6CqzagKYbIPnuvCr0XsYjQhGwm0Rhqt2ynr4Ig7FqUhpwmLCQtP nEq1MbsxDtxugKhiP6kd5znoPdazrKAD9xaRYwSoG/7F4IJMMI5hiKhVya1Bvl5n M3heJBv61KzW3cvOHuUch6CEt1ypybkY08R8gwIDAQABAoIBAQCXVnTZ6t9oXlDB EI1jlFi14LJd2tBfhuY3IOrfgFZ+knvOyX53VcrB0ARdtOAeEoezstNomysuF/EU D1frWu5RZcLx5tM5deT22zAzxxHT1grXYdrl++Ml1k2jkOUde5MeaYH36oErx+/P hlYtlAk5gmP+6Gx2Ry1/hqGfk0rBAmY/eazqpT5hc1ANuW5dCmdQ5pqHog8CwH+K YnhKNUaW0VMqzWg9y3cQc8tlQItWUAsjl4+l/rSdOxsC9lTtuJZfMPIlrtLPi6tg tfjpX+N4zRbSwVblrD6mXOcKmAPbnuvLhyIBnBmDXeAHKCEnOYJ8eEJ6rT+GRjc8 aDvzsLmhAoGBAM0qj6lqdY4ZpHjCd7hJzGIitLBqsqRmHWgs9ymLIFQ+Z8LYI2HQ 1xja/oUfMkAnArcjz+q+gpDinC+oOVAnr4FQWB+lUdlMzzuE6OtYyWYYzjHpdTbO j4tHgqkOraiuRy1TanjgAJJSwR6oTwnBIC8PjEHa3o8xslVuexOobh1TAoGBAMlO JUHMMVmgxDaZq0c50Bn/r/k57QGj87E9mEbJeqBs+8fcxZoOFLEEd+Sb8Q1riqV9 12L2BAc6EoypoPUydbt0Q5/1G2VvCN1a6G43Ip7QM1cUTPrp17fvHWVSAMdq7lIr ntabqmtZVGqcxedmG1N7BVNXBd4Jy5HjOZ8Qfg4RAoGBAMyX5s9hNH1SIOuzscN7 BG/QgDN1E1RR6H1cadVpwgGAgeSRuSbwJa/JowqJg4jp3hFXix1igb2N3YbA0PaX vLLNtjNInwh9SiLmdYdL8Pr5PZYUYykWb5rK4wdHdfHCaYRPrNuBNdC06ZRy7u6h QkDr1khNxKczPc1n8SA3VCe1AoGAYdWb39WIaoHquoqGppAfZnNQp/SSDkkLR6mi 10xWT5+H4oOWeZ+8SKfeSPnM9nO8p194jXz5SjXcDAbo1iIW++qubxAlp2+GRGZJ Lj+XkM2pFfoky5FYqOkKRVLMVB7RAph2kuCGu7NnhoT43dRPFYxlczKJBHeIOzfO qlLOoLECgYEAkexlwKGeXyJj481SfqCYhjiTjCiibx/s6yS2cmamgEKOZCB2osmq 3m9PvOAp26Sm1ISiuINNbpLY3Gi5fEvNUSyRx8HzRXP2fydvdgpltDxJUPaUVxvn X46F8ewsMJ7/FDLSyjdzwvoDRvKCk99OBmGmofqh5zW0GrjcQjthmbk= -----END RSA PRIVATE KEY----- [root@linux-node1 ~]# 刚刚返回刚刚的区域，继续配置 现在我们复制 git 的url就不会出现验证提示 我们选择 gitlab，url 如下图 查看 gitlab 版本 [root@linux-node1 ~]# rpm -qa|grep gitlab gitlab-ce-8.14.5-ce.0.el7.x86_64 我们现在就添加了一个 git 仓库，现在保存就可以了！ 保存完毕后，我们选择立即构建 点击 Console Output 可以显示控制台的输出 现在基本就算是构建成功了 转自：持续集成之 Jenkins+Gitlab 实现持续集成 [二]]]></content>
      <categories>
        <category>Jenkins</category>
      </categories>
      <tags>
        <tag>Jenkins</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[持续集成之 Jenkins+Gitlab 简介 [一]]]></title>
    <url>%2F2017%2F03%2F20%2F%E6%8C%81%E7%BB%AD%E9%9B%86%E6%88%90%E4%B9%8BJenkins%2BGitlab%E7%AE%80%E4%BB%8B%20%5B%E4%B8%80%5D%2F</url>
    <content type="text"><![CDATA[摘要 DevOps（英文 Development（开发）和 Operations（技术运营）的组合）是一组过程、方法与系统的统称，用于促进开发（应用程序 / 软件工程）、技术运营和质量保障（QA）部门之间的沟通、协作与整合。它的出现是由于软件行业日益清晰地认识到：为了按时交付软件产品和服务，开发和运营工作必须紧密合作 持续集成之 Jenkins+Gitlab 简介 [一]持续集成概念 持续集成 Continuous Integration—-CI 持续交付 Continuous Delivery—-CD 持续部署 Continuous Deployment—–CD 1.1 什么是持续集成：持续集成是指开发者在代码的开发过程中，可以频繁的将代码部署集成到主干，并进程自动化测试 1.2 什么是持续交付：持续交付指的是在持续集成的环境基础之上，将代码部署到预生产环境 1.3 持续部署：在持续交付的基础上，把部署到生产环境的过程自动化，持续部署和持续交付的区别就是最终部署到生产环境是自动化的。 1.4 部署代码上线流程 1. 代码获取（直接了拉取） 2. 编译 （可选） 3. 配置文件放进去 4. 打包 5.scp 到目标服务器 6. 将目标服务器移除集群 7. 解压并放置到 Webroot 8.Scp 差异文件 9. 重启 （可选） 10. 测试 11. 加入集群 运维必知 OWASPJenkins 上 OWASP 插件介绍 ： 它是开放式 Web 应用程序安全项目[OWASP,Open Web Application Secunity Project] 它每年会出一个 top10 的安全漏洞，我们需要知道当前 top10 的漏洞有哪些 https://www.owasp.org/images/5/57/OWASP_Proactive_Controls_2.pdf https://www.owasp.org/index.php/Top_10_2013-Top_10 Gitlab 介绍 GitLab 是一个利用 Ruby on Rails 开发的开源应用程序，实现一个自托管的 Git 项目仓库，可通过 Web 界面进行访问公开的或者私人项目。 GitLab 拥有与 Github 类似的功能，能够浏览源代码，管理缺陷和注释。可以管理团队对仓库的访问，它非常易于浏览提交过的版本并提供一个文件历史库。它还提供一个代码片段收集功能可以轻松实现代码复用，便于日后有需要的时候进行查找。 环境准备[root@linux-node1 ~]# cat /etc/redhat-release CentOS Linux release 7.3.1611 (Core) [root@linux-node1 ~]# uname -r 3.10.0-514.2.2.el7.x86_64 下载 epel 源 [root@linux-node1 ~]# wget http://mirrors.aliyun.com/epel/epel-release-latest-7.noarch.rpm [root@linux-node1 ~]# wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo 关闭 NetworkManager 和防火墙 [root@linux-node1 ~]#systemctl stop firewalld.service systemctl disable firewalld systemctl disable NetworkManager 关闭 SELinux 并确认处于关闭状态 sed -i &apos;s/SELINUX=enforcing/SELINUX=disabled/&apos; /etc/selinux/config grep SELINUX=disabled /etc/selinux/config setenforce 0 更新系统并重启 [root@linux-node1 ~]# yum update -y &amp;&amp; reboot 我们一共有 2 台：192.168.56.11和 192.168.56.12 我们安装在 192.168.56.11 上 [root@linux-node1 /]# yum install curl policycoreutils openssh-server openssh-clients postfix -y [root@linux-node1 /]# systemctl start postfix [root@linux-node1 /]# curl -sS https://packages.gitlab.com/install/repositories/gitlab/gitlab-ce/script.rpm.sh | sudo bash [root@linux-node1 /]# yum install -y gitlab-ce 由于网络问题，国内用户，建议使用清华大学的镜像源进行安装 [root@linux-node1 ~]# cat /etc/yum.repos.d/gitlab-ce.repo [gitlab-ce] name=gitlab-ce baseurl=http://mirrors.tuna.tsinghua.edu.cn/gitlab-ce/yum/el7 repo_gpgcheck=0 gpgcheck=0 enabled=1 gpgkey=https://packages.gitlab.com/gpg.key [root@linux-node1 ~]# yum makecache [root@linux-node1 /]# yum install -y gitlab-ce 在安装一个 git 客户端 [root@linux-node1 /]# yum install -y git 配置并启动gitlab-ce [root@linux-node1 ~]# gitlab-ctl reconfigure #时间可能比较长，耐心你等待即可！---- gitlab 常用命令： 关闭 gitlab：[root@linux-node2 ~]# gitlab-ctl stop 启动 gitlab：[root@linux-node2 ~]# gitlab-ctl start 重启 gitlab：[root@linux-node2 ~]# gitlab-ctl restart 重载配置文件: gitlab-ctl reconfigure 可以使用 gitlab-ctl 管理gitlab，例如查看gitlab 状态： [root@linux-node1 /]# gitlab-ctl status run: gitlab-workhorse: (pid 7437) 324s; run: log: (pid 7436) 324s run: logrotate: (pid 7452) 316s; run: log: (pid 7451) 316s run: nginx: (pid 8168) 2s; run: log: (pid 7442) 318s run: postgresql: (pid 7293) 363s; run: log: (pid 7292) 363s run: redis: (pid 7210) 369s; run: log: (pid 7209) 369s run: sidekiq: (pid 7479) 265s; run: log: (pid 7426) 326s run: unicorn: (pid 7396) 327s; run: log: (pid 7395) 327s 提示： 我们要保证 80 端口不被占用 我们可以查看一下端口 [root@linux-node1 /]# gitlab-ctl restart ok: run: gitlab-workhorse: (pid 8353) 0s ok: run: logrotate: (pid 8360) 1s ok: run: nginx: (pid 8367) 0s timeout: down: postgresql: 0s, normally up, want up ok: run: redis: (pid 8437) 0s ok: run: sidekiq: (pid 8445) 0s ok: run: unicorn: (pid 8450) 0s [root@linux-node1 /]# lsof -i:80 COMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAME nginx 8367 root7u IPv4 54972 0t0 TCP *:http (LISTEN) nginx 8368 gitlab-www7u IPv4 54972 0t0 TCP *:http (LISTEN) Web：访问：192.168.56.11 提示：启动 gitlab 需要时间！ Web页面提示我们需要设置一个账号密码（我们要设置最少 8 位数 的一个账号密码）我们设置密码为：12345678 我们在后面的页面设置用户名 我们现在是以管理员的身份登陆 我们点击右上角管理区域 第一步：我们关闭自动注册，因为我们内部使用不需要用户自己注册，由运维分配用户即可 提示：Save在页面最下放！！！！！！ 记得点保存！！！！！！！！！！！！ 现在在查看首页就没有注册页面了 第二步：我们创建一个用户，在创建一个项目 先创建一个组 提示：gitlab 上面有一个项目跟组的概念，我们要创建一个组，才可以在创建一个项目。因为 gitlab 的路径上首先是 ip 地址，其次是组 点击下方Create group 然后我们在组里面创建项目 下一步： 创建完成之后它提示我们可以创建一个 key 对它进行管理 我们点击上面的 README 然后我们随便在里面写点东西 填写完成我们点击前面进行查看 我们要做免密验证，现在去 192.168.56.11 复制下面的.ssh/id_rsa.pub [www@linux-node1 ~]$ cat .ssh/id_rsa.pub ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQC8wfTSQcSyhlsGYDSUtuxZNb1Gl3VU56nAPuxAEF2wP2ZWZ2yva354ZdKOOb6rZx2yZxqy5XIj7opBJPbhraXap+NtCH5qWyktR7dH19RBmCS7vUGgvk/5RQC0mVFrC8cztBp0M/5HxMuhVir6mD1rhbDvvaLL6S5y4gljzC1Gr2VRHIb4Et9go/38c2tqMjYCike7WWbFRyL9wTal6/146+9uREZ/r69TBTKrGuRqF44fROQP8/ly02XFjlXyl6J5NnGTk6AU855pwasX0W9aNPce3Ynrpe1TBTubmfQhrH1BwteEmg+ZXNRupxjumA+tPWfBUX+u51r/w7W/d4PD www@linux-node1 #提示：需要提前做秘钥认证 设置Keys 添加完之后我们就可以使用 www 用户，就可以拉了 点击Projects 选择SSH，我们要将代码拉去下来 [www@linux-node1 ~]$ cd /deploy/code/ [www@linux-node1 code]$ ls web-demo [www@linux-node1 code]$ rm -rf web-demo/ [www@linux-node1 ~]$ git clone git@linux-node1:web/web-demo.git Cloning into &apos;web-demo&apos;... The authenticity of host &apos;linux-node1 (192.168.56.11)&apos; can&apos;t be established. ECDSA key fingerprint is b5:74:8f:f1:03:2d:cb:7d:01:28:30:12:34:9c:35:8c. Are you sure you want to continue connecting (yes/no)? yes Warning: Permanently added &apos;linux-node1&apos; (ECDSA) to the list of known hosts. remote: Counting objects: 3, done. remote: Compressing objects: 100% (2/2), done. remote: Total 3 (delta 0), reused 0 (delta 0) Receiving objects: 100% (3/3), done. [www@linux-node1 ~]$ ls web-demo/ README.md #git clone 是克隆的意思 我们来模拟开发继续写代码提交 [www@linux-node1 ~]$ mkdir -p /web-demo [www@linux-node1 ~]$ vim web-demo/index.html [www@linux-node1 ~]$ cd web-demo/ [www@linux-node1 web-demo]$ [www@linux-node1 web-demo]$ ls index.html README.md [www@linux-node1 web-demo]$ git add * [www@linux-node1 web-demo]$ git commit -m &quot;add index.html&quot; *** Please tell me who you are. Run git config --global user.email &quot;you@example.com&quot; git config --global user.name &quot;Your Name&quot; to set your account&apos;s default identity. Omit --global to set the identity only in this repository. fatal: empty ident name (for &lt;www@linux-node1.(none)&gt;) not allowed 需要身份验证： [www@linux-node1 web-demo]$ git config --global user.email &quot;you@example.com&quot; [www@linux-node1 web-demo]$ git config --global user.name &quot;Your Name&quot; [www@linux-node1 web-demo]$ git commit -m &quot;add index.html&quot; [master be8a547] add index.html 1 file changed, 169 insertions(+) create mode 100644 index.html git push命令用于将本地分支的更新，推送到远程主机。它的格式与 git pull 命令相仿。 [www@linux-node1 web-demo]$ git push warning: push.default is unset; its implicit value is changing in Git 2.0 from &apos;matching&apos; to &apos;simple&apos;. To squelch this message and maintain the current behavior after the default changes, use: git config --global push.default matching To squelch this message and adopt the new behavior now, use: git config --global push.default simple See &apos;git help config&apos; and search for &apos;push.default&apos; for further information. (the &apos;simple&apos; mode was introduced in Git 1.7.11. Use the similar mode &apos;current&apos; instead of &apos;simple&apos; if you sometimes use older versions of Git) Counting objects: 4, done. Delta compression using up to 4 threads. Compressing objects: 100% (3/3), done. Writing objects: 100% (3/3), 7.66 KiB | 0 bytes/s, done. Total 3 (delta 0), reused 0 (delta 0) To git@linux-node1:web/web-demo.git 0c1d357..be8a547 master -&gt; master 我们的 gitlab 安装在 opt/gitlabgitlab 配置文件 存放在etc/gitlab/gitlab.rb # 现在 git 需要加上主机名，我们可以修改配置文件，让它使用 IP 进行访问 编辑配置文件 [root@linux-node1 ~]# vim /etc/gitlab/gitlab.rb external_url &apos;http://192.168.56.11&apos; [root@linux-node1 ~]# gitlab-ctl reconfigure #提示：修改完需要使用 reconfigure 重载配置才会生效 我们从新登陆进行查看 咦！ 为啥还没改呢！ 我们从新创建一个项目在试一下 友情提示： 关于 Git 可以查看 徐布斯博客 or 廖雪峰 Git 自动化运维之 DevOps DevOps（英文 Development（开发 ）和 Operations（ 技术运营）的组合）是一组过程、方法与系统的统称，用于促进开发（应用程序 / 软件工程）、技术运营和质量保障（QA）部门之间的沟通、协作与整合。 它的出现是由于软件行业日益清晰地认识到：为了按时交付软件产品和服务，开发和运营工作必须紧密合作 简单的来说 DevOps 是一种文化，是让开发开发、运维、测试能够之间沟通和交流 自动化运维工具：saltstack、jenkins、等。因为他们的目标一样，为了我们的软件、构建、测试、发布更加的敏捷、频繁、可靠 如果运维对 git 不熟，是无法做自动化部署。因为所有的项目都受制于开发 Jenkins 介绍 Jenkins 只是一个平台，真正运作的都是插件。这就是 jenkins 流行的原因，因为 jenkins 什么插件都有 Hudson 是Jenkins的前身，是基于 Java 开发的一种持续集成工具，用于监控程序重复的工作，Hudson后来被收购，成为商业版。后来创始人又写了一个 jenkins，jenkins 在功能上远远超过hudson Jenkins 官网：https://jenkins.io/ 安装 安装 JDK Jenkins 是 Java 编写的，所以需要先安装 JDK，这里采用 yum 安装，如果对版本有需求，可以直接在 Oracle 官网下载 JDK。 [root@linux-node1 ~]# yum install -y java-1.8.0 安装 jenkins [root@linux-node1 ~]# cd /etc/yum.repos.d/ [root@linux-node1 yum.repos.d]# wget http://pkg.jenkins.io/redhat/jenkins.repo [root@linux-node1 ~]# rpm --import http://pkg.jenkins.io/redhat/jenkins.io.key [root@linux-node1 ~]# yum install -y jenkins [root@linux-node1 ~]# systemctl start jenkins #本文使用 yum 进行安装，大家也可以使用编译安装。 新版本的 jenkins 为了保证安全，在安装之后有一个锁，需要设置密码之后才可以解锁 Jenkins Web访问地址：192.168.56.11:8080 友情提示：jenkins 如果跟 gitlab 在一台服务器需要将 jenkins 的端口进行修改，需要将 jenkins 的 8080 修改为 8081 [root@linux-node1 ~]# cat /var/lib/jenkins/secrets/initialAdminPassword 490a2f35a2df49b6b8787ecb27122a3a 复制这个文件下面的 ID，否则不可以进行安装。 我们选择推荐安装即可 它会给我们安装一些基础的插件 设置用户名密码： 点击保存并退出 早期 jenkins 默认是不需要登陆的 我们先来介绍一下 jenkins 基础功能 我们点击 新建 这里就是构建一个项目 用户界面：主要是一些用户的管理 可以看到当前登陆用户及用户权限等 任务历史：可以查看到所有构建过的项目的历史 # 之所以叫构建，是因为都是 java，因为如果不是 java 程序就没有构建这个词。但是我们也可以把一些工作称之为构建 系统管理：存放 jenkins 所有的配置 My Views：视图功能，我们可以自己创建一个自己的视图 构建队列：如果当前有视图任务都会显示在这里 构建执行状态：显示在正构建的任务 系统管理：- 系统设置 设置 Jenkins 全局设置 &amp; 路径 Jenkins 系统管理比较重要的就是插件管理了 # 因为 jenkins 所有的东西都需要靠插件来完成， 点击已安装可以查看我们的安装 我们想安装什么插件，我们可以选择可选插件 我们为了和 gitlab 和在一起，我们需要安装一个插件 查看还可以去 jenkins 官网下载，然后上传插件 因为很多插件需要翻墙才可以继续下载，jenkins 还提供了代理的设置 还是在服务器目录下进行上传插件 目录路径 = /var/lib/jenkins/plugins/ 这个目录下是我们安装所有的插件 转自：持续集成之 Jenkins+Gitlab 简介 [一]]]></content>
      <categories>
        <category>Jenkins</category>
      </categories>
      <tags>
        <tag>Jenkins</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Zabbix-3.2.3 实现微信（WeChat）告警]]></title>
    <url>%2F2016%2F12%2F10%2FZabbix-3.2.3%E5%AE%9E%E7%8E%B0%E5%BE%AE%E4%BF%A1%EF%BC%88WeChat%EF%BC%89%E5%91%8A%E8%AD%A6%E3%80%90%E8%BD%AC%E3%80%91%2F</url>
    <content type="text"><![CDATA[摘要 Zabbix 可以通过多种方式把告警信息发送到指定人，常用的有邮件，短信报警方式，但是越来越多的企业开始使用 zabbix 结合微信作为主要的告警方式，这样可以及时有效的把告警信息推送到接收人，方便告警的及时处理。 Zabbix-3.2.3 实现微信（WeChat）告警 Zabbix 可以通过多种方式把告警信息发送到指定人，常用的有邮件，短信报警方式，但是越来越多的企业开始使用 zabbix 结合微信作为主要的告警方式，这样可以及时有效的把告警信息推送到接收人，方便告警的及时处理。 关于邮件报警可以参考：Zabbix Web 邮件报警 一、微信企业号申请 地址： https://qy.weixin.qq.com/ 第一步注册 提示：这里简单的说一下，微信企业号和微信公众号是不一样的！ 到邮件查看邮件，继续下一步 提示一下：注册以后就不可以修改微信号类型 我们选择注册团队 由于我已经注册了，下一步就不继续操作了 二、配置微信企业号 当我们设置完微信号的信息之后，请继续跟我操作 我们点击通讯录–&gt; 创建子部门–&gt; 运维组 提示： 我们需要记录运维组的 ID，用于脚本接收报警 我们点击运维–&gt; 添加成员 关于认证可以参考官方说明： 我们可以使用扫描二维码认证或者邀请认证 我们点击创建应用 选择消息型 设置组合用户，将运维整个组添加进去 设置完成之后如下图所示！提示：我们需要记录应用 ID，在接收邮件时会使用 设置权限，让运维组有查看的选项。管理员可以不进行设置 需要确定管理员有权限使用应用发送消息，需要管理员的 CorpID 和 Sercrt。（重要） 准备事项：微信企业号 企业号已经被部门成员关注 企业号有一个可以发送消息的应用，一个授权管理员，可以使用应用给成员发送消息 需要得到的信息 成员账号 组织部门 ID 应用 ID CorpID 和 Secret 三、修改 Zabbix.conf[root@abcdocker ~]# grep alertscripts /etc/zabbix/zabbix_server.conf AlertScriptsPath=/usr/lib/zabbix/alertscripts 我们设置 zabbix 默认脚本路径，这样在 web 端就可以获取到脚本 四、设置 python 脚本# 安装 simplejson wget https://pypi.python.org/packages/f0/07/26b519e6ebb03c2a74989f7571e6ae6b82e9d7d81b8de6fcdbfc643c7b58/simplejson-3.8.2.tar.gz tar zxvf simplejson-3.8.2.tar.gz &amp;&amp; cd simplejson-3.8.2 python setup.py build python setup.py install 下载 wechat.py 脚本 git clone https://github.com/X-Mars/Zabbix-Alert-WeChat.git cp Zabbix-Alert-WeChat/wechat.py /usr/lib/zabbix/alertscripts/ cd /usr/lib/zabbix/alertscripts/ chmod +x wechat.py &amp;&amp; chown zabbix:zabbix wechat.py 提示：这里需要修改 py 脚本 看注释，这就不解释了 [root@abcdocker ~]# cat /usr/lib/zabbix/alertscripts/wechat.py #!/usr/bin/python #_*_coding:utf-8 _*_ import urllib,urllib2 import json import sys import simplejson reload(sys) sys.setdefaultencoding(&apos;utf-8&apos;) def gettoken(corpid,corpsecret): gettoken_url = &apos;https://qyapi.weixin.qq.com/cgi-bin/gettoken?corpid=&apos; + corpid + &apos;&amp;corpsecret=&apos; + corpsecret print gettoken_url try: token_file = urllib2.urlopen(gettoken_url) except urllib2.HTTPError as e: print e.code print e.read().decode(&quot;utf8&quot;) sys.exit() token_data = token_file.read().decode(&apos;utf-8&apos;) token_json = json.loads(token_data) token_json.keys() token = token_json[&apos;access_token&apos;] return token def senddata(access_token,user,subject,content): send_url = &apos;https://qyapi.weixin.qq.com/cgi-bin/message/send?access_token=&apos; + access_token send_values = { &quot;touser&quot;:user,# 企业号中的用户帐号，在 zabbix 用户 Media 中配置，如果配置不正常，将按部门发送。 &quot;toparty&quot;:&quot;2&quot;,# 企业号中的部门 id。 &quot;msgtype&quot;:&quot;text&quot;, #消息类型。 &quot;agentid&quot;:&quot;2&quot;,# 企业号中的应用 id。 &quot;text&quot;:{&quot;content&quot;:subject + &apos;\n&apos; + content}, &quot;safe&quot;:&quot;0&quot; } #send_data = json.dumps(send_values, ensure_ascii=False) send_data = simplejson.dumps(send_values, ensure_ascii=False).encode(&apos;utf-8&apos;) send_request = urllib2.Request(send_url, send_data) response = json.loads(urllib2.urlopen(send_request).read()) print str(response) if __name__ == &apos;__main__&apos;: user = str(sys.argv[1]) #zabbix 传过来的第一个参数 subject = str(sys.argv[2]) #zabbix 传过来的第二个参数 content = str(sys.argv[3]) #zabbix 传过来的第三个参数 corpid = &apos;11111111111111&apos; #CorpID 是企业号的标识 corpsecret = &apos;222222222222222222&apos; #corpsecretSecret 是管理组凭证密钥 accesstoken = gettoken(corpid,corpsecret) senddata(accesstoken,user,subject,content) 执行 py 脚本，进行测试 [root@abcdocker alertscripts]# ./wechat.py www www 123 https://qyapi.weixin.qq.com/cgi-bin/gettoken?corpid=wx6dadb9cc293b793e&amp;corpsecret=JjesoeixbFt6dDur7_eXtamVBx2SjPBuXMQ0Jte3YLkz8l-VBnr0JvU12P0kvpGJ {u&apos;invaliduser&apos;: u&apos;all user invalid&apos;, u&apos;errcode&apos;: 0, u&apos;errmsg&apos;: u&apos;ok&apos;} 五、zabbix web 界面配置 创建报警媒介 创建报警用户 这里填写运维组 ID 设置报警动作 报警消息设置如下： hostname: ({HOST.NAME} Time:{EVENT.DATE} {EVENT.TIME} level:{TRIGGER.SEVERITY} message:{TRIGGER.NAME} event:{ITEM.NAME}:{ITEM.VALUE} url:www.abcdocker.com 恢复报警如下： hostname: ({HOST.NAME} Time:{EVENT.DATE} {EVENT.TIME} level:{TRIGGER.SEVERITY} message:{TRIGGER.NAME} event:{ITEM.NAME}:{ITEM.VALUE} url:www.abcdocker.com 报警配置如下 恢复配置如下 提示： 不要忘记先点小的add--&gt; 小的 update--&gt;Update 六、测试 为了验证效果我们停掉 zabbix-agent, 进行查看报警 [root@abcdocker ~]# systemctl stop zabbix-agent 报警如下 本文参考： Zabbix-3.0.3 实现微信（WeChat）告警 转自：Zabbix-3.2.3 实现微信（WeChat）告警]]></content>
      <categories>
        <category>zabbix</category>
      </categories>
      <tags>
        <tag>zabbix</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Zabbix-3.0.X 安装 Graphtree]]></title>
    <url>%2F2016%2F12%2F05%2FZabbix-3.0.X%20%E5%AE%89%E8%A3%85Graphtree%2F</url>
    <content type="text"><![CDATA[Zabbix-3.0.X 安装 Graphtreezabbix本文转载http://blog.csdn.net/wh211212/article/details/52735180 Zabbix 中，想要集中展示图像，唯一的选择是 screen，后来zatree 解决了 screen 的问题，但性能不够好。Graphtree 由 OneOaaS 开发并开源出来，用来解决 Zabbix 的图形展示问题，性能较好。 提示：zatree 不支持 3.x 和 3.2 暂时只支持 2.4 一、Graphtree 功能概述 ▲ 集中展示所有分组设备 ▲ 集中展示一个分组图像 ▲ 集中展示一个设备图像 ▲ 展示设备下的 Application ▲ 展示每个应用下的图像 ▲ 展示每个应用下的日志 ▲ 对原声无图的监控项进行绘图 注意事项： 主机和组级别下，默认只显示系统初始的图形 二、Zabbix 版本要求：3.0.x提示：暂时只支持 3.0 版本，根据测试不支持 3.2 版本 1、插件安装 Zabbix-web 目录 提示：如果是 yum 安装并且是 centos7 目录会在 /usr/share/zabbix 可以使用 find 进行查找 cd /usr/local/nginx/html/zabbix 下载 Graphtree 补丁包 wget https://raw.githubusercontent.com/OneOaaS/graphtrees/master/graphtree3-0-1.patch 安装 Linux 下打补丁命令 patch yum -y install patch 打补丁 patch -Np0 &lt; graphtree3-0-1.patch 三、Graphtree 效果图1、删除提示信息vim /usr/local/nginx/html/zabbix/graphtree.right.php 根据自己的路径进行修改 d7d #删除 344-350 行 2、重启载入 Zabbix-web，可以看到 Graphtree 已出效果 转自：Zabbix-3.0.X 安装 Graphtree]]></content>
      <categories>
        <category>zabbix</category>
      </categories>
      <tags>
        <tag>zabbix</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Zabbix_sender 介绍及配置]]></title>
    <url>%2F2016%2F11%2F28%2FZabbix_sender%E4%BB%8B%E7%BB%8D%E5%8F%8A%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[Zabbix_sender 是什么? 有什么作用？ zabbix 获取 key 值 有超时时间，如果自定义的 key 脚本一般需要执行很长时间，这根本没法去做监控，那怎么办呢？这时候就需要使用 zabbix 监控类型 zabbix trapper，配合zabbix_sender 给它传递数据。所以说 zabbix_sender 是更新 items 值 最快的方式 安装 在 centos5 上安装rpm -ivh http://mirrors.aliyun.com/zabbix/zabbix/3.0/rhel/5/x86_64/zabbix-sender-3.0.5-1.el5.x86_64.rpm 在 centos6 上安装 zabbix_senderrpm -ivh http://mirrors.aliyun.com/zabbix/zabbix/3.0/rhel/6/x86_64/zabbix-sender-3.0.5-1.el6.x86_64.rpm 在 centos7 上安装 zabbix_senderrpm -ivh http://mirrors.aliyun.com/zabbix/zabbix/3.0/rhel/7/x86_64/zabbix-sender-3.0.5-1.el7.x86_64.rpm 命令解释 zabbix_sender 命令详解 最简易使用方法一: zabbix_sender -z server -s host -k key -o value 最简易使用方法二： zabbix_sender -c config-file -k key -o value 最简易使用方法三： zabbix_sender -z server -i file 更多的使用方法可以man zabbix_sender 主要的使用参数 -c --config &lt;file&gt; zabbix_agent 配置文件绝对路径 -z --zabbix-server &lt;server&gt; zabbix server 的 IP 地址 -p --port &lt;server port&gt; zabbix server 端口. 默认 10051 -s --host &lt;hostname&gt; 主机名，与 zabbix_server web 上主机的 hostname 一致 例如 -I --source-address &lt;IP address&gt; 源 IP -k --key &lt;key&gt; 监控项的 key -o --value &lt;key value&gt;key 值 -i --input-file &lt;input file&gt; 从文件里面读取 hostname、key、value 一行为一条数据，使用空格作为分隔符，如果主机名带空格，那么请使用双引号包起来 -r --real-time 将数据实时提交给服务器 -v --verbose 详细模式, -vv 更详细 案例 下面：我们创建一个监控项item zabbix_sender -z 192.168.56.11 -s 192.168.56.100 -k login.users -o 111 如下图所示 检验 -o的值也可以引用命令： [root@muban ~]# zabbix_sender -z 192.168.56.11 -s 192.168.56.100 -k login.users -o $(w|sed &apos;1,2d&apos;|wc -l) 使用 zabbix_sender 批量发送 首先多准备几个 zabbix_trapper 类型的监控项 编写批量列表，每行以 hostname、key、value 的方式 [root@muban ~]# cat f.txt 192.168.56.100 login.users 12 192.168.56.100 login.users1 13 192.168.56.100 login.users2 14 192.168.56.100 login.users3 15 测试 zabbix_sender -z 192.168.56.11 -i f.txt 转自：Zabbix_sender 介绍及配置]]></content>
      <categories>
        <category>zabbix</category>
      </categories>
      <tags>
        <tag>zabbix</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Zabbix Web 邮件报警]]></title>
    <url>%2F2016%2F11%2F27%2FZabbix%20Web%20%E9%82%AE%E4%BB%B6%E6%8A%A5%E8%AD%A6%2F</url>
    <content type="text"><![CDATA[zabbix 3.0 版本系列会造成无法使用 smtp 进行报警我们可以使用邮件报警，可以参考文章Zabbix 使用脚本发送邮件 完整配置邮件步骤如下： 首先点击配置，选择报警媒介，点击邮件[Email] 提示：这里的密码不是 QQ 登陆密码，而是 QQ 邮箱的授权密码 设置收件人邮箱 设置发送邮件动作 此处注意如果没有开启需要开启，要确保状态是 Enabled 这里可以设置脚本内容，我们默认就行 测试 我自己服务器安装 zabbix 3.2(3.2 安装文档)，zabbix 3.2 有默认的网卡监控，我设置一个超过 10M 报警的动作。我们可以看邮件如下 报警邮件 恢复邮件 谁要在说发送不了邮件，我打死他！ 转自：Zabbix Web 邮件报警]]></content>
      <categories>
        <category>zabbix</category>
      </categories>
      <tags>
        <tag>zabbix</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Zabbix 3.0 自动化监控 [十]]]></title>
    <url>%2F2016%2F11%2F25%2FZabbix%203.0%20%E8%87%AA%E5%8A%A8%E5%8C%96%E7%9B%91%E6%8E%A7%20%5B%E5%8D%81%5D%2F</url>
    <content type="text"><![CDATA[自动化分类 所有的自动化都可以分为 2 种 1. 自动注册 Zabbix agnet 自动添加 2. 主动发现 1. 自动发现 Discover 2.zabbix api 因为我们只有 2 台web，为了方便演示。我们将原来添加的 proxy 删掉. 提示： 主动模式下设置自动注册 一、自动注册设置agent 配置文件修改 [root@linux-node2 ~]# vim /etc/zabbix/zabbix_agentd.conf LogFileSize=0 StartAgents=0 Server=192.168.56.11 ServerActive=192.168.56.11 Hostname=192.168.56.11 HostMetadata=system.uname #Server IP 地址 HostMetadataItem=system.uname #特征 1. 可以我们自己写一个特征 2. 我们执行一个 key #手写级别大于执行 key 过滤出我们的配置[如下] [root@CentOS6 zabbix]# egrep -v &quot;#|^$&quot; zabbix_agentd.conf PidFile=/var/run/zabbix/zabbix_agentd.pid LogFile=/var/log/zabbix/zabbix_agentd.log LogFileSize=0 StartAgents=0 Server=192.168.56.11 ServerActive=192.168.56.11 Hostname=192.168.56.12 HostMetadata=system.uname Include=/etc/zabbix/zabbix_agentd.d/ 我们先不重启，因为重启就生效了。我们需要设置一个规则. 注意自动发现必须要设置 ServerActive 让客户端启动主动去寻找服务端 提示，zabbix-agent 起来的时候去找 server，这时候就会产生一个事件，然后我们可以基于这个事件来完成一个动作 提示： zabbix-agent 起来的时候回去找 Server，这时候就会产生一个事件，然后我们可以基于这个事件来完成一个动作。 我们需要选中，然后在进行创建 如果选项匹配到 Linux，为什么匹配 Linux 呢？ 因为 Linux 可以在输入任何命令都可以生成 [root@linux-node2 ~]# uname Linux [root@linux-node2 ~]# uname -a Linux linux-node2.example.com 3.10.0-327.36.1.el7.x86_64 #1 SMP Sun Sep 18 13:04:29 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux 提示： 需要点击小的 Add 才可以继续操作 设置操作 我们先点击Add，在选择Host 我们在添加一个主机组，随便选一个就可以。 我们在添加一个模板 解释： 这样设置后我发现你这台主机我会给你设置一个主机组和一个模板。并且是 Linux 最后我们选择 Add 修改完之后我们在 重启 一下 [root@linux-node2 ~]# systemctl restart zabbix-agent.service 如果还没有出来，我们可以稍等一会 自动注册完! ————————分割线———————- 二、自动发现设置 因为我们的服务器只用了 2 台，所以昨晚 自动注册 我们在把它停掉。要不总会影响我们 我们在删除刚刚添加的主机 自动发现可以去扫描 IP 地址范围（需要手动设置）进行发现的动作 官方说明： https://www.zabbix.com/documentation/3.0/manual/discovery/network_discovery 创建 Zabbix 自动发现（生产一般不用） 唯一的标识我们可以设置 IP 地址，或者 key 值 然后我们创建一个Action(动作) 现在它自己就添加上去了 三、API 介绍 Zabbix 提供了一个丰富的 API，Zabbix 提供的 API 有2种功能。 一个是管理 一个是查询 请求方法 POST我们可以进行访问查看 无法打开，我们需要进行 POST 请求才可以。官方说明文档：https://www.zabbix.com/documentation/3.0/manual/api curl -s -X POST -H &apos;Content-Type:application/json-rpc&apos; -d&apos; { &quot;jsonrpc&quot;: &quot;2.0&quot;, &quot;method&quot;: &quot;user.login&quot;, &quot;params&quot;: { &quot;user&quot;: &quot;zhangsan&quot;, &quot;password&quot;: &quot;123456&quot; }, &quot;id&quot;: 1 }&apos; http://192.168.56.11/zabbix/api_jsonrpc.php | python -m json.tool -d 请求的内容 -H 类型 id 名字，类似一个标识 user 我们登陆用的是 zhangsan 默认是 Admin password 默认是 zabbix，我们修改为 123456 了 [root@linux-node1 ~]# curl -s -X POST -H &apos;Content-Type:application/json-rpc&apos; -d&apos; &gt; { &gt; &quot;jsonrpc&quot;: &quot;2.0&quot;, &gt; &quot;method&quot;: &quot;user.login&quot;, &gt; &quot;params&quot;: { &gt; &quot;user&quot;: &quot;zhangsan&quot;, &gt; &quot;password&quot;: &quot;123456&quot; &gt; }, &gt; &quot;id&quot;: 1 &gt; }&apos; http://192.168.56.11/zabbix/api_jsonrpc.php | python -m json.tool -------------------------- 分割线 ------------------------ 下面是返回的结果！！！！！！！！！！！！！！！！！！！！！！ { &quot;id&quot;: 1, &quot;jsonrpc&quot;: &quot;2.0&quot;, &quot;result&quot;: &quot;d8286f586348b96b6b0f880db3db8a02&quot; } 例如：我们获取所有主机的列表 官方文档：https://www.zabbix.com/documentation/3.0/manual/api/reference/host/get curl -s -X POST -H &apos;Content-Type:application/json-rpc&apos; -d&apos; { &quot;jsonrpc&quot;: &quot;2.0&quot;, &quot;method&quot;: &quot;host.get&quot;, &quot;params&quot;: {&quot;output&quot;: [&quot;host&quot;] }, &quot;auth&quot;: &quot;d8286f586348b96b6b0f880db3db8a02&quot;, &quot;id&quot;: 1 }&apos; http://192.168.56.11/zabbix/api_jsonrpc.php | python -m json.tool 提示： auth 里面填写的是我们刚刚返回的 result 里面的值, 如果我们在 [&quot;hostid&quot;] 加上 id 就会显示id。想全显示主机名就直接写host [root@linux-node1 ~]# curl -s -X POST -H &apos;Content-Type:application/json-rpc&apos; -d&apos; { &quot;jsonrpc&quot;: &quot;2.0&quot;, &quot;method&quot;: &quot;host.get&quot;, &quot;params&quot;: {&quot;output&quot;: [&quot;host&quot;] }, &quot;auth&quot;: &quot;d8286f586348b96b6b0f880db3db8a02&quot;, &quot;id&quot;: 1 }&apos; http://192.168.56.11/zabbix/api_jsonrpc.php | python -m json.tool { &quot;id&quot;: 1, &quot;jsonrpc&quot;: &quot;2.0&quot;, &quot;result&quot;: [ { &quot;host&quot;: &quot;Zabbix server&quot;, &quot;hostid&quot;: &quot;10084&quot; }, { &quot;host&quot;: &quot;linux-node1.example.com&quot;, &quot;hostid&quot;: &quot;10105&quot; }, { &quot;host&quot;: &quot;linux-node1.example.com1&quot;, &quot;hostid&quot;: &quot;10107&quot; }, { &quot;host&quot;: &quot;linux-node2.example.com&quot;, &quot;hostid&quot;: &quot;10117&quot; } ] } 对比图 例如：如何获取模板 官方文档：https://www.zabbix.com/documentation/3.0/manual/api/reference/template/get curl -s -X POST -H &apos;Content-Type:application/json-rpc&apos; -d&apos; { &quot;jsonrpc&quot;: &quot;2.0&quot;, &quot;method&quot;: &quot;template.get&quot;, &quot;params&quot;: {&quot;output&quot;: &quot;extend&quot;}, &quot;auth&quot;: &quot;d8286f586348b96b6b0f880db3db8a02&quot;, &quot;id&quot;: 1 }&apos; http://192.168.56.11/zabbix/api_jsonrpc.php | python -m json.tool 默认太多不发了，看图！ 过滤 过滤主机有 OS LINUX 的模板 curl -s -X POST -H &apos;Content-Type:application/json-rpc&apos; -d&apos; { &quot;jsonrpc&quot;: &quot;2.0&quot;, &quot;method&quot;: &quot;template.get&quot;, &quot;params&quot;: { &quot;output&quot;: &quot;extend&quot;, &quot;filter&quot;: { &quot;host&quot;: [&quot;Template OS Linux&quot;] } }, &quot;auth&quot;: &quot;d8286f586348b96b6b0f880db3db8a02&quot;, &quot;id&quot;: 1 }&apos; http://192.168.56.11/zabbix/api_jsonrpc.php | python -m json.tool 效果图如下！ 我们提供一个快速认证的 Python 脚本 链接：http://pan.baidu.com/s/1gf0pQwF 密码：m7dq 脚本内容如下 [root@linux-node1 ~]# cat zabbix_auth.py #!/usr/bin/env python # -*- coding:utf-8 -*- import requests import json url = &apos;http://192.168.56.11/zabbix/api_jsonrpc.php&apos; post_data = { &quot;jsonrpc&quot;: &quot;2.0&quot;, &quot;method&quot;: &quot;user.login&quot;, &quot;params&quot;: { &quot;user&quot;: &quot;zhangsan&quot;, &quot;password&quot;: &quot;123123&quot; }, &quot;id&quot;: 1 } post_header = {&apos;Content-Type&apos;: &apos;application/json&apos;} ret = requests.post(url, data=json.dumps(post_data), headers=post_header) zabbix_ret = json.loads(ret.text) if not zabbix_ret.has_key(&apos;result&apos;): print &apos;login error&apos; else: print zabbix_ret.get(&apos;result&apos;) 我们可以执行一下进行查看 提示： 需要修改里面的 用户名 和密码！ # 安装 python 环境 [root@linux-node1 ~]# yum install python-pip -y [root@linux-node1 ~]# pip install requests You are using pip version 7.1.0, however version 8.1.2 is available. You should consider upgrading via the &apos;pip install --upgrade pip&apos; command. Collecting requests Downloading requests-2.11.1-py2.py3-none-any.whl (514kB) 100% |████████████████████████████████| 516kB 204kB/s Installing collected packages: requests Successfully installed requests-2.11.1 ################################################ ################################################ ################################################ 执行结果 [root@linux-node1 ~]# python zabbix_auth.py 5b21317186f2a47404214556c5c1d846 四、案例：使用 API 进行自动添加主机 首先我们需要删除主机和自动发现 我们使用 API 来实现自动添加监控主机 使用 API 添加主机：https://www.zabbix.com/documentation/3.0/manual/api/reference/host/create curl -s -X POST -H &apos;Content-Type:application/json-rpc&apos; -d&apos; { &quot;jsonrpc&quot;: &quot;2.0&quot;, &quot;method&quot;: &quot;host.create&quot;, &quot;params&quot;: { &quot;host&quot;: &quot;Zabbix agent 192&quot;, &quot;interfaces&quot;: [ { &quot;type&quot;: 1, &quot;main&quot;: 1, &quot;useip&quot;: 1, &quot;ip&quot;: &quot;192.168.56.12&quot;, &quot;dns&quot;: &quot;&quot;, &quot;port&quot;: &quot;10050&quot; } ], &quot;groups&quot;: [ {&quot;groupid&quot;: &quot;8&quot;} ], &quot;templates&quot;: [ {&quot;templateid&quot;: &quot;10001&quot;} ] }, &quot;auth&quot;: &quot;5b21317186f2a47404214556c5c1d846&quot;, &quot;id&quot;: 1 }&apos; http://192.168.56.11/zabbix/api_jsonrpc.php | python -m json.tool 用户组 ID 获取方法 模板 IP 查看方法 执行结果如下： [root@linux-node1 ~]# curl -s -X POST -H &apos;Content-Type:application/json-rpc&apos; -d&apos; &gt; { &gt; &quot;jsonrpc&quot;: &quot;2.0&quot;, &gt; &quot;method&quot;: &quot;host.create&quot;, &gt; &quot;params&quot;: { &gt; &quot;host&quot;: &quot;Zabbix agent 192&quot;, &gt; &quot;interfaces&quot;: [ &gt; { &gt; &quot;type&quot;: 1, &gt; &quot;main&quot;: 1, &gt; &quot;useip&quot;: 1, &gt; &quot;ip&quot;: &quot;192.168.56.12&quot;, &gt; &quot;dns&quot;: &quot;&quot;, &gt; &quot;port&quot;: &quot;10050&quot; &gt; } &gt; ], &gt; &quot;groups&quot;: [ &gt; { &gt; &quot;groupid&quot;: &quot;8&quot; &gt; } &gt; ], &gt; &quot;templates&quot;: [ &gt; { &gt; &quot;templateid&quot;: &quot;10001&quot; &gt; } &gt; ] &gt; }, &gt; &quot;auth&quot;: &quot;5b21317186f2a47404214556c5c1d846&quot;, &gt; &quot;id&quot;: 1 &gt; }&apos; http://192.168.56.11/zabbix/api_jsonrpc.php | python -m json.tool { &quot;id&quot;: 1, &quot;jsonrpc&quot;: &quot;2.0&quot;, &quot;result&quot;: { &quot;hostids&quot;: [&quot;10118&quot;] } } 查看 Zabbix 页面 提示： 里面的主机名 / 模板 都是我们设置好的 Zabbix 完！ 转自：Zabbix 3.0 自动化监控 [十]]]></content>
      <categories>
        <category>zabbix</category>
      </categories>
      <tags>
        <tag>zabbix</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Zabbix 3.0 分布式监控 [九]]]></title>
    <url>%2F2016%2F11%2F21%2FZabbix%203.0%20%E5%88%86%E5%B8%83%E5%BC%8F%E7%9B%91%E6%8E%A7%20%5B%E4%B9%9D%5D%2F</url>
    <content type="text"><![CDATA[Zabbix Proxy是一个类似于代理的服务，可以代替 Zabbix-server 获取 zabbix-agent信息。其中 数据 存到本地（Proxy 有自己的数据库）然后在发送给 Server，这样可以保证数据不丢失 Zabbix-server -----&gt;Zabbix-Proxy -----&gt;Zabbix-Server 地址：https://www.zabbix.com/documentation/3.0/manual/distributed_monitoring/proxies Zabbix Proxy 使用场景 常用于多机房情况或者监控主机特别多，几千台左右。这时候使用Zabbix Proxy 可以减轻服务器server 的压力，还可以减轻 Zabbix 的维护。 最常用的特点是适用于 多机房 、 网络不稳定 的时候，因为如果直接由 Zabbix-server 发送信息可能 agent 没有收到，但是直接使用 Zabbix-Proxy 就不会遇到这个问题。 Zabbix 官方说明（分布式监控）Proxy 有如下功能 地址： Distributed monitoring NO - 中文解释 1. 没有 Web 界面 2. 本身不做任何告警通知（告警通知都是 Server 做） 小结： Zabbix Proxy 可以有多个，用来代理 Zabbix server 来运行。Proxy会将所有数据暂存于本地, 然后同一转发到 Zabbix Server 上 Proxy 只需要一条 TCP 链接，可以连接到 Zabbix-server 上即可。所以防火墙只需要添加一条 Zabbix Proxy 即可 我们可以参考上面的Zabbix Proxy 图 Proxy 是需要使用单独的 数据库 ，所以不能将Server 和Agent放在一起 Proxy 说明：https://www.zabbix.com/documentation/3.0/manual/distributed_monitoring/proxies 安装文档：https://www.zabbix.com/documentation/3.0/manual/installation/install 官方文档使用的是源码安装，因为方便我们使用 yum 安装，因为我们只有 2 台，所以就用 agent 当做 Proxy [root@linux-node2 ~]# yum install -y zabbix-proxy zabbix-proxy-mysql mariadb-server 我们需要启动 MySQL [root@linux-node2 ~]# systemctl start mariadb.service 我们还需要创建一个库 mysql create database zabbix_proxy character set utf8; grant all on zabbix_proxy.* to zabbix_proxy@localhost identified by &apos;zabbix_proxy&apos;; 我们需要导入数据 [root@linux-node2 ~]# cd /usr/share/doc/zabbix-proxy-mysql-3.0.5/ [root@linux-node2 zabbix-proxy-mysql-3.0.5]# zcat schema.sql.gz | mysql -uzabbix_proxy -p zabbix_proxy Enter password: #密码是：zabbix_proxy 是我们数据库授权的密码 检查数据库 mysql show databases; use zabbix_proxy; show tables; #查看是否含有数据 我们需要修改 proxy 的配置文件 [root@linux-node2 zabbix-proxy-mysql-3.0.5]# vim /etc/zabbix/zabbix_proxy.conf Server=192.168.56.11 Hostname=Zabbix proxy DBName=zabbix_proxy #数据库名称 DBUser=zabbix_proxy #用户名 DBPassword=zabbix_proxy #用户密码 配置文件中没有配置的内容如下：（有需要可以配置） # ProxyLocalBuffer=0 #数据保留的时间（小时为单位） # ProxyOfflineBuffer=1 #连不上 Server，数据要保留多久（小时为单位，默认 1 小时） # DataSenderFrequency=1 #数据的发送时间间隔（默认是 1 秒） # StartPollers=5 #启动的线程数 # StartIPMIPollers=0 #启动 IPMI 的线程数 从这往下都是性能的监控，就不一次说明了。 上面都有中文注释 过滤修改过的配置如下： [root@linux-node2 zabbix-proxy-mysql-3.0.5]# grep &apos;^[a-Z]&apos; /etc/zabbix/zabbix_proxy.conf Server=192.168.56.11 Hostname=Zabbix proxy LogFile=/var/log/zabbix/zabbix_proxy.log LogFileSize=0 PidFile=/var/run/zabbix/zabbix_proxy.pid DBName=zabbix_proxy DBUser=zabbix_proxy DBPassword=zabbix_proxy SNMPTrapperFile=/var/log/snmptrap/snmptrap.log Timeout=4 ExternalScripts=/usr/lib/zabbix/externalscripts LogSlowQueries=3000 启动 [root@linux-node2 ~]# systemctl start zabbix-proxy 查看 proxy 进程 [root@linux-node2 ~]# netstat -lntup Active Internet connections (only servers) Proto Recv-Q Send-Q Local Address Foreign Address State PID/Program name tcp0 0 0.0.0.0:33060.0.0.0:* LISTEN 15685/mysqld tcp0 0 0.0.0.0:22 0.0.0.0:* LISTEN 1073/sshd tcp0 0 127.0.0.1:250.0.0.0:* LISTEN 2498/master tcp0 0 0.0.0.0:10051 0.0.0.0:* LISTEN 15924/zabbix_proxy tcp6 0 0 :::44589:::*LISTEN 9052/java tcp6 0 0 :::8080 :::*LISTEN 9052/java tcp6 0 0 :::22 :::*LISTEN 1073/sshd tcp6 0 0 :::8888 :::*LISTEN 9052/java tcp6 0 0 ::1:25 :::*LISTEN 2498/master tcp6 0 0 :::39743:::*LISTEN 9052/java tcp6 0 0 :::10051:::*LISTEN 15924/zabbix_proxy tcp6 0 0 127.0.0.1:8005 :::*LISTEN 9052/java tcp6 0 0 :::8009 :::*LISTEN 9052/java Zabbix-proxy 监控 10051 端口，因为是代理就必须跟 Server 的端口相同，对于 Agent Proxy 就是 Server Zabbix Web 添加 点击 Add 即可 我们需要将这台主机的 Server 设置为 Proxy编辑 192.168.56.12 这台主机，需要将 Server 的IP 地址修改成自己的 因为现在是主动模式，我们只需要修改主动模式的 Server 即可 [root@linux-node2 ~]# vim /etc/zabbix/zabbix_agentd.conf ServerActive=192.168.56.12 #配置文件修改完需要重启 [root@linux-node2 ~]# systemctl restart zabbix-agent 这时候我们就可以看到那个 proxy 都管理了那些机器, 做到方便管理的机制 proxy 简单的理解就是一个 Server 完！ 转自：Zabbix 3.0 分布式监控 [九]]]></content>
      <categories>
        <category>zabbix</category>
      </categories>
      <tags>
        <tag>zabbix</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Zabbix 3.0 主备模式 [八]]]></title>
    <url>%2F2016%2F11%2F20%2FZabbix%203.0%20%E4%B8%BB%E5%A4%87%E6%A8%A1%E5%BC%8F%20%5B%E5%85%AB%5D%2F</url>
    <content type="text"><![CDATA[监控常遇到的问题？ 1. 监控主机多，性能跟不上，延迟大 2. 多机房，防火墙因素 Zabbix 轻松解决以上问题，Nagios 不太好解决的问题。 Zabbix 模式介绍： 1、被动模式 2、主动模式 默认是被动模式，我们可以通过以下方式查看监控项是什么模式 因为我们使用的是模板，无法进行修改。我们可以修改配置文件或者新建 item 的时候设置。 注意： 1、当监控主机超过 300+，建议使用主动模式（此处是一个经验值，要根据服务器的硬件来进行考虑） 2、还需要保证 Queue 对列里面没有延迟的主机 Queue 对列介绍 如果此处的延迟主机有点多的话，我们就需要将被动模式修改为主动模式. 主动模式设置 将 192.168.56.12 监控设置为主动模式 1、修改配置文件 为了方便模拟，我们将 node2(192.168.56.12)从 Zabbix 删除从新添加 [root@linux-node2 ~]# vim /etc/zabbix/zabbix_agentd.conf #Server=192.168.56.11 #我们需要注释 Server，因为这个是被动模式用的 StartAgents=0 #设置为 0 之后就不会 TCP 端口，之前监听 TCP 端口是因为 Server 要去问 agent 信息所以需要开启 ServerActive=192.168.56.11 #此处可以是 IP 或者是域名，他会连接 10051 端口 Hostname=linux-node2.example.com #唯一识别符，我们需要修改成我们本机的主机名。如果我们不设置，它默认会通过 item 来获取 [root@linux-node2 ~]# systemctl restart zabbix-agent.service 保存重启 保存重启之后我们可以查看我们监听的一些端口，因为我们关闭的被动模式所以不会在监听 zabbix 端口了 [root@linux-node2 ~]# netstat -lntup Active Internet connections (only servers) Proto Recv-Q Send-Q Local Address Foreign Address State PID/Program name tcp0 0 0.0.0.0:22 0.0.0.0:* LISTEN 1073/sshd tcp0 0 127.0.0.1:250.0.0.0:* LISTEN 2498/master tcp6 0 0 :::44589:::*LISTEN 9052/java tcp6 0 0 :::8080 :::*LISTEN 9052/java tcp6 0 0 :::22 :::*LISTEN 1073/sshd tcp6 0 0 :::8888 :::*LISTEN 9052/java tcp6 0 0 ::1:25 :::*LISTEN 2498/master tcp6 0 0 :::39743:::*LISTEN 9052/java tcp6 0 0 127.0.0.1:8005 :::*LISTEN 9052/java tcp6 0 0 :::8009 :::*LISTEN 9052/java 我们可以查看日志，进行检查 [root@linux-node2 ~]# tailf /var/log/zabbix/zabbix_agentd.log 14932:20161011:084303.210 **** Enabled features **** 14932:20161011:084303.210 IPv6 support: YES 14932:20161011:084303.210 TLS support: YES 14932:20161011:084303.210 ************************** 14932:20161011:084303.210 using configuration file: /etc/zabbix/zabbix_agentd.conf 14932:20161011:084303.210 agent #0 started [main process] 14933:20161011:084303.227 agent #1 started [collector] 14934:20161011:084303.227 agent #2 started [active checks #1] 14934:20161011:084303.271 no active checks on server [192.168.56.11:10051]: host [linux-node2.example.com] not found 14934:20161011:084503.415 no active checks on server [192.168.56.11:10051]: host [linux-node2.example.com] not found 日志解释： zabbix—agent设置完主动模式后，会去主动问 server 需求。相当于入职刚入职运维需要老大进行分配任务。并且以后就会根据这个任务清单进行执行 因为我们还没有配置server，所以现在会出现错误 Zabbix-web 设置 我们需要添加 zabbix-agent 添加模板 ，zabbix 没有提供主动模式的模板。所以我们需要克隆一下 OS Linux 找到 OS Linux 模板，移动到最下面 点击复制 我们从新进行设置名称 修改我们刚刚添加的模板名为OS Linux Active 我们点击刚刚创建模板的 item 然后选择最下方 Update 结果如下： 在次查看模板，发现 zabbix 还依赖一个模板。我们需要把它也改了或者是删掉。 我们添加主机 添加模板 # 提示：我们已经可以获取到数据了，但是发现 zabbix 这个模块发红。可能是由于我们没有修改他的依赖造成的 如下图： 可能是通过 agent.ping 来获取信息, 没有看过源码 所以不太清楚，我研究它 zabbix 主备模式完成 转自：Zabbix 3.0 主备模式 [八]]]></content>
      <categories>
        <category>zabbix</category>
      </categories>
      <tags>
        <tag>zabbix</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Zabbix 3.0 监控 Web [七]]]></title>
    <url>%2F2016%2F11%2F15%2FZabbix%203.0%20%E7%9B%91%E6%8E%A7Web%20%5B%E4%B8%83%5D%2F</url>
    <content type="text"><![CDATA[Zabbix 默认自带一个 web 监控 我们可以从 Monitoring---&gt;Web 进行查看 按照前面的文章，我们在 192.168.56.12 上面已经开启了一个 Tomcat 端口为 8080. 如果没有的小伙伴可以阅读 [Zabbix 3.0 生产案例 [四] 一、检查 首先我们需要检查 192.168.56.12 是否有 tomcat，是否可以运行。能否访问 1. 查看进程 [root@linux-node2 ~]# ps -ef|grep java root 8048 25468 0 10:31 pts/000:00:00 grep --color=auto java root 42757 1 0 Sep26 pts/000:38:59 /usr/bin/java -Djava.util.logging.config.file=/usr/local/tomcat/conf/logging.properties -Djava.util.logging.manager=org.apache.juli.ClassLoaderLogManager -Djdk.tls.ephemeralDHKeySize=2048 -Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.port=8888 -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.authenticate=false -Djava.rmi.server.hostname=192.168.56.12 -classpath /usr/local/tomcat/bin/bootstrap.jar:/usr/local/tomcat/bin/tomcat-juli.jar -Dcatalina.base=/usr/local/tomcat -Dcatalina.home=/usr/local/tomcat -Djava.io.tmpdir=/usr/local/tomcat/temp org.apache.catalina.startup.Bootstrap start 2. 查看端口 [root@linux-node2 ~]# lsof -i:8080 COMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAME java42757 root 48u IPv6 379379 0t0 TCP *:webcache (LISTEN) 3. 测试是否可以访问 8080 端口 [root@linux-node2 ~]# curl -I 192.168.56.11:8080 HTTP/1.1 200 OK Server: nginx/1.10.1 Date: Mon, 10 Oct 2016 05:08:18 GMT Content-Type: text/html Content-Length: 612 Last-Modified: Mon, 19 Sep 2016 01:59:49 GMT Connection: keep-alive ETag: &quot;57df4695-264&quot; Accept-Ranges: bytes 二、Zabbix Web 界面配置 提示： 监控 Web 不依赖于 agent，是server 直接发送请求的 提示： 这里名字叫做 Web 场景，因为我们可以设置触发上面 3 个选项后，才进行报警 提示： 字符串里面可以添加一些字符串，当请求下来有这个字符串就是正常，没有就是不正常。但是最常用的还是状态 然后我们选择Add 比较坑的一点是，我们新添加了一个 Web 监控。zabbix 默认没有给我们安装触发器 三、触发器添加 Web 监控中默认不含有触发器，所以需要手动添加 点右上角，进行创建触发器 四、触发器报警测试1、停掉 tomcat，要想返回值不是 200 停掉 tomcat 是最简单的 [root@linux-node2 ~]# /usr/local/tomcat/bin/shutdown.sh Using CATALINA_BASE: /usr/local/tomcat Using CATALINA_HOME: /usr/local/tomcat Using CATALINA_TMPDIR: /usr/local/tomcat/temp Using JRE_HOME:/usr Using CLASSPATH: /usr/local/tomcat/bin/bootstrap.jar:/usr/local/tomcat/bin/tomcat-juli.jar 检查 [root@linux-node2 ~]# ps aux|grep tomcat root 8723 0.0 0.0 112648 976 pts/1R+ 12:21 0:00 grep --color=auto tomcat 报警如下： 回复如上 邮件报警设置可以访问 Zabbix 3.0 生产案例 [五]我们还可以优化动作[Actions] Zabbix 就是一个万能的什么都可以监控，只要我们有 key。什么都可以监控key 我们可以使用脚本，程序等等等 转自：Zabbix 3.0 监控 Web [七]]]></content>
      <categories>
        <category>zabbix</category>
      </categories>
      <tags>
        <tag>zabbix</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Zabbix 3.0 监控 MySQL [六]]]></title>
    <url>%2F2016%2F11%2F15%2FZabbix%203.0%20%E7%9B%91%E6%8E%A7MySQL%20%5B%E5%85%AD%5D%2F</url>
    <content type="text"><![CDATA[Mysql 监控 zabbix 自带了一个监控 mysql 的模板，但是真正监控 mysql 的并不是 zabbix 自带的模板。而是 percona 公司的一个监控 mysql 模板 percona 官网： www.percona.com Percona 组成介绍 1、php 脚本 用来数据采集2、shell 脚本 用来调用采集信息3、zabbix 配置文件4、zabbix 模板文件 安装文档：https://www.percona.com/doc/percona-monitoring-plugins/LATEST/zabbix/index.html percona 利用的是 php 来获取 mysql 的相关信息，所以如果我们想使用 percona 插件监控 mysql 就需要在 agent 端安装php。在安装文档上有写哦~ 安装步骤： 查看上面的链接也可以进行安装 我们安装在 zabbix-server 上，因为上面有一个 MySQL [root@linux-node1 web]# yum install http://www.percona.com/downloads/percona-release/redhat/0.1-3/percona-release-0.1-3.noarch.rpm [root@linux-node1 web]# yum install percona-zabbix-templates php php-mysql -y #percona 插件是通过 php 去获取 mysql 的参数，所以我们要安装 php 和 php-mysql 我们可以查看它都安装了那些软件 [root@linux-node1 web]# rpm -ql percona-zabbix-templates /var/lib/zabbix/percona /var/lib/zabbix/percona/scripts /var/lib/zabbix/percona/scripts/get_mysql_stats_wrapper.sh #shell 脚本 /var/lib/zabbix/percona/scripts/ss_get_mysql_stats.php #php 获取 mysql 信息 /var/lib/zabbix/percona/templates /var/lib/zabbix/percona/templates/userparameter_percona_mysql.conf #zabbix 配置文件 /var/lib/zabbix/percona/templates/zabbix_agent_template_percona_mysql_server_ht_2.0.9-sver1.1.6.xml #zabbix 模板文件 在 percona 组成我们已经说过了，此处只是略微介绍。 我们将 zabbix 模板下载下来 [root@linux-node1 web]# sz /var/lib/zabbix/percona/templates/zabbix_agent_template_percona_mysql_server_ht_2.0.9-sver1.1.6.xml 然后我们需要将模板通过 web 界面导入到 zabbix 中 提示：如果出现错误，可能是 zabbix 3.0 版本的问题。我们这里提供了一个生产的模板 下载链接： https://pan.baidu.com/s/1TgsPR3qjWyxjwKYQrz6fWQ密码:u09h 然后从新上传即可 复制配置文件 [root@linux-node1 web]# cp /var/lib/zabbix/percona/templates/userparameter_percona_mysql.conf /etc/zabbix/zabbix_agentd.d/ [root@linux-node1 web]# ls /etc/zabbix/zabbix_agentd.d/ #安装完软件包后会在 /var/lib/zabbix/percona/templates/ 目录下产生一个配置文件，我们将它拷贝，因为在前面的博文中，我们已经修改过 zabbix 的配置文件[Include=/etc/abbix/zabbix_agentd.d/] 所以将配置文件放在这个目录下，zabbix 就会自己在这个目录下查找相关信息 [root@linux-node1 web]# systemctl restart zabbix-agent.service 重启一下！ 下面就应该配置与 MySQL 的连接 在/var/lib/zabbix/percona/scripts/ss_get_mysql_stats.php.cnf创建一个文件 [root@linux-node1 ~]# cat /var/lib/zabbix/percona/scripts/ss_get_mysql_stats.php.cnf &lt;?php $mysql_user = &apos;root&apos;; $mysql_pass = &apos;&apos;; #用户名密码可以自己创建，有密码写密码，没密码为空就好了 提示： 正常这里的用户我们应该创建一个专门用来监控的，由于我这里是测试环境。就不浪费时间了 测试 查看是否可以获取到值，随便找一个测试 [root@linux-node1 ~]# cat /var/lib/zabbix/percona/templates/userparameter_percona_mysql.conf 选择一个肯定有值的 key [root@linux-node1 ~]# cat /var/lib/zabbix/percona/templates/userparameter_percona_mysql.conf|grep gm UserParameter=MySQL.read-views,/var/lib/zabbix/percona/scripts/get_mysql_stats_wrapper.sh gm 测试结果如下： [root@linux-node1 ~]# cd /var/lib/zabbix/percona/scripts/ [root@linux-node1 scripts]# ./get_mysql_stats_wrapper.sh gm 1 [root@linux-node1 scripts]# ./get_mysql_stats_wrapper.sh gw 9736342 可以获取到值，说明没有问题 温馨提示： shell 脚本中数据库的路径是 localhost，如果我们没有授权 localhost 会获取不到值 [root@linux-node1 scripts]# cat get_mysql_stats_wrapper.sh HOST=localhost RES=`HOME=~zabbix mysql -e &apos;SHOW SLAVE STATUS\G&apos; | egrep &apos;(Slave_IO_Running|Slave_SQL_Running):&apos; | awk -F: &apos;{print $2}&apos; | tr &apos;\n&apos; &apos;,&apos;` #mysql 是通过命令来获取的，如果环境变量不一样 也可能造成影响 Zabbix_Web 界面配置 模板已经上传到 zabbix 中，这时候我们就需要进行设置了 提示： 我们还需要授权 /tmp 下的一个文件，因为默认情况下 zabbix 在文件中获取的值 修改完就可以获取值了，所以我们还需要测试 结果如下图 思想： 如果出现错误我们需要先查看 shell 的脚本，因为 shell 是去调用 php。 错误的因素有很多，最简单的方法就是用 shell 后面加上 key 看看是否可以有值。 其中报错最多的地方就是 php 和 mysql 连接的问题，还有我们 mysql 授权的一些问题。 MYSQL 监控 完！ 转载自：Zabbix 3.0 监控 MySQL [六]]]></content>
      <categories>
        <category>zabbix</category>
      </categories>
      <tags>
        <tag>zabbix</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Zabbix 3.0 生产案例 [五]]]></title>
    <url>%2F2016%2F11%2F12%2FZabbix%203.0%20%E7%94%9F%E4%BA%A7%E6%A1%88%E4%BE%8B%20%5B%E4%BA%94%5D%2F</url>
    <content type="text"><![CDATA[上面我们说到了监控 TCP 和 Nginx 状态，但是光是监控是没有任何作用的。监控完我们不知道跟没监控没啥区别，下面我们进行 监控项 的讲解 1. 触发器 首先我们给 Nginx 添加触发器1. 选择Configuration---&gt;Hosts 2. 找到我们相对应的主机进入 3. 选择主机中的 Triggers—&gt; 添加(Create trigger) 我们设置一个事件 我们选择 Insert，然后选择Add 即可 4. 查看报警状态 因为我们设置的级别大于 1 就报警，默认 Nginx 是 0，随便访问以下就是 1. 所以肯定就会报警。报警邮件可以根据我们前面 [Zabbix 3.0 部署监控 [三]]文章进行设置 报警邮件如下： 我们可以查看这个事件的相关过程 以上就是我们添加的一个触发器报警步骤 Zabbix 默认触发器的预值比较低，我们需要调大。这个在面试过程中会被问到 我们进行修改默认模板 路径下图： 我们可以看到默认是大于 300 进行报警, 我们点进去修改即可 根据实际情况进行修改，我们设置 600 即可。同时触发器支持多个条件进行报警，如 or all 等，只需要在上面的值后面继续添加即可。 我们修改完之后 还有一个有警告显示磁盘不够，因为是虚拟机我们不予理会，我们可以查看到恢复之后的邮件 2. 脚本发送邮件 提示： Zabbix 邮件报警是 3.0 才有的，以前不支持用户名密码。所以早期都是使用脚本进行发送邮件报警。 由于时间关系我们就不进行写了请下载发送邮件的 python 脚本：链接：http://pan.baidu.com/s/1gfkGrgZ 密码：6bsh 脚本注释： Python 脚本中三个相关的参数 receiver = sys.argv[1] #收件人地址 subject = sys.argv[2] #发送邮件的主题 mailbody = sys.argv[3] #发送邮件的内容 smtpserver = &apos;smtp.exmail.qq.com&apos; #邮件服务器地址，本脚本使用的是企业邮箱 username = &apos;username&apos; #用户名 password = &apos;password&apos; #密码 sender = username #发送人名称 我们如果要写一个发送邮件的脚本，需要支持三个参数 1、收件人 2、标题 3、内容 自定义告警脚本 我们也可以使用shell 写一个最简单的 脚本存放路径：我们可以在配置文件中查看 [root@linux-node1 web]# vim /etc/zabbix/zabbix_server.conf AlertScriptsPath=/usr/lib/zabbix/alertscripts 提示： 这行配置文件定义了邮件脚本的存放路径，因为它默认会从 usr/lib/zabbix/alertscripts 查找邮件脚本 [root@linux-node1 web]# vim /usr/lib/zabbix/alertscripts/sms.sh #!/bin/bash ALTER_TO=$1 ALTER_TITLE=$2 ALTER_BODY=$3 echo $ALTER_TO &gt;&gt; /tmp/sls.log echo $ALTER_TITLE &gt;&gt; /tmp/sms.log echo $ALTER_BODY &gt;&gt; /tmp/sms.log 我们可以写完之后进行检测，如果这里有信息说明已经调用这个脚本。 如果我们有短信通道将里面的内容换一下即可，短信通道都是有售后的 修改权限 [root@linux-node1 web]# chmod +x /usr/lib/zabbix/alertscripts/sms.sh [root@linux-node1 web]# ll /usr/lib/zabbix/alertscripts/sms.sh -rwxr-xr-x 1 root root 152 Oct 8 20:26 /usr/lib/zabbix/alertscripts/sms.sh 我们写的脚本是短信报警，首先你需要有一个短信通道，我们可以使用阿里云大鱼，本次我们使用文件追加的形式来模拟. Zabbix 页面设置 点击右上角创建报警介质 点击最下面的 Add 提示：先点击小的 Update 在点最下面的Update 我们还需要修改报警媒介 找到相对应的用户，点击。 接下来就需要我们触发报警了 上面我们设置的连接数是大于 1，所以我们多刷新几次就可以了 这里显示发送完成，我们去日志进行查看 13122323232 为发送的手机号 PROBLEM： 为主题信息 Nginx Active 监控项 Original……..：为故障信息，2 代表连接数是 2 提示： 因为中国的短信收费是 70 个字符 2 毛，字母也算是。所以我们发送邮件的报警信息就需要简介明了一点 优化图如下： 修改后如下： 设置完成之后最好数一下，不要超过 70 个字符 http://www.alidayu.com/ 有兴趣的同学可以自己了解一下阿里大鱼，可以提供短信通道、语音、验证码等业务。 短信通道比较出名的几款产品： 亿美软通 阿里大鱼 腾讯云也有 微信报警 短信报警和邮件报警已经说过了，我们简单的说一下微信报警 因为在很早之前就说过，个人服务号和订阅号不支持直接跟订阅用户进行沟通。如果是企业号可以直接获取到一个类似 key，拿着这个 key 直接 curl 就可以了发了。 具体内容可以进行百度或者谷哥搜索。 扩展： 除了以上三种报警，还有 钉钉报警 以前还有 QQ 报警、 飞信报警，但是现在已经不开源了 提示： 上面那三行最好不要删除，在生产环境中追加到一个文件中。记录发送邮件的信息 转载自：Zabbix 3.0 生产案例 [五]]]></content>
      <categories>
        <category>zabbix</category>
      </categories>
      <tags>
        <tag>zabbix</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Zabbix 字符集乱码及 Centos7 补全设置]]></title>
    <url>%2F2016%2F11%2F10%2FZabbix%E5%AD%97%E7%AC%A6%E9%9B%86%E4%B9%B1%E7%A0%81%E5%8F%8ACentos7%E8%A1%A5%E5%85%A8%E8%AE%BE%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[Centos 补全安装软件包 [root@linux-node1 ~]# yum install -y bash-completion 从新打开窗口即可 操作： 1. 找到本地 C:\Windows\Fonts\simkai.ttf（楷体）上传到服务器 zabbix 网站目录 fonts 目录下。 [root@localhost /]# whereis zabbix zabbix: /usr/lib/zabbix /etc/zabbix /usr/share/zabbix [root@localhost /]# cd /usr/share/zabbix [root@localhost zabbix]# ll |grep fonts drwxr-xr-x. 3 root root75 Jun 12 14:04 fonts [root@localhost zabbix]# cd fonts/ [root@localhost fonts]# ll total 30176 drwxr-xr-x. 2 root root 26 Jun 12 14:01 fonts_bak -rw-r--r--. 1 root root 720012 Jun 12 14:03 graphfont.ttf -rw-r--r--. 1 root root 11785184 Jun 11 2009 simkai.ttf -rw-r--r--. 1 root root 18387092 Jun 12 14:04 uming.ttf [root@localhost fonts]# 2. 修改 zabbix php 配置文件 [root@localhost /]# find -name defines.inc.php ./usr/share/zabbix/include/defines.inc.php [root@localhost fonts]# cd /usr/share//zabbix/ [root@localhost zabbix]# ll |grep include drwxr-xr-x. 4 root root 4096 Jun 12 14:37 include #从网上抄的，不适合本机 sed -i &apos;s/DejaVuSans/simkai/g&apos; ./include/defines.inc.php #自己修改的，做了两次，后发现界面上文字没有了。 sed -i &apos;s/graphfont/simkai/g&apos; ./include/defines.inc.php sed -i &apos;s/fonts/simkai/g&apos; ./include/defines.inc.php #检查 defines.inc.php 文件 [root@localhost zabbix]# vim defines.inc.php #查找到“simkai”关键字，修改 ZBX_FONTPATH&apos;（红色标记部分） // the maximum period to display history data for the latest data and item overview pages in seconds // by default set to 86400 seconds (24 hours) define(&apos;ZBX_HISTORY_PERIOD&apos;, 86400); define(&apos;ZBX_WIDGET_ROWS&apos;, 20); define(&apos;ZBX_FONTPATH&apos;, realpath(&apos;/usr/share/zabbix/fonts/&apos;)); // where to search for font (GD &gt; 2.0.18) define(&apos;ZBX_GRAPH_FONT_NAME&apos;, &apos;simkai&apos;); // font file name /simkai define(&apos;ZBX_FLAG_DISCOVERY_NORMAL&apos;, 0x0); define(&apos;ZBX_FLAG_DISCOVERY_RULE&apos;, 0x1); define(&apos;ZBX_FLAG_DISCOVERY_PROTOTYPE&apos;, 0x2); define(&apos;ZBX_FLAG_DISCOVERY_CREATED&apos;,0x4); define(&apos;EXTACK_OPTION_ALL&apos;, 0); define(&apos;EXTACK_OPTION_UNACK&apos;, 1); define(&apos;EXTACK_OPTION_BOTH&apos;,2); define(&apos;TRIGGERS_OPTION_RECENT_PROBLEM&apos;,1); define(&apos;TRIGGERS_OPTION_ALL&apos;, 2); define(&apos;TRIGGERS_OPTION_IN_PROBLEM&apos;,3); define(&apos;ZBX_ACK_STS_ANY&apos;, 1); define(&apos;ZBX_ACK_STS_WITH_UNACK&apos;,2); define(&apos;ZBX_ACK_STS_WITH_LAST_UNACK&apos;, 3); define(&apos;EVENTS_OPTION_NOEVENT&apos;, 1); define(&apos;EVENTS_OPTION_ALL&apos;, 2); define(&apos;EVENTS_OPTION_NOT_ACK&apos;, 3); define(&apos;ZBX_FONT_NAME&apos;, &apos;simkai&apos;); define(&apos;ZBX_AUTH_INTERNAL&apos;, 0); define(&apos;ZBX_AUTH_LDAP&apos;, 1); define(&apos;ZBX_AUTH_HTTP&apos;, 2); #重启 zabbix 服务 service zabbix-server restart 提示：如果我们找不到配置文件可以使用以下方法 [root@linux-node1 ~]# find / -type f -name &quot;defines.inc.php&quot; /usr/share/zabbix/include/defines.inc.php 将字体导入到 /usr/share/zabbix/fonts 效果图如下 图一，修改前 图一，修改后 转载自：Zabbix 字符集乱码及 Centos7 补全设置]]></content>
      <categories>
        <category>zabbix</category>
      </categories>
      <tags>
        <tag>zabbix</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Zabbix 3.0 生产案例 [四]]]></title>
    <url>%2F2016%2F11%2F10%2FZabbix%203.0%20%E7%94%9F%E4%BA%A7%E6%A1%88%E4%BE%8B%20%5B%E5%9B%9B%5D%2F</url>
    <content type="text"><![CDATA[Zabbix 生产案例实战 一、项目规划 1、主机分组： 交换机 Nginx Tomcat MySQL 2、监控对象识别： 1、使用 SNMP 监控交换机 2、使用 IPMI 监控服务器硬件 3、使用 Agent 监控服务器 4、使用 JMX 监控 Java 应用 5、监控 MySQL 6、监控 Web 状态 7、监控 Nginx 状态 3、操作步骤：SNMP 监控 1.1 在交换机上开启Snmpconfig t snmp-server community public ro end 提示：如果不知道我们可以百度 ####1.2 在 Zabbix 上添加SNMP 监控 步骤：Configuration---&gt;Hosts---&gt; 设置 1.3 Host 页面设置 1.4 Templates 模板设置 设置 SNMP 团体名称 Macros 宏 这里的设置要跟我们创建的 SNMP 的设置相同 因为 Zabbix 监控的时候依赖团体名称 1.5 生产图片 Zabbix 会自动给我们进行检测端口，每个端口都会添加一个网卡的流量图，每个端口都会加上一个触发器。（端口的状态 ） 还会帮我们添加VLAN 的一个监控 1.6 案例图 含有有进口和出口流量 提示：此图是 Zabbix SNMP 模板自动生成的 IPMI 监控 2.1 添加 IPMIConfiguration---&gt;Hosts---&gt; 选择主机 ---&gt; 设置 IPMI 端口及主机 ---&gt; 用户名密码 因为 IMP 容易超时，建议使用自定义 item，本地执行ipmitool 命令来获取数据 JMX 监控 Zabbix 默认提供了一个监控 JMX, 通过 java gateway 来监控 java 地址：https://www.zabbix.com/documentation/3.2/manual/appendix/config/zabbix_java JAVA GATEWAY需要独立安装，相当于一个网关，因为 zabbix_server 和 zabbix-agent 不可以直接获取 java 信息。所以需要一个代理来获取 zabbix java Gateway不存任何数据, 只是一个简单的代理 1、安装[root@linux-node1 ~]# yum install -y zabbix-java-gateway java-1.8.0 提示：java-gateway 需要 java 环境 2、配置 修改 java-gateway [root@linux-node1 ~]# vim /etc/zabbix/zabbix_java_gateway.conf # LISTEN_IP=&quot;0.0.0.0&quot; 监听的 IP 地址 # LISTEN_PORT=10052 监听的端口 PID_FILE=&quot;/var/run/zabbix/zabbix_java.pid&quot; 存放 pid 路径 # START_POLLERS=5 开通几个进程, 默认是 5。你有多少 java 进行可以设置多少个，也可以设置 java 进程的一半。 TIMEOUT=3 超时时间 1-30，如果网络环境差，超时时间就修改长一点 我们默认就可以了，不进行修改 3、启动[root@linux-node1 ~]# systemctl start zabbix-java-gateway.service 4、端口、进程查看 我们可以进行进程的查看 [root@linux-node1 ~]# netstat -lntp Active Internet connections (only servers) Proto Recv-Q Send-Q Local Address Foreign Address State PID/Program name tcp0 0 0.0.0.0:33060.0.0.0:* LISTEN 10439/mysqld tcp0 0 0.0.0.0:80800.0.0.0:* LISTEN 33484/nginx: master tcp0 0 0.0.0.0:22 0.0.0.0:* LISTEN 1054/sshd tcp0 0 127.0.0.1:250.0.0.0:* LISTEN 2484/master tcp0 0 0.0.0.0:10050 0.0.0.0:* LISTEN 76482/zabbix_agentd tcp0 0 0.0.0.0:10051 0.0.0.0:* LISTEN 34572/zabbix_server tcp0 0 127.0.0.1:199 0.0.0.0:* LISTEN 11143/snmpd tcp6 0 0 :::80 :::*LISTEN 10546/httpd tcp6 0 0 :::22 :::*LISTEN 1054/sshd tcp6 0 0 ::1:25 :::*LISTEN 2484/master tcp6 0 0 :::10050:::*LISTEN 76482/zabbix_agentd tcp6 0 0 :::10051:::*LISTEN 34572/zabbix_server tcp6 0 0 :::10052:::*LISTEN 13465/java 10052 zabbix-java-gateway默认端口已经起来了！ 它是一个 java 应用，需要安装 jdk [root@linux-node1 ~]# ps -aux|grep java root 13465 0.4 3.4 2248944 34060 ? Sl 19:17 0:01 java -server -Dlogback.configurationFile=/etc/zabbix/zabbix_java_gateway_logback.xml -classpath lib:lib/android-json-4.3_r3.1.jar:lib/logback-classic-0.9.27.jar:lib/logback-core-0.9.27.jar:lib/slf4j-api-1.6.1.jar:bin/zabbix-java-gateway-3.0.4.jar -Dzabbix.pidFile=/var/run/zabbix/zabbix_java.pid -Dzabbix.timeout=3 -Dsun.rmi.transport.tcp.responseTimeout=3000 com.zabbix.gateway.JavaGateway root 13584 0.0 0.0 112648 972 pts/0S+ 19:21 0:00 grep --color=auto java 5、通知 zabbix-server 我们需要通知 zabbix-server，java-gateway 在哪里 修改配置文件 [root@linux-node1 ~]# vim /etc/zabbix/zabbix_server.conf 编辑 zabbix-server 来指定 zabbix-java-gateway JavaGateway=192.168.56.11 #IP 地址是安装 java-gateway 的服务器 # JavaGatewayPort=10052 端口，默认就可以 StartVMwareCollectors=5 预启动多少个进程[zabbix---&gt;java-gateway 的数量] 6、重启 zabbix-server[root@linux-node1 ~]# systemctl restart zabbix-server.service 7、准备 apache我们安装 tomcat-8 版本 官网：http://tomcat.apache.org下载软件包 [root@linux-node2 src]# wget http://mirror.bit.edu.cn/apache/tomcat/tomcat-8/v8.5.5/bin/apache-tomcat-8.5.5.tar.gz 我们将 tomcat 安装在 apache 服务器上，来模拟监控 jvm [root@linux-node2 src]# tar xf apache-tomcat-8.5.5.tar.gz [root@linux-node2 src]# mv apache-tomcat-8.5.5 /usr/local/ [root@linux-node2 src]# ln -s /usr/local/apache-tomcat-8.5.5/ /usr/local/tomcat [root@linux-node2 src]# yum install -y java-1.8.0#tomcat 需要在 java 环境运行 [root@linux-node2 src]# /usr/local/tomcat/bin/startup.sh Using CATALINA_BASE: /usr/local/tomcat Using CATALINA_HOME: /usr/local/tomcat Using CATALINA_TMPDIR: /usr/local/tomcat/temp Using JRE_HOME:/usr Using CLASSPATH: /usr/local/tomcat/bin/bootstrap.jar:/usr/local/tomcat/bin/tomcat-juli.jar Tomcat started. 在 web2 上面查看运行状态 [root@linux-node2 src]# netstat -lntup Active Internet connections (only servers) Proto Recv-Q Send-Q Local Address Foreign Address State PID/Program name tcp0 0 0.0.0.0:22 0.0.0.0:* LISTEN 1073/sshd tcp0 0 127.0.0.1:250.0.0.0:* LISTEN 2498/master tcp0 0 0.0.0.0:10050 0.0.0.0:* LISTEN 10088/zabbix_agentd tcp6 0 0 :::8080 :::*LISTEN 25750/java tcp6 0 0 :::22 :::*LISTEN 1073/sshd tcp6 0 0 ::1:25 :::*LISTEN 2498/master tcp6 0 0 :::10050:::*LISTEN 10088/zabbix_agentd tcp6 0 0 127.0.0.1:8005 :::*LISTEN 25750/java tcp6 0 0 :::8009 :::*LISTEN 25750/java JMX 三种类型： 1. 无密码认证 2. 用户面密码认证 3.ssl 开启 JMX 远程监控 官方文档：http://tomcat.apache.org/tomcat-8.0-doc/monitoring.html我们创建一个无密码认证 [root@linux-node2 src]# vim /usr/local/tomcat/bin/catalina.sh CATALINA_OPTS=&quot;$CATALINA_OPTS -Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.port=8888 #端口号 -Dcom.sun.management.jmxremote.ssl=false #SSL 关闭 -Dcom.sun.management.jmxremote.authenticate=false #用户密码验证关闭 -Djava.rmi.server.hostname=192.168.56.12&quot; #监控的主机 修改完成后重启 tomcat 可以使用 ./shutdown.sh 或者使用kill 的方式 [root@linux-node2 src]# /usr/local/tomcat/bin/shutdown.sh Using CATALINA_BASE: /usr/local/tomcat Using CATALINA_HOME: /usr/local/tomcat Using CATALINA_TMPDIR: /usr/local/tomcat/temp Using JRE_HOME:/usr Using CLASSPATH: /usr/local/tomcat/bin/bootstrap.jar:/usr/local/tomcat/bin/tomcat-juli.jar 中间可以使用 px -aux|grep java 查看是否被杀死 [root@linux-node2 src]# /usr/local/tomcat/bin/startup.sh Using CATALINA_BASE: /usr/local/tomcat Using CATALINA_HOME: /usr/local/tomcat Using CATALINA_TMPDIR: /usr/local/tomcat/temp Using JRE_HOME:/usr Using CLASSPATH: /usr/local/tomcat/bin/bootstrap.jar:/usr/local/tomcat/bin/tomcat-juli.jar Tomcat started. 我们 JMX 端口设置为 8888 [root@linux-node2 src]# netstat -lntup Active Internet connections (only servers) Proto Recv-Q Send-Q Local Address Foreign Address State PID/Program name tcp0 0 0.0.0.0:22 0.0.0.0:* LISTEN 1073/sshd tcp0 0 127.0.0.1:250.0.0.0:* LISTEN 2498/master tcp0 0 0.0.0.0:10050 0.0.0.0:* LISTEN 10088/zabbix_agentd tcp6 0 0 :::8080 :::*LISTEN 26226/java tcp6 0 0 :::22 :::*LISTEN 1073/sshd tcp6 0 0 :::8888 :::*LISTEN 26226/java tcp6 0 0 ::1:25 :::*LISTEN 2498/master tcp6 0 0 :::10050:::*LISTEN 10088/zabbix_agentd tcp6 0 0 :::38532:::*LISTEN 26226/java tcp6 0 0 127.0.0.1:8005 :::*LISTEN 26226/java tcp6 0 0 :::8009 :::*LISTEN 26226/java tcp6 0 0 :::38377:::*LISTEN 26226/java 我们可以在 windows 上面安装 jdk ，使用命令行来监控 java 我们下载安装，具体步骤不说了，然后我们找到 jconsole.exe 文件运行 填写安装 JMX 的服务器 因为在配置文件中我们设置的是无密码认证，所以这里不需要输入密码直接连接。端口号我们设置的是 8888 连接即可 这样我们就可以在图形化监控 tomcat 提示：按照现在观察，java-gateway 已经安装成功，我们可以加入到 zabbix 中 找我们要添加的主机 填写安装 java-gateway 的主机 我们还需要设置一个模板 这个模板就是我们自带的一个监控 JMX 的模板，然后我们点击Update. 更新 我们需要等待一会才可以出图 提示：可以在 Zabbix-server 上使用 zabbix-get 获取某一台机器的某一个 key , 效果图如下：需要等待一会 手动检测监控状态 Zabbix-Server 操作： [root@linux-node1 ~]# yum install -y zabbix-get Key： 我们随便找一个 key，然后我们复制后面的 key [root@linux-node1 ~]# zabbix_get -s 192.168.56.12 -k jmx[&quot;java.lang:type=Runtime&quot;,Uptime] ZBX_NOTSUPPORTED: Unsupported item key. 提示：未支持的 key，现在并不能获取到这个 key 因为没有获取到这个值，所以不会显示。我们可以获取别的试一下 [root@linux-node1 ~]# zabbix_get -s 192.168.56.12 -k system.cpu.util[,user] 0.079323 [root@linux-node1 ~]# zabbix_get -s 192.168.56.12 -k system.cpu.util[,user] 0.075377 [root@linux-node1 ~]# zabbix_get -s 192.168.56.12 -k system.cpu.util[,user] 0.075377 [root@linux-node1 ~]# zabbix_get -s 192.168.56.12 -k system.cpu.util[,user] 0.073547 小结： Zabbix 其实就是通过 zabbix_get 获取到的这个值进行比较的 日志 开启 zabbix debug 模式 [root@linux-node2 tomcat]# systemctl restart zabbix-agent ### Option: DebugLevel # Specifies debug level: # 0 - basic information about starting and stopping of Zabbix processes # 1 - critical information # 2 - error information # 3 - warnings # 4 - for debugging (produces lots of information) # 5 - extended debugging (produces even more information) DebugLevel=4 如果及别是 4 就是 debug 模式，修改完配置文件之后需要重启生效 Zabbix 生产案例1. 开启 Nginx 监控 2. 编写脚本来进行数据采集 3. 设置用户自定义参数 4. 重启 zabbix-agent 5. 添加 item 6. 创建图形 7. 创建触发器 8. 创建模板 实践步骤 脚本编写： 我们这里提供已经写好的脚本, 链接：https://pan.baidu.com/s/19JrCetaRZYGY_mvq4CyoJQ 密码：94us 需要修改一下 zabbix-agent 的配置文件 vim /etc/zabbix/zabbix_agentd.conf 修改 Include 设置, 这样我们可以把脚本放在这个目录下。配置就是.conf 结尾 Include=/etc/zabbix/zabbix_agentd.d/*.conf 3. 添加权限及测试脚本 [root@linux-node1 zabbix_agentd.d]# chmod +x zabbix_linux_plugin.sh [root@linux-node1 zabbix_agentd.d]# sh zabbix_linux_plugin.sh Usage: zabbix_linux_plugin.sh {tcp_status key|memcached_status key|redis_status key|nginx_status key} 提示： 这个脚本要用 zabbix 用户执行的权限，因为都是 zabbix 用户在执行，监控 TCP 会在 /tmp/ 目录生成一个文件用于监控使用 4. 修改 nginx 配置文件 提示：nginx 默认路径是 /usr/local/nginx 编译安装需要查看安装路径 [root@linux-node1 zabbix_agentd.d]# vim /usr/local/nginx/conf/nginx.conf location /nginx_status { stub_status on; allow 127.0.0.1; access_log off; } 因为脚本的 url 是 nginx_status 所以我们配置文件也要这样修改 测试脚本 [root@linux-node1 zabbix_agentd.d]# curl 192.168.56.11:8080/nginx_status Active connections: 1 server accepts handled requests 2823682 2823682 2821835 Reading: 0 Writing: 1 Waiting: 0 [root@linux-node1 zabbix_agentd.d]# ./zabbix_linux_plugin.sh nginx_status 8080 active 1 [root@linux-node1 zabbix_agentd.d]# ./zabbix_linux_plugin.sh nginx_status 8080 reading 0 [root@linux-node1 zabbix_agentd.d]# ./zabbix_linux_plugin.sh nginx_status 8080 handled 2823688 设置 Key，首先是Key 的名称 [root@linux-node1 zabbix_agentd.d]# cat linux.conf UserParameter=linux_status[*],/etc/zabbix/zabbix_agentd.d/zabbix_linux_plugin.sh &quot;$1&quot; &quot;$2&quot; &quot;$3&quot; [*]代表一个传参，可以将后面的 $1,$2,$3 引入进行 ，后面是脚步本的路径 需要重启 agent [root@linux-node1 zabbix_agentd.d]# systemctl restart zabbix-agent 我们使用 zabbix_get 进行测试 [root@linux-node1 zabbix_agentd.d]# zabbix_get -s 192.168.56.11 -k linux_status[nginx_status,8080,active] 1 [-k] 就是指定 key 不细说了 [*] * 的作用在 web 界面配置 item 会显示出来 5.Zabbix web 界面设置 我们需要添加 item，因为要加好多。我们就使用模板的方式进行添加 提示：我们写一下注释然后选择 Add 即可 找到我们的模板 我们创建 item 创建 各参数前文都有讲解不细说！ 修改完成吼点击 Add 添加完成后我们要复制很多个用来监控 Nginx status 的所有状态，所以我们使用克隆。来克隆多个进行设置 点进我们的 item，然后拖到最下面选择克隆 填一些基本的修改即可，例如下： 添加完成如下图： item 添加完成我们还需要添加一个图形，用于展示，找到图形路径。点击创建 因为我们主机还没有加入我们的模板，所以我们这里是没有数据的 下面将模板加入到主机中 修改模板 查看结果如下： 6. 导出模板 因为设置模板比较麻烦，我们可以将模板导出 导出之后我们需要修改名称就可以了 7. 导入模板 我们需要导出自然需要导入，操作如下： 点击添加即可 提示： 模板之间的名称不可以相同 以上就是 Nginx 完整的监控使用8. 导入 TCP 模板 加入模板的步骤跟刚刚加入 Nginx 的一样，这里我们就使用模板了。下载链接：http://pan.baidu.com/s/1i54ULjJ 密码：25lh我们导入模板即可 导入完成之后我们可以查看模板 在里面我们可以见到 TCP 的 11 种状态，这个 item 是我们需要根据我们脚本进行同步的。 我们可以随便点击一个进行查看，其中这里的 key 要和脚本的相同 我们在两台服务器都加载这个模板 步骤和上面的一样 添加完成 查看脚本需要等待 1 分钟，这主要看我们设置的获取值的时间而定。我们可以查看图形 更多内容请看下集！~ 转载自：Zabbix 3.0 生产案例 [四]]]></content>
      <categories>
        <category>zabbix</category>
      </categories>
      <tags>
        <tag>zabbix</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Zabbix 3.0 部署监控 [三]]]></title>
    <url>%2F2016%2F10%2F11%2FZabbix%203.0%20%E9%83%A8%E7%BD%B2%E7%9B%91%E6%8E%A7%20%5B%E4%B8%89%5D%20%EF%BC%88%E8%BD%AC%EF%BC%89%2F</url>
    <content type="text"><![CDATA[Dashboard 首页信息介绍 Status of Zabbix（Zabbix 状态）介绍 Zabbix server is running #Zabbix 服务器是否运行 Number of hosts (enabled/disabled/templates) #主机数量（已启用 / 已禁用 / 模板） Number of items (enabled/disabled/not supported) #监控项数量（已启用 / 已禁用 / 不支持） Number of triggers (enabled/disabled [problem/ok]) #触发器数量（已启用 / 已禁用 / 问题 / 正常） Number of users (online) #用户数（线上） Required server performance, new values per second #要求的主机性能，每秒新值 此处需要注意的事项如下：1、需要时刻关注那些主机数量中已禁用的（例如：那一天有一台监控有问题，顺手关闭了。没有打开 结果后期导致监控出现问题） 2、监控项数量里面最好不要放置已禁用，要么删除这个监控项或者不让他报警。尽量不要给他禁用 3、触发器只禁用几个没什么大问题，但是如果一下禁用几十个不方便进行管理 4、正式环境最好划分主机组，可以按照业务划分，类型划分。那个出现问题都方便查看处理 Latest data 最新数据介绍 加入监控 刚刚之前我们一直使用的是一台服务器，因为不方便解释。我们新添加一台服务器 加入监控的几个步骤： 1、安装软件 2、修改配置 1、设置 yum 源[root@linux-node2 ~]# rpm -ivh http://mirrors.aliyun.com/zabbix/zabbix/3.0/rhel/7/x86_64/zabbix-release-3.0-1.el7.noarch.rpm 2、安装软件包[root@linux-node2 ~]# yum install -y zabbix-agent 3、修改配置文件[root@linux-node2 ~]# vim /etc/zabbix/zabbix_agentd.conf Server=192.168.56.11 ServerActive=192.168.56.11 #提示：这里的 IP 地址改成 Server 端的 IP 地址 4、启动[root@linux-node2 ~]# systemctl start zabbix-agent[root@linux-node2 ~]# netstat -lntup|grep zabbixtcp0 0 0.0.0.0:10050 0.0.0.0: LISTEN 10088/zabbix_agentdtcp6 0 0 :::10050:::LISTEN 10088/zabbix_agentd 5、web 界面设置 克隆~ 步骤：我们随便点击一个进去。拉到最下面有一个全部克隆 剩下的我们就改一下就可以了 模板修改 其他的就没有什么可以配置的，模板主要是添加 Template OS Linux。然后我们选择Add 即可 创建完成如下： 新添加的 IP 如上述所示 Maps 优化设置 上次只是简单的连接线的设置，这次我们进行深入设置 路径：Monitoring---&gt;Maps---&gt;Edit map进行修改 我们点击 Zabbix server 没有设置主机的，选择Host 修改linux-node2。 提示：此处我们修改了 2 台主机，这个可以根据业务需求进行设置 我们新添加一台，然后进行连接。Ctrl + 主机 然后点击 Link：Add 例如我们想查看他们的 流量带宽 首先，他们必须要连接在一起，然后点击 Links 选项后面的 Edit 进行编辑 我们可以在 Label 表里面写监控项的值 我们可以在 Configuration---&gt;Hosts---&gt;items 中查看到 括号内写入发下： {linux-node2.example.com:net.if.out[eth0].last(0)} linux-node2.example.com= 主机名 net.if.out=key 值 last（0）= 获取最新的一个数据 现在我们就可以实时的监控流量 切记需要update 保存如下图显示 如何让 Zabbix 报警 我们可以先打开Events 查看事件 zabbix 事件有很多类型 Trigger= 触发器的事件 Disovery= 自动发现事件 还有内部的事件以及自动注册的事件 我们可以选择 主机，查看相对应的事件 Zabbix的报警可以当做事件通知，当这个事件发生时。zabbix进行通知（报警） 事件报警分为 2 种方式： 1、怎么通知 2、通知给谁 Zabbix 通知方式：Zabbix 通知方式通过 Actions 进行通知 Zabbix默认有一个，我们可以点开进行查看 条件设置 操作设置 温馨提示：保存的时候需要先点击下方小的 Update 否则就木有啦, 这里的步骤可以让报警邮件发送的级别、例如：先发送给 运维、项目经理、项目总监 等 例如如下： 刚刚的填写完成，现在提示的是 1-2 发送的人 我们可以点击下面的 New 在添加几个 模拟设置，当报警 1-2 次时候发送给 XX，2-4次发送给XX。 依次叠加 我们需要配置报警媒介类型，用于发送邮件 温馨提示：3.0之前发送邮件需要启动邮件相关服务来进行安全认证，3.0之后默认自带安全认证 我们以 qq 邮箱为例 我们还需要配置 用户 的邮箱，因为上面已经选择发送给那个 用户。接下来就改配置用户的邮箱 我们点开之后选择Media（报警媒介进行设置）如果看不懂英文我们可以设置中文 然后我们选择下方的Add 设置收件人地址 小结：步骤就不截图了，可以调成中文，按照步骤来。 1、报警媒介 2、动作（active）配置（操作–编辑） 注意点小的 update 3、创建用户群组（注意权限） 4、创建用户（权限和报警媒介设置）权限只能按照用户组分配（我们可以选择用户 / 管理员 / 超级管理员） 提示：添加新主机后，要注意确认权限分配 我们的使用 QQ 邮箱需要开启 SNMP 和一个授权码。 填写发件人密码时需要设置授权码为密码 邮件结果如下：异常 因为我们开启了正常之后继续发送邮件，所以正常之后邮件如下 提示：当异常时它会一直发邮件，直到服务正常或者匹配规则到时 转载自：Zabbix 3.0 部署监控 [三]]]></content>
      <categories>
        <category>zabbix</category>
      </categories>
      <tags>
        <tag>zabbix</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Zabbix 3.0 部署监控 [二]]]></title>
    <url>%2F2016%2F10%2F11%2FZabbix%203.0%20%E5%9F%BA%E7%A1%80%E4%BB%8B%E7%BB%8D%20%5B%E4%BA%8C%5D%EF%BC%88%E8%BD%AC%EF%BC%89%2F</url>
    <content type="text"><![CDATA[一、添加监控主机及设置1. 创建主机 Agent 可以干一些 SNMP 无法干的事情，例如自定义监控项snmp 相关文章：http://www.abcdocker.com/abcdocker/1376 这里我们先不着急点add，还需要设置其他选项 点击监控模板 zabbix 监控是由 监控项 组成（cpu使用率监控就是一个 监控项 / 内存使用率 就是一个监控项），如果是 100 台服务器就需要监控 模板 了。只需要将监控项和模板 关联 起来即可 举个例子：我们上面主机使用的是 SNMP，就可以直接搜索SNMP。提示：有的模板需要自己定义 温馨提示：请点击下面的小add 然后在点大的。否则会出现问题哦 IPMI如果有的话，需要在这里写上 用户名 和密码 宏定义，这个宏其实就是一个变量。我们给可以给变量附一个值 因为我们设置的是 SNMP，SNMP 有一个团体名。并且可以设置定义 团体名是中间的abcdocker，具体的可以看http://www.abcdocker.com/abcdocker/1376 [root@localhost ~]# cat /etc/snmp/snmpd.conf rocommunity abcdocker 192.168.56.11 值：{$SNMP_COMMUNITY} 主机资产设置分为 3 中 1、关闭 Disabled2、手动 Manual3、自动 Automatic （自动代表的是你在定义监控项的时候，他有一个小箭头，勾上之后监控项的值就会填写在这里） 我们这设置好模板就可以选择add 了 等 SNMP 变绿就好了 现在的状态是用 SNMP 进行监控了，我们只是添加了一个 SNMP OS LINUX 的模板，但是出现了 4 个。这 4 个链接。可以和多个 模板 连起来用 进入监控项，下面这个菜单是过滤搜索用的 下面全都是模板 我们可以随便点击一个，这里我们新建一个监控项 点击创建 类型选择 Zabbix agent 被动 Zabbix agent (active 主动模式) Simple check 简单检测 SNMPv1 agent …… 在 Key 这行点击 Select 可以进行选择 我们随便选择一个，例如 agent.version。查看 agent 的版本Numeric 是无符号整数型 2. 图形说明Configuration----hosts----Graphs 绘图靠的是 监控项，我们可以随便打开一个看看 颜色等都是可以随意设置 3、聚合图形 screens 设置 提示 ：因为咱们用的版本是 3.0 当 2.4 的时候需要在Configuration---- 下面来创建screens 创建 Screens 我们创建一个 2*2 命名为test screens 的screens 然后我们点进去 点击 编辑 点击 Change 进行设置 多添加几个之后就是以下结果 二、监控案例 [自定义监控项] 例如 ：我们自己添加一个监控项来进行监控当前的活动连接数nginx 安装地址：http://www.abcdocker.com/abcdocker/1376Nginx 状态模块配置如下，过于简单不说了 [root@localhost ~]# cat /usr/local/nginx/conf/nginx.conf listen 8080; location /status { stub_status on; access_log off; allow 192.168.56.0/24; deny all; } 修改 nginx 端口并重启 测试：http://192.168.56.11:8080/status 解释说明：使用 zabbix 来监控活动连接数，通过 status 状态模块为前提 , 我们现在命令取出我们想要的值，例如： [root@localhost ~]# curl -s http://192.168.56.11:8080/status|grep Active|awk -F &quot;[]&quot; &apos;{print $3}&apos; 1 因为我们是监控他的活动连接数，他的活动连接数为1 [root@linux-node1 ~]# vim /etc/zabbix/zabbix_agentd.conf Include=/etc/zabbix/zabbix_agentd.d/ 提示： 如果想要加自定义监控项，不要在配置文件中写入，可以在 Include 里面定义的目录写上 , 只要我们写在 Include 目录下，都可以识别到 [root@linux-node1 ~]# cd /etc/zabbix/zabbix_agentd.d/ [root@linux-node1 zabbix_agentd.d]# ls userparameter_mysql.conf #默认有一个 MySQL 的，我们可以参考 MySQL 的进行操作 UserParameter=mysql.ping,HOME=/var/lib/zabbix mysqladmin ping | grep -c alive #提示，前面是 key 的名称 后面的 key 的命令 UserParameter=mysql.version,mysql -V 我们自己编辑一个文件 [root@linux-node1 zabbix_agentd.d]# cat nginx.conf UserParameter=nginx.active,/usr/bin/curl -s http://192.168.56.11:8080/status|grep Active|awk -F &quot;[]&quot; &apos;{print $3}&apos; 提示，此处配置文件的名字可以随便起 如果是多个命令可以写一个 脚本 ，命令最好写 绝对路径 ！这个过程其实就是我们定义监控的过程，前面是key 的名字，后面是命令 修改完配置文件之后需要重启zabbix-agent [root@linux-node1 zabbix_agentd.d]# systemctl restart zabbix-agent 配置完成之后先在 server 端测试，是否可以获取到 agent 上的值。不要着急添加 , 我们现在只用了 1 台服务器，本机是 server 也是 agent。然后使用 zabbix-get 进行 测试 [root@linux-node1 zabbix_agentd.d]# yum list|grep zabbix zabbix-agent.x86_64 3.0.4-1.el7@zabbix zabbix-release.noarch 3.0-1.el7 installed zabbix-server-mysql.x86_64 3.0.4-1.el7@zabbix zabbix-web.noarch 3.0.4-1.el7@zabbix zabbix-web-mysql.noarch 3.0.4-1.el7@zabbix python-pyzabbix.noarch 0.7.3-2.el7epel uwsgi-stats-pusher-zabbix.x86_642.0.13.1-2.el7 epel zabbix-get.x86_64 3.0.4-1.el7zabbix 查看 zabbix_get [root@linux-node1 zabbix_agentd.d]# yum install -y zabbix-get zabbix-get使用参数如下： [root@linux-node1 zabbix_agentd.d]# zabbix_get -s 192.168.56.11 -p 10050 -k &quot;nginx.active&quot; -s 指定我们要查看的服务器 -p 端口，可以不加。默认是 10050 -k 监控项的名称（根据上面的配置来定义的） 更多参数：zabbix_get --help 错误案例： 如果出现如下错误，大致意思是拒绝连接 [root@linux-node1 zabbix_agentd.d]# zabbix_get -s 192.168.56.11 -p 10050 -k &quot;nginx.active&quot; zabbix_get [24234]: Check access restrictions in Zabbix agent configuration 解决方法： [root@linux-node1 ~]# vim /etc/zabbix/zabbix_agentd.conf Server= 192.168.56.11 因为我们当时只允许本机 127.0.0.1 进行连接。所以会出现这样问题 [root@linux-node1 ~]# systemctl restart zabbix-agent 修改完配置文件都要 重启 提示： zabbix-agent 的配置文件中指定允许那个 server 连接，那个才可以进行连接。 [root@linux-node1 zabbix_agentd.d]# zabbix_get -s 192.168.56.11 -p 10050 -k &quot;nginx.active&quot; 1 正确结果如上！提示：如果在 zabbix-agent 上面修改了，还需要在网页上进行修改 在 /etc/zabbix/zabbix-agent.conf 上面指定的 Server 是谁，就只会允许谁通过。如果有多个 ip 可以使用逗号进行分割 添加 item 找到一个安装zabbix-agent，点击 点击items 然后添加Create item（创建 item） Data type：数据类型，这里我们选择 Decimal。其他的基本上用不上 Units：单位 超过 1 千就写成 1k 了。 可以在这里做一个单位的设置。默认就可以 Use custom multiplier：如果这里面设置了一个数，得出来的结果都需要乘以文本框设定的值 Update interval（in sec）:监控项刷新时间间隔（一般不要低于 60 秒） Custom intervals:创建时间间隔（例如：1 点 -7 点每隔多少秒进行监控）格式大致为：周，时，分 History storage period: 历史数据存储时间（根据业务来设置，默认就可以） Trend storage period: 趋势图要保存多久 New application: 监控项的组 application: 选择一个监控项组 Populates host inventory field: 资产，可以设定一个监控项。把获取的值设置在资产上面 描述！必须要写。 要不你就是不负责任 添加自定义监控项小结： 1、添加用户自定义参数（在 /etc/zabbix/zabbix.agent.d/ 定义了一个 nginx.conf 步骤如上） 2、重启zabbix-agent 3、在 Server 端使用 zabbix_get 测试 获取（命令如上） 4、在 web 界面创建item（监控项） 自定义图形 Name：名字 Width：宽度 Height：高度 Graph type：图形类型 其他 默认 即可 然后我们点击 Add 添加 Items 监控项，找到我们刚刚设置的服务器 然后找到我们刚刚添加的 监控项 还可以选择颜色，添加其他的很多设置。不细说 点击 Prewview 可以进行预览，如果出现字符乱码可以阅读我们另一篇文章（zabbix 默认不支持中文）, 确定没有问题，选择下方 Add 即可 出现我们添加的 需要在 Monitoring---&gt;Graphs---&gt; 选择我们添加的主机即可 接下来我们需要进行 测试 ： 测试前： 使用 ab 测试工具进行测试，设置 100 万 并发进行访问 [root@linux-node1 ~]# ab -c 1000 -n 1000000 http://192.168.56.11:8080/ This is ApacheBench, Version 2.3 &lt;$Revision: 1430300 $&gt; Copyright 1996 Adam Twiss, Zeus Technology Ltd, http://www.zeustech.net/ Licensed to The Apache Software Foundation, http://www.apache.org/ Benchmarking 192.168.56.11 (be patient) 测试后： 我们可以查看 zabbix 监控图标 我们中间设置了间隔 60 秒，说明每隔 60 秒 我们进行获取一次, 我们可以设置它的方式显示 找到 Graph 选择类型，Stacked是堆叠显示，其他的大家可以自行百度。不细说 堆叠显示如下： 如果我们想加多个图形都显示在一张图上，可以进行如下操作 找到Graphs 找到我们设置的图形 点击添加即可 我们可以让多个图标显示在一个图片上 点击我们创建一个聚合图形（screens） 点击进去 点击编辑 选择 item 添加的地方，因为上面创建聚合图形的时候我们选择了 2X2 所以这里会显示 2 个 找到相对应的添加即可 我们可以多添加几个 结果如上图显示 除了显示图片还可以显示其他内容 Action log：日志 Clock：时间 Data overview：数据概述 Graph：图形 History of events：历史事件 Host group issues：主机组问题 Host issues：主机问题 Hosts info：主机信息 Plain text：文本 Map：架构图 Screen：屏幕 Server info：服务器信息 Simple graph：简单的图 Simple graph prototype：简单的原型图 System status：系统状态 Triggers info：触发器信息 Tiggers overview：概述 URL：URL 地址 例如我们输入一个 URL： 我们还可以自定义一个Maps，一张架构图。操作如下： 第二步：选择编辑Edit map 因为他默认图片比较小，我们可以点击下方，进行调整图片大小。 点击右上角 编辑，然后我们点中图中的服务器即可 我们模拟有 2 台服务器 然后我们选中新添加的服务器进行修改 点击 Apply 就可以了。按住 Ctrl 点中 zabbix server 和另一台服务器 然后我们点击左上方的Link：他们就连接起来了 温馨提示：修改完成后需要点击保存 [update] 如果不点后果就是从新在做一遍~ 未完！ 转载自：Zabbix 3.0 部署监控 [二] | abcdocker 运维博客]]></content>
      <categories>
        <category>zabbix</category>
      </categories>
      <tags>
        <tag>zabbix</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Zabbix 3.0 基础介绍 [一]]]></title>
    <url>%2F2016%2F10%2F10%2FZabbix%203.0%20%E5%9F%BA%E7%A1%80%E4%BB%8B%E7%BB%8D%20%5B%E4%B8%80%5D%EF%BC%88%E8%BD%AC%EF%BC%89%2F</url>
    <content type="text"><![CDATA[摘要 本文主要讲述 Zabbix 的简介以及 Zabbix 安装及页面设置 Zabbix 3.0 基础介绍 [一]一、Zabbix 介绍zabbix 简介 Zabbix 是一个高度集成的网络监控解决方案，可以提供企业级的开源分布式监控解决方案，由一个国外的团队持续维护更新，软件可以自由下载使用，运作团队靠提供收费的技术支持赢利 zabbix 是一个基于 Web 界面的，提供分布式系统监控以及网络监视功能的企业级的开源解决方案。 zabbix 能监视各种网络参数，保证服务器系统的安全运营，并提供灵活的通知机制以让系统管理员快速定位 / 解决存在的各种问题 zabbix 主要由 2 部分构成 zabbix server 和 zabbix agent，可选组建 zabbix proxy zabbix server 可以通过 SNMP，zabbix agent，fping 端口监视等方法对远程服务器或网络状态完成监视，数据收集等功能。同时支持 Linux 以及 Unix 平台，Windows 平台只能安装客户端 Zabbix 功能 ①具备常见的商业监控软件所具备的功能（主机的性能监控、网络设备性能监控、数据库、性能监控、FTP 等通用协议监控、多种告警方式、详细的报表图表绘制） ②支持自动发现网络设备和服务器（可以通过配置自动发现服务器规则来实现） ③支持自动发现（low discovery）key 实现动态监控项的批量监控（需写脚本） ④支持分布式，能集中展示、管理分布式的监控点 ⑤扩展性强，server 提供通用接口（api 功能），可以自己开发完善各类监控（根据相关接口编写程序实现）编写插件容易，可以自定义监控项，报警级别的设置。 ⑥数据收集 可用和性能检测 支持 snmp(包括 trapping and polling)，IPMI，JMX，SSH，TELNET 自定义的检测 自定义收集数据的频率 服务器 / 代理和客户端模式 灵活的触发器 可以定义非常灵活的问题阈值，称为触发器，从后端数据库的参考值 高可定制的报警 发送通知，可定制的报警升级，收件人，媒体类型 通知可以使用宏变量有用的变量 自动操作包括远程命令 实时的绘图功能 监控项实时的将数据绘制在图形上面 WEB 监控能力 ZABBIX 可以模拟鼠标点击了一个网站，并检查返回值和响应时间 Api 功能 应用 api 功能，可以方便的和其他系统结合，包括手机客户端的使用。 更多功能请查看http://www.zabbix.com/documentation.php Zabbix 版本 Zabbix 3.0 Manual Zabbix 2.4 Manual Zabbix 2.2 Manual Zabbix 2.0 Manual 下载地址：http://www.zabbix.com/documentation.php 本次采用 yum 安装，安装 zabbix3.0. 使用 Centos7 Zabbix 优缺点 优点 1、开源，无软件成本投入 2、Server 对设备性能要求低 3、支持设备多，自带多种监控模板 4、支持分布式集中管理，有自动发现功能，可以实现自动化监控 5、开放式接口，扩展性强，插件编写容易 6、当监控的 item 比较多服务器队列比较大时可以采用被动状态，被监控客户端主动从 7、server 端去下载需要监控的 item 然后取数据上传到 server 端。这种方式对服务器的负载比较小。 8、Api 的支持，方便与其他系统结合 缺点 需在被监控主机上安装 agent，所有数据都存在数据库里，产生的数据据很大, 瓶颈主要在数据库。 Zabbix 监控原理 Zabbix 通过 C/S 模式采集数据，通过 B/S 模式在 web 端展示和配置。 被监控端：主机通过安装 agent 方式采集数据，网络设备通过 SNMP 方式采集数据 Server 端：通过收集 SNMP 和 agent 发送的数据，写入数据库（MySQL，ORACLE 等），再通过 php+apache 在 web 前端展示。 Zabbix 运行条件 Server：Zabbix Server 需运行在 LAMP（Linux+Apache+Mysql+PHP）环境下（或者 LNMP），对硬件要求低 Agent：目前已有的 agent 基本支持市面常见的 OS，包含 Linux、HPUX、Solaris、Sun、windows SNMP：支持各类常见的网络设备SNMP(Simple Network Management Protocol, 简单网络管理协议 Zabbix 监控过程逻辑图 Zabbix 监控类型 硬件监控：适用于物理机、远程管理卡（iDRAC），IPMI（只能平台管理接口） ipmitools:MegaCli（查看 Raid 磁盘） 系统监控: 监控 cpt：lscpu、uptime、top、vmstat 1 、mpstat 1、htop 监控内存： free -m 监控硬盘：df -h、iotop 监控网络：iftop、netstat、ss 应用服务监控：nfs、MySQL、nginx、apache、php、rsync 更详细的监控类型可以参考：http://www.abcdocker.com/abcdocker/1376 引入 Zabbix所有监控范畴，都可以整合到 Zabbix 中 硬件监控：Zabbix、IPMI、lnterface 系统监控：Zabbix、Agent、Interface Java 监控：Zabbix、JMX、lnterface 网络设备监控：Zabbix、SNMP、lnterface 应用服务监控：Zabbix、Agent、UserParameter MySQL 数据库监控：percona-monitoring-plulgins URL 监控：Zabbix Web 监控 ## 二、Zabbix 环境配置 1、环境信息 [root@localhost ~]# cat /etc/redhat-release CentOS Linux release 7.2.1511 (Core) [root@localhost ~]# uname -r 3.10.0-327.28.3.el7.x86_64 2、yum 安装 阿里云 yum 源已经提供了 zabbix3.0，因此我们需要使用官方 yum 源。官方 yum 源下载会比较慢 [root@localhost ~]# rpm -ivh http://mirrors.aliyun.com/zabbix/zabbix/3.0/rhel/7/x86_64/zabbix-release-3.0-1.el7.noarch.rpm 问题：为什么要下载 release 版本的 zabbix？ [root@localhost ~]# ls /etc/yum.repos.d/ CentOS-Base.repo CentOS-Media.repo epel.repo.rpmnew CentOS-CR.repo CentOS-Sources.repo epel-testing.repo CentOS-Debuginfo.repo CentOS-Vault.repo zabbix.repo CentOS-fasttrack.repo epel.repo 因为下载这个版本会在 yum.repos.d 下面生成一个 zabbix.repo 的文件 3、安装相关软件包 [root@localhost ~]# yum install zabbix-server zabbix-web zabbix-server-mysql zabbix-web-mysql mariadb-server mariadb -y 注：如果 Server 端也需要监控则需要安装 zabbix-agent 提示：在 Centos7 中，mysql 改名为 mariadb 4、修改 PHP 时区设置 [root@localhost ~]# sed -i &apos;s@# php_value date.timezone Europe/Riga@php_value date.timezone Asia/Shanghai@g&apos; /etc/httpd/conf.d/zabbix.conf #要注意需要改的配置文件是 /etc/httpd/conf.d/zabbix.conf 而不是 /etc/php.ini， 三、数据库设置1. 启动数据库 [root@localhost ~]# systemctl start mariadb 2. 创建 zabbix 数据库及用户 mysql create database zabbix character set utf8 collate utf8_bin; grant all on zabbix.* to zabbix@&apos;localhost&apos; identified by &apos;123456&apos;; exit 3. 导入数据 [root@localhost ~]# cd /usr/share/doc/zabbix-server-mysql-3.0.4/ [root@localhost zabbix-server-mysql-3.0.4]# ll total 1836 -rw-r--r-- 1 root root 98 Jul 22 11:05 AUTHORS -rw-r--r-- 1 root root 687803 Jul 22 11:05 ChangeLog -rw-r--r-- 1 root root 17990 Jul 22 11:06 COPYING -rw-r--r-- 1 root root 1158948 Jul 24 02:59 create.sql.gz -rw-r--r-- 1 root root 52 Jul 22 11:06 NEWS -rw-r--r-- 1 root root 188 Jul 22 11:05 README [root@localhost zabbix-server-mysql-3.0.4]# zcat create.sql.gz |mysql -uzabbix -p123456 zabbix 我们使用 zcat，专门查看 sql.gz 包。和 cat 基本相似 4. 修改 zabbix 配置文件 [root@localhost zabbix-server-mysql-3.0.4]# vim /etc/zabbix/zabbix_server.conf DBHost=localhost #数据库所在主机 DBName=zabbix #数据库名 DBUser=zabbix #数据库用户 DBPassword=123456 #数据库密码 5. 启动 zabbix 及 apache [root@localhost ~]# systemctl start zabbix-server [root@localhost ~]# systemctl start httpd 注意：如果没有启动成功，要看一下是不是 80 端口被占用 6.Web 界面安装 master访问地址：http://192.168.56.11/zabbix/setup.php 点击 Next step 进行安装 首先要确保没有no，如果时区没有改好会提示我们进行修改 账号密码都是我们刚刚在配置文件中设置的，端口默认就是 3306 为我们的 zabbix 起个名字，一会在右上角会显示 最后是展示我们的配置信息，可以查看到哪里有错误 点击 Finish 提示：登录上去之后请立即修改密码 7. 配置 zabbix-agent 端 [root@localhost ~]# vim /etc/zabbix/zabbix_agentd.conf Server=127.0.0.1 修改 Server 端的 IP 地址（被动模式 IP 地址） ServerActive=127.0.0.1 主动模式，主动向 server 端报告 [root@localhost ~]# systemctl start zabbix-agent 查看端口号 [root@localhost ~]# netstat -lntp Active Internet connections (only servers) Proto Recv-Q Send-Q Local Address Foreign Address State PID/Program name tcp 0 0 0.0.0.0:3306 0.0.0.0:* LISTEN 7806/mysqld tcp 0 0 0.0.0.0:22 0.0.0.0:* LISTEN 1062/sshd tcp 0 0 127.0.0.1:25 0.0.0.0:* LISTEN 2208/master tcp 0 0 0.0.0.0:10050 0.0.0.0:* LISTEN 11511/zabbix_agentd tcp 0 0 0.0.0.0:10051 0.0.0.0:* LISTEN 11335/zabbix_server tcp 0 0 127.0.0.1:199 0.0.0.0:* LISTEN 2692/snmpd tcp6 0 0 :::80 :::* LISTEN 11408/httpd tcp6 0 0 :::22 :::* LISTEN 1062/sshd tcp6 0 0 ::1:25 :::* LISTEN 2208/master tcp6 0 0 :::443 :::* LISTEN 11408/httpd tcp6 0 0 :::10050 :::* LISTEN 11511/zabbix_agentd tcp6 0 0 :::10051 :::* LISTEN 11335/zabbix_server 10051 为 server 端口，10050 为 agent 端口 四、Web 界面配置 找到 Configuration—-&gt;Hosts 添加一台监控主机 开启后，如果出现错误我们可以看一下 zabbix 的日志 [root@localhost ~]# ls /var/log/zabbix/zabbix_ zabbix_agentd.log zabbix_server.log 当 ZBX 变成绿色的时候，说明监控成功。因为我们没有配置 SNMP、JMX、IPMI 等。所以我发监控 因为我们现在只安装了一台服务器，所以只有一个主机。我们可以查看现在这台主机的 CPU 等及基本的信息 点击 Monitoring—–Graphs，选择我们要监控的内容 我们选择可以随便选择一个进行查看信息 例如：我们查看 CPU 的负载 某一段时间内，CPU 正在处理以及等待 CPU 处理的进程数的之和。Load Average 是从另一个角度来体现 CPU 的使用状态的。 这些监控其实就是 zabbix 在数据库查找数据，然后使用 jd 进行画图Zabbix 性能依赖于 mysql 数据库 五、Zabbix 页面安全设置1、设置默认账号密码 设置完中文 六、Zabbix 菜单说明Zabbix 上方的菜单简单介绍说明 Doshboard 下面可以设置你想设置的图形，添加方法如下： 这时，就可以找到你喜爱的了，直接打开 screens 其实就是一个聚合图形，可以把多个图片合在一起。然后放在大屏幕上，供别人查看 maps 就是一个架构图 Status of Zabbix 就是一个状态栏 第一行是 Server 是否运行 [yes] 和后面的运行地址 第二行监控的机器 （启用的 / 关闭的 / 模板） 第三行监控项 （启用的 / 关闭的 / 不支持的） 第四行触发器的状态 （启用的 / 关闭的 /【故障 / 正常】） 第五行 当前用户数量 （在线数量） 第六行 zabbix 每秒可以收到的一个新值 告警的级别 我们可以设置报警响铃，让他在前端响 我们首页的监控列表是可以随意拖动的 我们还可以将它关闭，并且设置刷新时间 Zabbix 基础完! 转载自：Zabbix 3.0 基础介绍 [一] | abcdocker 运维博客]]></content>
      <categories>
        <category>zabbix</category>
      </categories>
      <tags>
        <tag>zabbix</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo NexT 主题内接入网页在线联系功能]]></title>
    <url>%2F2015%2F02%2F26%2FHexo%20NexT%E4%B8%BB%E9%A2%98%E5%86%85%E6%8E%A5%E5%85%A5%E7%BD%91%E9%A1%B5%E5%9C%A8%E7%BA%BF%E8%81%94%E7%B3%BB%E5%8A%9F%E8%83%BD%2F</url>
    <content type="text"><![CDATA[之前有访问过一些大佬的个人博客，里面有个在线联系功能，看着不错，所以也试着在自己的站点上接入了此功能。 注册 首先在 DaoVoice 注册个账号，点击 -&gt;邀请码 是2e5d695d。 完成后，会得到一个app_id，后面会用到： 修改 head.swig修改 /themes/next/layout/_partials/head.swig 文件，添加内容如下： {% if theme.daovoice %} (function(i,s,o,g,r,a,m){i["DaoVoiceObject"]=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;a.charset="utf-8";m.parentNode.insertBefore(a,m)})(window,document,"script",('https:' == document.location.protocol ? 'https:' : 'http:') + "//widget.daovoice.io/widget/0f81ff2f.js","daovoice") daovoice('init', {app_id: "{{theme.daovoice_app_id}}" }); daovoice('update'); {% endif %} 位置贴图： 主题配置文件 在_config.yml文件中添加内容： # Online contact daovoice: true daovoice_app_id: # 这里填你刚才获得的 app_id 聊天窗口配置 附上我的聊天窗口的颜色、位置等设置信息： 至此，网页的在线联系功能已经完成，重新 hexo g，hexo d 上传 GitHub 后，页面上就能看到效果了。 就比如说你现在往右下角看看(～￣▽￣)～ ，欢迎撩我（滑稽）。]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo NexT 主题内加入动态背景]]></title>
    <url>%2F2015%2F02%2F25%2FHexo%20NexT%E4%B8%BB%E9%A2%98%E5%86%85%E5%8A%A0%E5%85%A5%E5%8A%A8%E6%80%81%E8%83%8C%E6%99%AF%2F</url>
    <content type="text"><![CDATA[主题内新添加内容 _layout.swig 找到 themes\next\layout\_layout.swig 文件，添加内容：在 &lt;body&gt; 里添加： &lt;div class=&quot;bg_content&quot;&gt; &lt;canvas id=&quot;canvas&quot;&gt;&lt;/canvas&gt; &lt;/div&gt; 仍是该文件，在末尾添加： &lt;script type=&quot;text/javascript&quot; src=&quot;/js/src/dynamic_bg.js&quot;&gt;&lt;/script&gt; dynamic_bg.js 在 themes\next\source\js\src 中新建文件dynamic_bg.js，代码链接中可见：dynamic_bg.js custom.styl在 themes\next\source\css\_custom\custom.styl 文件末尾添加内容： .bg_content { position: fixed; top: 0; z-index: -1; width: 100%; height: 100%; } 以上整理主要参照下面的文档，如涉及侵权请联系本人，进行删除。 参考： Hexo NexT 主题内加入动态背景]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[OSSFS 实现阿里云 OSS 文件系统数据共享]]></title>
    <url>%2F2015%2F01%2F24%2FOSSFS%E5%AE%9E%E7%8E%B0%E9%98%BF%E9%87%8C%E4%BA%91OSS%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F%E6%95%B0%E6%8D%AE%E5%85%B1%E4%BA%AB%2F</url>
    <content type="text"><![CDATA[阿里云 ESC 服务器挂载 OSS 文件系统 ossfs 能让您在 Linux/Mac OS X 系统中把 Aliyun OSS bucket 挂载到本地文件 系统中，您能够便捷的通过本地文件系统操作 OSS 上的对象，实现数据的共享。 阿里云 oss 官方：ossfs 挂载，您可以理解为把挂载的 bucket 当做一个 ecs 目录来操作的，存储文件到挂载的 bucket 中是占用的这个 bucket 的内存，不会占用您 ecs 的内存。 安装 下载文件 ossfs_1.80.3_centos7.0_x86_64.rpm 到阿里云 安装sudo yum localinstall ossfs_1.80.3_centos7.0_x86_64.rpm 写入 oss 配置echo my-bucket:my-access-key-id:my-access-key-secret &gt; /etc/passwd-ossfs, 例： echo ossfs-xuan:LTAIw5M5SHnIoNcm:ci1Oj7*******ZqDziBj &gt; /etc/passwd-ossfs 更改配置文件权限chmod 640 /etc/passwd-ossfs 创建挂载目录mkdir /ossfs 挂载ossfs ossfs-xuan /ossfs -ourl=oss-cn-shenzhen-internal.aliyuncs.com 额外的命令# 允许 linux 其他用户对改 oss 文件系统进行操作 ossfs ossfs-xuan /ossfs -ourl=oss-cn-shenzhen-internal.aliyuncs.com -o allow_other #卸载挂载 oss 目录 umount /ossfs 可能出现的错误 InvalidBucketName 错误可以看出 BucketName 重复了 ossfs: bad request &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;Error&gt; &lt;Code&gt;InvalidBucketName&lt;/Code&gt; &lt;Message&gt;The specified bucket is not valid.&lt;/Message&gt; &lt;RequestId&gt;5A93BFD701A3E286AC09FDDD&lt;/RequestId&gt; &lt;HostId&gt;ossfs-xuan.ossfs-xuan.oss-cn-shenzhen-internal.aliyuncs.com&lt;/HostId&gt; &lt;BucketName&gt;ossfs-xuan.ossfs-xuan&lt;/BucketName&gt; &lt;/Error&gt; 解决：-ourl=oss-cn-shenzhen-internal.aliyuncs.com不需要带BucketName]]></content>
      <categories>
        <category>阿里云</category>
      </categories>
      <tags>
        <tag>阿里云</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CentOS7.4 安装 GlusterFS]]></title>
    <url>%2F2015%2F01%2F23%2FCentOS7.4%E5%AE%89%E8%A3%85GlusterFS%2F</url>
    <content type="text"><![CDATA[介绍 Gluster 是一个大尺度文件系统。 主要功能 简单卷 distribute volume 分布式卷，两台主机的磁盘融合一个磁盘 stripe volume 条带卷，一个文件分成数据块存储到不同的地方 replica volume 复制卷，一个文件分别保存到两台主机 复合卷 1+2，1+3，2+3，1+2+3 总结常用命令gluster peer status #查看集群各主机连接状态 gluster volume list# 查看挂载卷信息 gluster volume list #查看卷列表 #创建挂在卷，force 忽略在 root 目录创建挂在卷的警告 gluster volume create swarm-volume replica 3 worker:/xuan/docker/gluster-volume home:/xuan/docker/gluster-volume xuanps:/xuan/docker/gluster-volume force gluster volume start swarm-volume #启动 gluster volume stop swarm-volume #停止 gluster volume delete swarm-volume #删除 ，了文件还会保留 #挂载本地目录到 glusterfs 卷（swarm-volume），在本地目录添加的会自动同步到其他挂载卷 #eg 在本机 mnt 添加文件，其他 volume-name 目录也会添加 mount [- 参数] [设备名称] [挂载点] mount -t glusterfs worker:/swarm-volume /mnt/ umount worker:/swarm-volume #卸载了就不会同步了 #重置，删除所有数据 systemctl stop glusterd rm -rf /var/lib/glusterd/ systemctl start glusterd #删除节点 gluster peer detach home 安装 准备工作： 三台局域网主机（centos7 修改主机名 ） hostnameip备注xuanpsleft-aligned10.14.0.1 workercentered10.14.0.4homeright-aligned10.14.0.5 三台都需要安装 GlusterFS # 搜索 glusterfs 可安装的版本 yum search centos-release-gluster #安装最新长期稳定版本 (Long Term Stable) 的 gluster 软件 yum -y install centos-release-gluster #安装 glusterfs-server yum --enablerepo=centos-gluster*-test install glusterfs-server glusterfs -V #测试 systemctl enable glusterd #开机启动 systemctl start glusterd #启动 systemctl status glusterd #查看是否正常运行 #修改 hosts 不然不能通过主机名连接到对方 vim /etc/hosts #---------- 三台都要添加如下设置 -------------------------- 10.14.0.1 xuanps 10.14.0.4 worker 10.14.0.5 home #------------------------------------------------------ #从 xuanps 上执行下面两条，其他主机不用执行 gluster peer probe worker gluster peer probe home #三台都执行该命令是否都是 connected gluster peer status #查看挂载卷信息 gluster volume info #创建挂在卷，force 忽略在 root 目录创建挂在卷的警告 gluster volume create volume-name replica 3 worker:/xuan/docker/gluster-volume/test home:/xuan/docker/gluster-volume/test xuanps:/xuan/docker/gluster-volume/test force #启动 gluster volume start volume-name #启动 nfs 同步，测试需验证，这里要不要开启 gluster volume set volume-name nfs.disable off #挂载本地目录到 glusterfs 卷（volume-name），在本地目录添加的会自动同步到其他挂载卷 #eg 在本机 mnt 添加文件，其他 volume-name 目录也会添加 mount -t glusterfs worker:/volume-name /mnt/ 以上整理主要参照下面的文档，如涉及侵权请联系本人，进行删除。 参考 官方文档 centos 官方安装手册 基于 GlusterFS 实现 Docker 集群的分布式存储]]></content>
      <categories>
        <category>服务器</category>
      </categories>
      <tags>
        <tag>CentOS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CentOS7.4 安装 OpenVpn]]></title>
    <url>%2F2015%2F01%2F22%2FCentOS7.4%E5%AE%89%E8%A3%85OpenVpn%2F</url>
    <content type="text"><![CDATA[openvpn service 安装与配置1. 下载脚本 wget https://git.io/vpn -O openvpn-install.sh# 添加执行权限 chmod +x openvpn-install.sh #总结 wget https://git.io/vpn -O openvpn-install.sh &amp;&amp; bash openvpn-install.sh 2. 运行脚本./openvpn-install.sh, 设置如下 监听地址设置为空 IP address: Protocol:[2]TCP Port:1194 不选 DNS: client name: client_k2 External IP : 112.74.51.136 3. 配置服务端 vim /etc/openvpn/server.conf# 指定 ip, 所以记录 ip 没效果屏蔽 ;ifconfig-pool-persist ipp.txt ;push &quot;redirect-gateway def1 bypass-dhcp&quot; #推送服务器路由 push &quot;route 10.14.0.0 255.255.255.0&quot; #推送 k2 客户端子网路由到所有客户端除了 ccd 里面申明了该路由的客户端 push &quot;route 192.168.123.0 255.255.255.0&quot; #添加服务器路由，访问客户端 K2 的 192.168.123.0 子网通过网关 10.14.0.2(k2 客户端 ip) route 192.168.123.0 255.255.255.0 10.14.0.2 #添加客户端配置目录，启用之后，每个客户端必须指定 ip，否正有可能访问不了其他客户端的子网 client-config-dir ccd #客户端访问客户端 client-to-client 4. 配置客户端路由 mkdir /etc/openvpn/ccd 和 vim /etc/openvpn/ccd/client_k2# 设置该客户端的 vpn 的 ip 是 10.14.0.2, 子网掩码必须是 255.255.255.0，如果启用 ccd，必须配置 ifconfig-push 10.14.0.2 255.255.255.0 #申明 192.168.123.0 是自己的子网，并且让子网也可以访问 vpn 服务器，申明之后不会推送该路由到该客户端 iroute 192.168.123.0 255.255.255.0 route 192.168.123.0 255.255.255.0 5. 添加客户端./openvpn-install.shSelect an option[1-4]:1 (add a new user) client name: client_worker # 编辑配置文件 vim /etc/openvpn/server.conf #重启生效 systemctl restart openvpn@server.service systemctl enable openvpn@server.service #注释掉客户端的 #setenv opt block-outside-dns 6. 下载 ovpn 文件，并修改配置，注释调 #setenv opt block-outside-dns 7. 常用命令# 重启生效 systemctl restart openvpn@server.service #使能服务 systemctl enable openvpn@server.service #ssh 下载文件 scp root@112.74.51.136:/root/client_xuan_ubuntu.ovpn ./ openvpn client 安装与配置1. 安装yum update #更新 yum install vim #安装 vim yum install epel-release #添加 epel 源 yum clean all # 可选 yum update # 可选 yum makecache # 可选 yum install openvpn iptables-services #安装 openvpn scp root@112.74.51.136:~/client_vm.ovpn /etc/openvpn/client/ #下载客户端配置 #注释掉客户端的 vim /etc/openvpn/client/client_vm.ovpn #setenv opt block-outside-dns #----------------------- 废弃 ------------------------------------------------ openvpn --daemon --cd /etc/openvpn/client --config client_vm.ovpn --log-append /etc/openvpn/openvpn.log #启动 tail -100f /etc/openvpn/openvpn.log #查看日志 ps -ef | grep openvpn #查看 openvpn 进程 kill &lt;pid&gt; #杀死进程 #--------------------- 废弃结束 ------------------------------------------------------ #openvpn-client 启动服务，反斜杠转义字符，实际名称是 openvpn-client@.service vim /lib/systemd/system/openvpn-client\@.service #修改 ExecStart=/usr/sbin/openvpn --suppress-timestamps --nobind --config %i.conf #为 ExecStart=/usr/sbin/openvpn --daemon --config %i.ovpn --log-append /etc/openvpn/openvpn.log #防止已经启动，@符号后面等效与 %i, 所以这里为客户端配置的名字 systemctl restart openvpn-client@client_vm #开机启动 systemctl enable openvpn-client@client_vm 以上整理主要参照下面的文档，如涉及侵权请联系本人，进行删除。 参考 官网 脚本 github 官网 Nyr/openvpn-install openvpn 的一个一键安装脚本“openvpn-install”让 openvpn 重放光彩（需翻墙） How to Configure OpenVPN Server on CentOS 7.3 使用 OpenVPN 互联多地机房及 Dokcer 跨主机 / 机房通讯 扩大 OpenVPN 使用范围，包含服务器或客户端子网中的其他计算机]]></content>
      <categories>
        <category>服务器</category>
      </categories>
      <tags>
        <tag>CentOS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CentOS7 修改网卡为 eth0]]></title>
    <url>%2F2015%2F01%2F22%2FCentOS7%E4%BF%AE%E6%94%B9%E7%BD%91%E5%8D%A1%E4%B8%BAeth0%2F</url>
    <content type="text"><![CDATA[使用 CentOS-7 最直观的变化就是服务管理了。这里介绍一下。 services 使用了 systemd 来代替 sysvinit 管理 systemd 是 Linux 下的一种 init 软件，由 Lennart Poettering 带头开发，并在 LGPL 2.1 及其后续版本许可证下开源发布。其开发目标是提供更优秀的框架以表示系统服务间的依赖关系，并依此实现系统初始化时服务的并行启动，同时达到降低 Shell 的系统开销的效果，最终代替现在常用的 System V 与 BSD 风格 init 程序。 与多数发行版使用的 System V 风格 init 相比，systemd 采用了以下新技术：采用 Socket 激活式与总线激活式服务，以提高相互依赖的各服务的并行运行性能；用 cgroups 代替 PID 来追踪进程，以此即使是两次 fork 之后生成的守护进程也不会脱离 systemd 的控制。从设计构思上说，由于 systemd 使用了 cgroup 与 fanotify 等组件以实现其特性，所以只适用于 Linux。systemd 的服务管理程序：systemctl 是主要的工具，它融合之前 service 和 chkconfig 的功能于一体。可以使用它永久性或只在当前会话中启用 / 禁用服务。 启动一个服务：systemctl start postfix.service 关闭一个服务：systemctl stop postfix.service 重启一个服务：systemctl restart postfix.service 显示一个服务的状态：systemctl status postfix.service 在开机时启用一个服务：systemctl enable postfix.service 在开机时禁用一个服务：systemctl disable postfix.service 查看服务是否开机启动：systemctl is-enabled postfix.service;echo $? 查看已启动的服务列表：systemctl list-unit-files|grep enabled CentOS7 修改网卡为 eth0编辑网卡信息[root@linux-node2~]# cd /etc/sysconfig/network-scripts/ #进入网卡目录 [root@linux-node2network-scripts]# mv ifcfg-eno16777728 ifcfg-eth0 #重命名网卡名称 [root@linux-node2network-scripts]# cat ifcfg-eth0 #编辑网卡信息 TYPE=Ethernet BOOTPROTO=static DEFROUTE=yes PEERDNS=yes PEERROUTES=yes IPV4_FAILURE_FATAL=no NAME=eth0 #name 修改为 eth0 ONBOOT=yes IPADDR=192.168.56.12 NETMASK=255.255.255.0 GATEWAY=192.168.56.2 DNS1=192.168.56.2 修改 grub[root@linux-node2~]# cat /etc/sysconfig/grub #编辑内核信息, 添加红色字段的 GRUB_TIMEOUT=5 GRUB_DEFAULT=saved GRUB_DISABLE_SUBMENU=true GRUB_TERMINAL_OUTPUT=&quot;console&quot; GRUB_CMDLINE_LINUX=&quot;crashkernel=auto rhgb net.ifnames=0 biosdevname=0 quiet&quot; GRUB_DISABLE_RECOVERY=&quot;true&quot; [root@linux-node2~]# grub2-mkconfig -o /boot/grub2/grub.cfg #生成启动菜单 Generatinggrub configuration file ... Foundlinux image: /boot/vmlinuz-3.10.0-229.el7.x86_64 Foundinitrd image: /boot/initramfs-3.10.0-229.el7.x86_64.img Foundlinux image: /boot/vmlinuz-0-rescue-1100f7e6c97d4afaad2e396403ba7f61 Foundinitrd image: /boot/initramfs-0-rescue-1100f7e6c97d4afaad2e396403ba7f61.img Done 也可以在开机启动加载安装系统界面设置。 验证是否修改成功[root@linux-node2~]# reboot #必须重启系统生效 [root@linux-node2~]# yum install net-tools #默认 centos7 不支持 ifconfig 需要看装 net-tools 包 [root@linux-node2~]# ifconfig eth0 #在次查看网卡信息 eth0:flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt; mtu 1500 inet 192.168.56.12 netmask 255.255.255.0 broadcast 192.168.56.255 inet6 fe80::20c:29ff:fe5c:7bb1 prefixlen 64 scopeid 0x20&lt;link&gt; ether 00:0c:29:5c:7b:b1 txqueuelen 1000 (Ethernet) RX packets 152 bytes 14503 (14.1 KiB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 98 bytes 14402 (14.0 KiB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 设置主机名解析[root@linux-node1 ~]# cat /etc/hosts 127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4 ::1 localhost localhost.localdomain localhost6 localhost6.localdomain6 192.168.56.11 linux-node1 linux-node1.example.com 192.168.56.12 linux-node2 linux-node2.example.com centos 7 修改主机名的方法 hostnamectl 命令 在 7 版本中，hostname 有三种形式 静态 (Static host name) 动态 (Transient/dynamic host name) 别名(Pretty host name) 查询主机名 hostnamectl 或 hostctl status 查询主机名 hostnamectl status [--static|--transient|--pretty] 修改 hostname hostnamectl set-hostname servername [--static|--transient|--pretty] 删除 hostname hostnamectl set-hostname &quot;&quot; hostnamectl set-hostname &quot;&quot; --static hostnamectl set-hostname &quot;&quot; --pretty 修改配置文件 hostname name vim /etc/hostname 通过 nmtui 修改，之后重启 hostnamed systemctl restart systemd-hostnamed 通过 nmcui 修改，之后重启 hostnamed nmcli general hostname servername systemctl restart systemd-hostnamed 安装 EPEL 仓库和常用命令[root@linux-node1 ~]# rpm -ivh http://mirrors.aliyun.com/epel/epel-release-latest-7.noarch.rpm [root@linux-node1 ~]# yum install -y net-tools vim lrzsz tree screen lsof tcpdump 关闭 NetworkManager 和防火墙[root@linux-node1 ~]# systemctl stop firewalld #关闭防火墙 [root@linux-node1 ~]# systemctl disable firewalld #设置开机不启动 [root@linux-node1 ~]# systemctl stop NetworkManager 关闭 SELinux[root@linux-node1 ~]# vim /etc/sysconfig/selinux SELINUX=disabled #修改为 disabled 检查结果如下 [root@linux-node1 ~]# getsebool getsebool: SELinux is disabled 更新系统并重启[root@linux-node1 ~]# yum update -y &amp;&amp; reboot centos7 设置开机脚本 新建开机脚本vim /root/Dropbox/save/bootstartscript.sh # 添加开机启动脚本 #开机启动 dropbox dropbox start -d 添加开机脚本到启动文件vim /etc/rc.d/rc.local # 开机启动脚本 /bin/sh /root/Dropbox/save/bootstartscript.sh 设置启动脚本生效 chmod +x /etc/rc.d/rc.local]]></content>
      <categories>
        <category>服务器</category>
      </categories>
      <tags>
        <tag>CentOS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Centos 7/6 内核版本由 3.10.0 升级至 4.12.4 方法]]></title>
    <url>%2F2015%2F01%2F21%2FCentos7%E5%86%85%E6%A0%B8%E7%94%B13.10%E5%8D%87%E7%BA%A7%E8%87%B34.12%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[【写在前面】公司打算上 Docker 服务，目前需要安装运行环境，Docker 新的功能除了需要 Centos 7 系统之外，内核的版本高低也决定着使用的效果，所以在此记录下系统内核版本升级过程。注：对于线上环境的内核版本还需要根据实际情况谨慎选择，越新的版本未来可能遇到的问题越多，此文只是记录升级方法而已。 【文章内容】关于内核版本的定义： 版本性质：主分支 ml(mainline)，稳定版(stable)，长期维护版 lt(longterm) 版本命名格式为 “A.B.C”： 数字 A 是内核版本号 ：版本号只有在代码和内核的概念有重大改变的时候才会改变，历史上有两次变化： 第一次是 1994 年的 1.0 版，第二次是 1996 年的 2.0 版，第三次是 2011 年的 3.0 版发布，但这次在内核的概念上并没有发生大的变化 数字 B 是内核主版本号：主版本号根据传统的奇 - 偶系统版本编号来分配：奇数为开发版，偶数为稳定版 数字 C 是内核次版本号：次版本号是无论在内核增加安全补丁、修复 bug、实现新的特性或者驱动时都会改变 一、查看那系统内核版本uname -r 3.10.0-514.el7.x86_64 cat /etc/redhat-release CentOS Linux release 7.3.1611 (Core) 二、升级内核Centos 6 和 Centos 7 的升级方法类似，只不过就是选择的 YUM 源或者 rpm 包不同罢了，下面主要是 Centos 7 的安装方法，中间也会有对于 Centos 6 升级的方法提示。 方法一： Centos 6 YUM 源：http://www.elrepo.org/elrepo-release-6-6.el6.elrepo.noarch.rpmCentos 7 YUM 源：http://www.elrepo.org/elrepo-release-7.0-2.el7.elrepo.noarch.rpm 升级内核需要先导入 elrepo 的 key，然后安装 elrepo 的 yum 源： rpm -import https://www.elrepo.org/RPM-GPG-KEY-elrepo.org rpm -Uvh http://www.elrepo.org/elrepo-release-7.0-2.el7.elrepo.noarch.rpm 仓库启用后，你可以使用下面的命令列出可用的内核相关包，如下图： yum --disablerepo=&quot;*&quot; --enablerepo=&quot;elrepo-kernel&quot; list available 上图可以看出，长期维护版本 lt 为 4.4，最新主线稳定版 ml 为 4.12，我们需要安装最新的主线稳定内核，使用如下命令：(以后这台机器升级内核直接运行这句就可升级为最新稳定版) yum -y --enablerepo=elrepo-kernel install kernel-ml.x86_64 kernel-ml-devel.x86_64 方法二： 对于一些无法上网的服务器，或者需要安装指定版本内核的需求，我们可以把 kernel image 的 rpm 包下载下来安装，下载地址如下： 下载指定版本 kernel： http://rpm.pbone.net/index.php3?stat=3&amp;limit=1&amp;srodzaj=3&amp;dl=40&amp;search=kernel 下载指定版本 kernel-devel：http://rpm.pbone.net/index.php3?stat=3&amp;limit=1&amp;srodzaj=3&amp;dl=40&amp;search=kernel-devel 官方 Centos 6: http://elrepo.org/linux/kernel/el6/x86_64/RPMS/ 官方 Centos 7: http://elrepo.org/linux/kernel/el7/x86_64/RPMS/ 将 rpm 包下载上传到服务器上，使用下面的命令安装即可： yum -y install kernel-ml-devel-4.12.4-1.el7.elrepo.x86_64.rpm yum -y install kernel-ml-4.12.4-1.el7.elrepo.x86_64.rpm 方法三： 还可以通过源码包编译安装，这种方式可定制性强，但也比较复杂，有需要的可自行查找资料安装，下面只给出各系统版本内核源码包的下载地址：https://www.kernel.org/pub/linux/kernel/ 三、修改 grub 中默认的内核版本 内核升级完毕后，目前内核还是默认的版本，如果此时直接执行 reboot 命令，重启后使用的内核版本还是默认的 3.10，不会使用新的 4.12.4，首先，我们可以通过命令查看默认启动顺序： awk -F\&apos; &apos;$1==&quot;menuentry &quot; {print $2}&apos; /etc/grub2.cfg CentOS Linux (4.12.4-1.el7.elrepo.x86_64) 7 (Core) CentOS Linux (3.10.0-514.el7.x86_64) 7 (Core) CentOS Linux (0-rescue-a43cc2091b4557f1fd10a52ccffa5db2) 7 (Core) 由上面可以看出新内核 (4.12.4) 目前位置在 0，原来的内核 (3.10.0) 目前位置在 1，所以如果想生效最新的内核，还需要我们修改内核的启动顺序为 0： vim /etc/default/grub 注：Centos 6 更改的文件相同，使用命令确定新内核位置后，然后将参数 default 更改为 0 即可。 接着运行 grub2-mkconfig 命令来重新创建内核配置，如下： grub2-mkconfig -o /boot/grub2/grub.cfg 四、重启系统并查看系统内核reboot 系统启动完毕后，可以通过命令查看系统的内核版本，如下： uname -r 4.12.4-1.el7.elrepo.x86_64 到此，Centos 7 内核升级完毕。]]></content>
      <categories>
        <category>服务器</category>
      </categories>
      <tags>
        <tag>CentOS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo NexT 主题添加点击爱心效果]]></title>
    <url>%2F2015%2F01%2F20%2FHexo%20NexT%E4%B8%BB%E9%A2%98%E6%B7%BB%E5%8A%A0%E7%82%B9%E5%87%BB%E7%88%B1%E5%BF%83%E6%95%88%E6%9E%9C%2F</url>
    <content type="text"><![CDATA[给 NexT 主题内添加页面点击出现爱心的效果 创建 js 文件 在/themes/next/source/js/src下新建文件 clicklove.js，接着把该链接下的代码拷贝粘贴到clicklove.js 文件中。代码如下： !function(e,t,a){function n(){c(&quot;.heart{width: 10px;height: 10px;position: fixed;background: #f00;transform: rotate(45deg);-webkit-transform: rotate(45deg);-moz-transform: rotate(45deg);}.heart:after,.heart:before{content: &apos;&apos;;width: inherit;height: inherit;background: inherit;border-radius: 50%;-webkit-border-radius: 50%;-moz-border-radius: 50%;position: fixed;}.heart:after{top: -5px;}.heart:before{left: -5px;}&quot;),o(),r()}function r(){for(var e=0;e&lt;d.length;e++)d[e].alpha&lt;=0?(t.body.removeChild(d[e].el),d.splice(e,1)):(d[e].y--,d[e].scale+=.004,d[e].alpha-=.013,d[e].el.style.cssText=&quot;left:&quot;+d[e].x+&quot;px;top:&quot;+d[e].y+&quot;px;opacity:&quot;+d[e].alpha+&quot;;transform:scale(&quot;+d[e].scale+&quot;,&quot;+d[e].scale+&quot;) rotate(45deg);background:&quot;+d[e].color+&quot;;z-index:99999&quot;);requestAnimationFrame(r)}function o(){var t=&quot;function&quot;==typeof e.onclick&amp;&amp;e.onclick;e.onclick=function(e){t&amp;&amp;t(),i(e)}}function i(e){var a=t.createElement(&quot;div&quot;);a.className=&quot;heart&quot;,d.push({el:a,x:e.clientX-5,y:e.clientY-5,scale:1,alpha:1,color:s()}),t.body.appendChild(a)}function c(e){var a=t.createElement(&quot;style&quot;);a.type=&quot;text/css&quot;;try{a.appendChild(t.createTextNode(e))}catch(t){a.styleSheet.cssText=e}t.getElementsByTagName(&quot;head&quot;)[0].appendChild(a)}function s(){return&quot;rgb(&quot;+~~(255*Math.random())+&quot;,&quot;+~~(255*Math.random())+&quot;,&quot;+~~(255*Math.random())+&quot;)&quot;}var d=[];e.requestAnimationFrame=function(){return e.requestAnimationFrame||e.webkitRequestAnimationFrame||e.mozRequestAnimationFrame||e.oRequestAnimationFrame||e.msRequestAnimationFrame||function(e){setTimeout(e,1e3/60)}}(),n()}(window,document); 修改_layout.swig在 \themes\next\layout\_layout.swig 文件末尾添加： &lt;!-- 页面点击小红心 --&gt; &lt;script type=&quot;text/javascript&quot; src=&quot;/js/src/clicklove.js&quot;&gt;&lt;/script&gt;]]></content>
      <categories>
        <category>Hexo Next</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何整理个人资料]]></title>
    <url>%2F2015%2F01%2F20%2F%E5%A6%82%E4%BD%95%E6%95%B4%E7%90%86%E4%B8%AA%E4%BA%BA%E8%B5%84%E6%96%99%2F</url>
    <content type="text"><![CDATA[序言 在现如今信息爆炸的时代，资料整理的方法显得越来越重要。好的资料整理方法可以让收集的资料发挥出它应有的价值，否则便和没有收集无异。 在平时工作学习中，看到好的技术文档或文章忍不住把它放到收藏夹或使用印象笔记类似的工具裁剪到笔记里，想着日后细细品读。可大多数是没有日后的，这便造成了收集的资料越来越多，越来越混乱，有时候还整理下，但随着数量的增多，发现整理这些东西也是需要很大的时间成本的。这些资料没有发挥其应用的价值，但还舍不得删掉。 之前一直想整理自己的知识体系，想着形成自己的一个知识库，可以随时翻阅。当需要用到哪方便的知识时，可以通过翻阅很快的回忆起来，立即上手。近几年有写博客的习惯，把一些读书笔记和技术的使用过程记录了下来，发挥了些知识库的作用。但总感觉不是那么完美，最近查了下资料，看了些有关资料和个人知识系统整理的文章，有些想法，梳理下加深印象。 整体流程 整个过程应该是这样的： 资料的收集 在现如今网络如此遍历，资料的获取已经不成问题。而如何有效的获取，成为关键。即如何在海一样的信息中，找到我们需要的。现如今我的资料收集方法如下：•搜索引擎 通过搜索引擎关键字，找到自己需要的资料。关键字很重要，必要的时候可以直接使用英文搜索。•微信公众号 通过平时碎片化的阅读，可收集到一些自己认为有价值的文章。•各技术社区 现如今国内各技术社区活跃，文章水平残次不齐。推荐先根据标题略读，发现好的文章后再精度。我们会发现一些高质量文章的产出作者，我们可以订阅他们，之后可重点关注他们的文章。 整理分类 根据我自己的资料，大致分如下几种：•新技术的文档：各种框架文档•技术学习文章：微信公众号文章、各技术社区文章•技术问题：stackoverflow/ 技术问题解决记录•大牛博客•其他工具性网址资料•产出的联系或功能代码 针对以上资料，该如何整理呢？ 工欲善其事必先利其器，整理以上众多资料，整理方法及使用工具如下：•xmind 构建自己的知识体系脑图，作为整个知识库的索引。•evernote 主要存储技术文章和不能公开的自己产出的文档笔记。可按语言和功能所属划分类别，再为文章打好 tag 便于检索。建立临时笔记本暂存未读完的或不知归类的文章，待后续阅读整理。•bookmarks 主要存储技术文档链接、牛人博客地址、工具性网址。同样分好类别，便于检索。•百度云盘 存储开源 pdf 书籍，方便各设备同步。•github 存储自己的周边项目。•博客 产出自己的想法和收获。 消化，产出价值 以上资源分类保存好后，便是消化产出价值了。 第一，在此阶段重要的还是整理，通过阅读我们知道了每篇文章的价值，是否需要留存待日后查询。 原则如下： •可以在网络上轻易找到的，直接删除 •过期的资料直接删除 第二，除了平时碎片化时间的阅读，还需要沉下心来花大块的时间，系统的来学习某项技术。 如何高效的学习消化，日后再整理篇，本篇暂不展开。 通过以上各流程方法，便构建了自己的知识库。完成了以下目标：•梳理自己的知识图谱•通过知识库，可以方便的回忆起某特定技术使用方法。•形成自己的 QA•记录下自己学习的周边项目，作为工作参考 以上整理主要参照下面的文档，如涉及侵权请联系本人，进行删除。 参考 如何有效的进行资料整理？ 信息爆炸的时代，如何静心学习？]]></content>
      <categories>
        <category>随笔</category>
      </categories>
      <tags>
        <tag>感悟</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Markdown 文件的常用编写语法]]></title>
    <url>%2F2015%2F01%2F19%2Fmarkdown%E5%B8%B8%E7%94%A8%E7%BC%96%E5%86%99%E8%AF%AD%E6%B3%95%2F</url>
    <content type="text"><![CDATA[序言:很久没有写博客了，感觉只要是不写博客，人就很变得很懒，学的知识点感觉还是记不住，渐渐地让我明白，看的越多，懂的越少（你这话不是有毛病吗？应该是看的越多，懂的越多才对），此话怎讲，当你在茫茫的前端知识库里面东看看，西看看的时候，很快就被海量的知识给淹没了，根本就不知道哪些是对的，哪些是错的，感觉好像这个也懂了，那个也懂了，但是真正写起来，脑子又一片空白，又好像什么都不懂，这种状态时有发生，这就叫不懂装懂，最根本的原因就是看的太多，写的太少，所以为了改掉这样毛病，把被动学习变成主动学习，接下来的日子，多写写，即使是写一些学习工作中遇到的坑也是好的，没事翻出来看看，还可以加深印象，好了，废话到处！ 起因：因为现在的前端基本上都用上了前端构建工具，那就难免要写一些 readme 等等的说明性文件，但是这样的文件一般都是.md 的文件，编写的语法自然跟其他格式的文件有所区别，置于为什么要用这种格式的文件，不要问我，我也不知道，大家都这么用，跟着用就对了，如果有大神知道的，不妨告知小弟，本文也是我学习写 markdown 文件的一个笔记吧，仅供参考！ 正文：1、标题的几种写法：第一种： 前面带 #号，后面带文字，分别表示 h1-h6, 上图可以看出，只到 h6，而且 h1 下面会有一条横线，注意，# 号后面有空格 第二种： 这种方式好像只能表示一级和二级标题，而且 = 和 - 的数量没有限制，只要大于一个就行 第三种： 这里的标题支持 h1-h6，为了减少篇幅，我就偷个懒，只写前面二个，这个比较好理解，相当于标签闭合，注意，标题与 #号要有空格 那既然 3 种都可以使用，可不可以混合使用呢？我试了一下，是可以的，但是为了让页面标签的统一性，不建议混合使用，推荐使用第一种，比较简洁，全面 为了搞清楚原理，我特意在网上搜一下在线编写 markdown 的工具，发现实际上是把这些标签最后转化为 html 标签，如图： 在线地址请看这里： http://tool.oschina.net/markdown/ （只是想看看背后的转换原理，没有广告之嫌） 2、列表 我们都知道，列表分为有序列表和无序列表，下面直接展示 2 种列表的写法： 可以看到，无序列表可以用 ， + ， — 来创建，用在线编辑器看，实际上是转换成了 ul&gt;li ，所以使用哪个都可以，推荐使用 吧 有序列表就相对简单一点，只有这一种方式，注意，数字后面的点只能是英文的点，特别注意，有序列表的序号是根据第一行列表的数字顺序来的，比如说： 第一组本来是 3 2 1 倒序，但是现实 3 4 5 ，后面一组 序号是乱的， 但是还是显示 3 4 5 ，这点必须注意了 3、区块引用 比如说，你想对某个部分做的内容做一些说明或者引用某某的话等，可以用这个语句 无序列表下方的便是引用，可以有多种用途，看你的需求了，用法就是在语句前面加一个 &gt; ，注意是英文的那个右尖括号，注意空格 引用因为是一个区块，理论上是应该什么内容都可以放，比如说：标题，列表，引用等等，看看下图： 将上面的代码稍微改一下，全部加上引用标签，就变成了一个大的引用，还有引用里面还有引用，那引用嵌套引用还没有别的写法呢？ 上图可以看出，想要在上一次引用中嵌套一层引用，只需多加一个 &gt;，理论上可以无限嵌套，我就不整那么多了，注意：多层嵌套的 &gt; 是不需要连续在一起的，只要在一行就可以了，中间允许有空格，但是为了好看，还是把排版搞好吧 4、华丽的分割线 分割线可以由 * - _（星号，减号，底线）这 3 个符号的至少 3 个符号表示，注意至少要 3 个，且不需要连续，有空格也可以 应该看得懂吧，但是为了代码的排版好看，你们自己定规则吧，前面有用到星号，建议用减号 5、链接 支持 2 种链接方式：行内式和参数式，不管是哪一种，链接文字都是用 [方括号] 来标记。 上图可知，行内式的链接格式是：链接的文字放在 [] 中，链接地址放在随后的（）中，举一反三，经常出现的列表链接就应该这样写： 链接还可以带 title 属性，好像也只能带 title，带不了其他属性，注意，是链接地址后面空一格，然后用引号引起来 这是行内式的写法，参数式的怎么写： 这就好理解了，就是把链接当成参数，适合多出使用相同链接的场景，注意参数的对应关系，参数定义时，这 3 种写法都可以： 还支持这种写法，如果你不想混淆的话： 其实还有一种隐式链接的写法，但是我觉得那种写法不直观，所以就不写了，经常用的一般就上面 2 种，如果你想了解隐式链接，可以看我文章最后放出的参考地址 6、图片 图片也有 2 种方式：行内式和参数式， 用法跟链接的基本一样，唯一的不同就是，图片前面要写一个！（这是必须的），没什么好说的 7、代码框 这个就比较重要了，很多时候都需要展示出一些代码 如果代码量比较少，只有单行的话，可以用单反引号包起来，如下： 要是多行这个就不行了，多行可以用这个： 多行用三个反引号，如果要写注释，可以在反引号后面写 8、表格 这个写的有点麻烦，注意看 从这 3 种不同写法看，表格的格式不一定要对的非常起，但是为了好看，对齐肯定是最好的，第一种的分割线后面的冒号表示对齐方式，写在左边表示左对齐，右边为右对齐，两边都写表示居中，还是有点意思的，不过现实出来的结果是，表格外面并没有线框包起来，不知道别人的怎么弄的 9、强调 一个星号或者是一个下划线包起来，会转换为 倾斜，如果是 2 个，会转换为 加粗 10、转义 就不一一列举了，基本上跟 js 转义是一样的 11、删除线 常用的基本上就这些了，如果还有一些常用的，可以跟我留言，我补充上去，我觉得图文并茂才是高效学习的正确姿势，但愿为你的学习带来帮助！ 以上整理主要参照下面的文档，如涉及侵权请联系本人，进行删除。 参考文献： Markdown 语法说明 (简体中文版) 认识与入门 Markdown]]></content>
      <categories>
        <category>技术工具</category>
      </categories>
      <tags>
        <tag>Markdown</tag>
      </tags>
  </entry>
</search>
